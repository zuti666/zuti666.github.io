---
layout: post
title:  Paper Reading 16 LLM finetuning and forgetting
categories: [Paper Reading,  LLM, Continual Learning,] 
description:  [Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models]
keywords: [Paper Reading,  LLM, Continual Learning, ] 
---



# Paper Reading 16  Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models



# 论文信息



Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models



**Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models**

[`semanticscholar`](https://www.semanticscholar.org/paper/9e2a811a6f5d1c5352ce19ac24303810eb1867f7)  [`Paper`](https://www.semanticscholar.org/paper/9e2a811a6f5d1c5352ce19ac24303810eb1867f7)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9e2a811a6f5d1c5352ce19ac24303810eb1867f7%3Ffields%3DcitationCount)

2023    Annual Meeting of the Association for Computational Linguistics 



这篇论文指出 SEQ sequence fine-tuning 下， PLMs 能够避免遗忘保持对大多数知识的记忆。 sequence fine-tuning下，PLMs 的遗忘现象主要由于分类器的偏移而不是 PLM遗忘了旧的知识。





# AI 总结

以下是对论文《Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models》的综合总结，包括范围、核心思想、方法、贡献、差异与创新、结果与结论、与其他方法对比以及讨论部分。

### 范围与背景 (Scope or Setting)

- **范围**: 论文聚焦于在增量学习 (Incremental Learning, IL) 场景中使用预训练语言模型（Pre-trained Language Models, PLMs）作为主干模型，研究其在分类任务中的表现。
- **研究场景**: 包括类别增量学习（Class-Incremental Learning, CIL）和任务增量学习（Task-Incremental Learning, TIL），任务涉及文本分类、意图分类、关系提取和命名实体识别。

### 核心思想 (Key Idea)

- 提出一个核心问题：**预训练语言模型（PLMs）是否真的会在增量学习中遭遇灾难性遗忘（Catastrophic Forgetting）？**
- 通过实验发现，PLMs在增量学习中具备强大的抗遗忘能力，这与现有许多假设相悖。
- 基于此，提出一种“令人沮丧的简单方法”SEQ*，通过冻结模型和其他优化策略，达到接近甚至超越当前最佳方法的效果。

### 方法 (Method)

1. 实验设置：

   - 使用20多种方法，在多个数据集上测试PLMs在CIL和TIL场景中的性能。
   - 测试两种主流架构（encoder-only和decoder-only）和不同规模的模型（参数从19M到1.21B）。
   
2. 测量遗忘:

   - 引入探测性能（Probing Performance）作为评估PLMs内部知识遗忘的标准，与传统的观测性能（Observed Performance）进行对比。

3. SEQ*:

   - 提出冻结PLM主干（Backbone）和旧分类器等策略，减少参数调整并防止灾难性遗忘。

### 贡献 (Contribution)

- 实验性发现:

  - 现有研究过度夸大了PLMs的遗忘问题。
  - PLMs在增量学习中，内置的抗遗忘能力源于预训练阶段和Transformer的架构。
  - 线性探测性能几乎不受遗忘影响，遗忘主要发生在分类器中。
  
- 实践意义:

  - 提出的SEQ*方法在多数设置中表现出优于或媲美当前SOTA方法的性能，同时显著减少了训练参数和时间。

### 差异与创新 (Difference and Innovation)

- **差异**: 与传统方法注重克服灾难性遗忘不同，该研究重新审视遗忘的定义，强调PLMs的抗遗忘特性。

- 创新:

  - 提出“冻结主干”的简单但有效的优化策略。
- 系统性地分析了探测性能与分类器性能的关系，揭示了分类器偏离导致遗忘的根本原因。

### 结果与结论 (Results and Conclusion)

- 在多个数据集和任务上，SEQ*方法展示了与SOTA方法相当甚至更好的性能，同时训练开销显著减少。
- 研究建议NLP领域重新审视灾难性遗忘假设，并提出设计包含领域知识的增量学习基准和更高效的算法作为未来方向。

### 与其他方法对比 (Contrast with Other Methods)

- SEQ*在减少训练参数和时间的同时，与基于参数微调和知识蒸馏的复杂方法相比，取得了竞争性的结果。
- 在某些需要调整类边界的任务中，如实体识别，SEQ*可能不如部分先进方法。

### 讨论 (Discussion)

1. 局限性:

   - 仅研究了分类任务，未探索PLMs在其他形式知识上的遗忘问题。
   - 尚未完全理解PLMs在SEQ方法下增量学习知识的机制。
   
2. 未来方向:

   - 设计更具领域特性要求的增量学习任务。
- 开发低成本、高效增量学习算法。

如果需要进一步详细分析某一部分内容，或者帮助整理成其他形式，欢迎告知！



# 个人总结

## Figure 3

文中提到了很多设置和概念，其代表性就是Figure3 , 这也是文中的主要思想之一 ，我们通过理解这张图来理解文中的设置

![image-20250106161929048](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250106161929113.png)



这个图中提到了两个点

第一个是 observed performance and probing performance, 其中 probing performance是作者提出的概念

第二个是 c,d,e,f 使用四种不同的度量方式来描述 probing performace

还有一个点就是 这里还是用了不同的参数量的大模型，从单独一张图中，就可以发现，趋势是类似的，但是模型参数量越大效果总是表现越好。



## Figure2  the probing performance

1  observed performance and probing performance



图中使用了图二进行解释 两者的不同， 图如下

![image-20250106162419426](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250106162419492.png)



接下来进行对这张图进行解释说明：

要想理解这两者的区别，主要来看作者新设置的这个 Probing Performance 这一部分，

其核心要点有两个

- 固定PLM ，训练一个分类器
- 使用所有任务的数据，来训练和评估这个分类器

这个分类器可以接触到所有任务的数据进行训练，没有涉及到增量学习或者持续学习的设置，这样子做法就类似于continual learning 设置中的 multi-task 设置，可以作为上界。

分类器的直接输入为 PLM 输出的特征向量组成的特征空间$PLM(X)$，这个PLM 又是固定的，所以最终分类结果就只依赖于样本和分类器，而样本是固定的。所以可以设置不同的分类器来探究不同的分类器的分类效果。这也就是是作者在后面提到设置的四种不同分类器：linear probing, cosine linear probing, prototype probing, and cosine prototyping。

另外，由于分类器是一个简单的模型，所以最终实验的效果其实取决于 PLM 的表征能力，如果他能够将不同的类别的输入表征为不同的输出特征，那么一个简单的线性分类器也能够工作地很好。这就说明PLM 有着充分地表征能力，也就可以认为PLM zero shoot 就能做到很好，他在预训练阶段就获得了表征这些新的样本的能力。

而下面的设置就是大家常见的增量学习设置，遇到新的任务就finetuning 大模型以及对应的分类层，根据我们的经验，这会导致灾难性遗忘，也就是随着训练地进行，模型在之前学习过得任务的表现会越来越差。



那么这两个实验就形成了对比，第一个实验效果很好的话，那就说明 大模型有着足够的表征能力，而且只需要一个简单的分类器就能达到很好的效果。那么第二个增量学习效果变差，原因会是什么呢

一个原因就是 训练时候接触到的数据不一样，第一个实验相当于multi-task 的设置，模型能够接触到所有任务的数据。而后面的增量学习，每次就只能接触到每个任务的数据。而这个设置是Continual learning 的必须设定，没法改变。 唯一的从数据的可接触性的突破就是 data/experience replay。





这个设置其实和一般的Continual Learning 的设置也没有区别，这个只能作为upper bound来对比实验结果，而无法采用到continual learning的设置中。但区别就是在于由于大模型的表征能力足够强，实际上在 continual learning 的设置下可以不用再训练大模型这一部分，而是只是每次有一个新任务就训练一个新的分类器就好了。 而作者提出的方法恰好就是这么设置的。 

但我有一个问题，如果遇到每次更新都是混合的类别，有没有影响。直接不训练，那么新的分类器是否能够进行分开。但这里不同的类别是什么意思呢，在文本之中的类别的概念。
另外还有一个点就是有了CLIP岂不是也可以直接拿来做图像分类任务。



但是我觉得这两者进行对比有些过于跨越了，其实还可以多设置一个实验，那就是在Incremental-Learning 的时候也不训练PLM,而是直接训练一个分类器，从而通过三者的对比就能更好说明原因。而实际上作者提出的改进方法就是包括这个。 保持原来的PLM不变。



##  different classifier

接下来详细介绍，作者在探究四种不同的分类器的设置



以下是关于四种分类器的解释：

### **1. Linear Probing**

- 核心概念:

  - 线性探测 (Linear Probing) 是最基本的探测方法，在冻结 PLM 参数的基础上，在其输出特征之上添加一个线性分类器。

- 实现细节:

  - 分类器的输出 logits $z$ 通过以下公式计算：
    $$
    z= W \cdot h 
    $$
    

    - $W$: 线性分类器的权重矩阵。
    - $h$: PLM 的隐藏状态（特征）。
    - $b$: 偏置向量。 文中说不需要这一项

  - 输出 logits $z$ 决定样本属于每个类别的概率。

- 优点:

  - 线性分类器简单直接，可以充分利用特征的范数（L2 范数）和方向信息。

- 适用场景:

  - 测试隐藏特征是否具有区分类别的能力。

------

### **2. Cosine Linear Probing**

- 核心概念:

  - 余弦线性探测与线性探测类似，但分类过程中使用余弦相似度代替内积来计算 logits。

- 实现细节:

  - logits 的计算公式为：
    $$
    z=\text{cos}(W, h) = \frac{W \cdot h}{\|W\| \cdot \|h\|}
    $$
    

    - $\text{cos}(W, h)$: $W$ 和 $h$ 之间的余弦相似度。
    - $|W|$: 分类器权重的 L2 范数。
    - $|h|$: 隐藏特征的 L2 范数。

- 优点:

  - 通过规范化权重和特征向量，消除了特征范数差异的影响，更关注特征方向与类别中心的对齐情况。

- 适用场景:

  - 避免类别预测偏向范数较大的新类别（解决增量学习中的类不平衡问题）。

- 局限性:

  - 忽略了特征范数的区分能力，可能导致精度下降。

------

### **3. Prototype Probing**

- 核心概念:

  - 使用每个类别的特征中心作为类别的“原型” (Prototype)，通过特征到类别中心的距离计算分类结果。

- 实现细节:

  - 每个类别 $c$ 的特征中心 $p_c$ 计算如下：
    $$
    p_c=1 \frac{1}{N_c} \sum_{i \in C_c} h_i
    $$
    

    - $N_c$: 类别 $c$ 的样本数。
    - $C_c$: 类别 $c$ 的样本集合。
    - $h_i$: 第 $i$ 个样本的隐藏特征。

  - logits 的计算方式是输入特征 $h$ 与所有类别特征中心的欧几里得距离： 
    $$
    z= - \|h - p_c\|^2
    $$
    

- 优点:

  - 不需要训练额外的分类器，直接使用特征中心作为类别表示。
  - 适用于样本分布集中、类别特征具有代表性的场景。

- 局限性:

  - 特征中心的方向可能受数据分布影响，难以充分表达复杂类别边界。

------

### **4. Cosine Prototype Probing**

- 核心概念:

  - 与 Prototype Probing 类似，但计算 logits 时使用余弦相似度而不是欧几里得距离。

- 实现细节:

  - logits 的计算公式为：
    $$
    z=\text{cos}(h, p_c) = \frac{h \cdot p_c}{\|h\| \cdot \|p_c\|}
    $$
    

    - $p_c$: 类别 $c$ 的特征中心。
    - $|h|$, $|p_c|$: 特征和特征中心的范数。

- 优点:

  - 通过余弦相似度，消除了范数的影响，更关注特征方向。
  - 对于类别特征中心较为分散的场景，能够更稳定地计算相似性。

- 局限性:

  - 忽略了特征范数可能蕴含的类别分布信息。

------

### **核心对比表格**

| 分类器类型                   | 特点                                   | Logits 计算方式                       | 优点                             | 局限性                         |
| ---------------------------- | -------------------------------------- | ------------------------------------- | -------------------------------- | ------------------------------ |
| **Linear Probing**           | 基于内积，结合特征范数与方向信息       | $W \cdot h + b$                       | 简单高效，充分利用特征范数和方向 | 可能受到范数不规范化的影响     |
| **Cosine Linear Probing**    | 基于余弦相似度，忽略特征范数           | $\frac{W \cdot h}{|W| \cdot |h|}$     | 减少对大范数类别的偏向           | 忽略范数，可能导致精度下降     |
| **Prototype Probing**        | 使用类别特征中心，基于欧几里得距离计算 | $-|h - p_c|^2$                        | 无需训练分类器，简单有效         | 依赖数据分布，难以表达复杂边界 |
| **Cosine Prototype Probing** | 使用类别特征中心，基于余弦相似度计算   | $\frac{h \cdot p_c}{|h| \cdot |p_c|}$ | 消除范数差异影响，关注特征方向   | 忽略范数对分类可能带来的信息   |

------



### 图示说明

![image-20250106180043992](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250106180044095.png)

从图中可以看到 使用 cos相似度距离的两种方法，特征向量的长度都进行了归一化，而使用线性和原型方法都没有归一化

另外基于原型的方法，特征向量和 特征距离更加近，作者分析这样会落在一个 narrow corn space之中



### 效果对比

![image-20250106181710937](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250106181711050.png)



![image-20250106181729061](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250106181729159.png)



![image-20250106182022122](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250106182022227.png)



![image-20250106184948193](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250106184948313.png)



![image-20250106185006651](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250106185006751.png)







#### **实验设置总结**

### 实验条件与结果概述

| **图编号**    | **实验任务**                                  | **数据集**     | **模型类型**         | **分类器类型**                                     | **评估指标** |
| ------------- | --------------------------------------------- | -------------- | -------------------- | -------------------------------------------------- | ------------ |
| **Figure 8**  | 意图分类（Intent Classification）             | CLINC150       | Generative Model     | Linear, Cosine Linear, Prototype, Cosine Prototype | Accuracy     |
| **Figure 9**  | 关系提取（Relation Extraction）               | FewRel         | Generative Model     | 同上                                               | Accuracy     |
| **Figure 10** | 关系提取（Relation Extraction）               | FewRel         | Discriminative Model | 同上                                               | Accuracy     |
| **Figure 11** | 命名实体识别（Named Entity Recognition, NER） | Ontonotes5     | Discriminative Model | 同上                                               | Accuracy     |
| **Figure 12** | 命名实体识别（NER）                           | I2B2           | Discriminative Model | 同上                                               | Accuracy     |
| **Figure 13** | 文本分类（Text Classification）               | Topic3Datasets | Generative Model     | 同上                                               | Accuracy     |
| **Figure 14** | 文本分类（Text Classification）               | Topic3Datasets | Discriminative Model | 同上                                               | Accuracy     |

### 对四种分类器效果的对比





以下是基于论文中 **Figure 3、Figure 8、Figure 9 和 Figure 10** 的实验结果，对四种分类器的效果对比分析，以及作者提出使用 **“Use cosine linear classifiers only when no old data is available in a CIL scenario. Otherwise, use linear classifiers”** 策略的原因分析。

------

#### **实验结果分析：四种分类器的效果对比**

**1. 实验结果中的主要观察**

从 **Figure 3**（Class-Incremental Intent Classification）和 **Figure 8, 9, 10**（分别对应 CLINC150 和 FewRel 数据集）可以观察到以下现象：

1. **Linear Probing (线性分类器)**:
   - 在所有实验中，线性分类器的探测性能（Probing Performance）始终高于其他分类器。
   - 其在初始任务（Task 1）后表现快速提升，且随着任务数量的增加，其性能几乎保持稳定或略有提升。
   - **原因**: 线性分类器可以利用特征的方向和范数信息，不受余弦正则化的限制，能够更好地提取特征中的有用信息。
2. **Cosine Linear Probing (余弦线性分类器)**:
   - 整体表现次于线性分类器，特别是在任务数量较多时，其性能有所下降。
   - 其设计消除了特征和权重向量的范数对分类决策的影响，因此在任务数据分布不均匀（例如新任务特征范数较大）的场景中，可以减轻偏向新类的现象。
   - **缺点**: 忽略了范数可能携带的信息，导致对旧类的区分能力减弱。
3. **Prototype Probing (原型分类器)**:
   - 基于欧几里得距离的分类器表现低于线性分类器，尤其在任务数量增加后，其性能迅速下降。
   - 由于特征中心可能落在分布不均的空间中，类别边界可能不准确，从而降低分类效果。
   - **缺点**: 对于特征分布复杂的类别，其特征中心无法精确代表类别特性。
4. **Cosine Prototype Probing (余弦原型分类器)**:
   - 性能高于原型分类器，但低于线性分类器和余弦线性分类器。
   - 使用余弦相似度减少了范数的影响，使其在特征分布分散或任务类别较少的情况下有一定优势。
   - **缺点**: 同样忽略了特征范数可能携带的信息，分类边界可能不够清晰。

------

**2. 实验结果中的分类器排名总结**

| 分类器类型                   | 性能排名（高到低）                         | 适用场景                                       |
| ---------------------------- | ------------------------------------------ | ---------------------------------------------- |
| **Linear Probing**           | 始终排名第一，性能最稳定                   | 任务数据充足或可以保存旧任务数据的场景。       |
| **Cosine Linear Probing**    | 次于线性分类器，但在避免偏向新类方面有优势 | 无法保存旧数据，且需要解决新类偏置问题的场景。 |
| **Cosine Prototype Probing** | 表现中等，略优于原型分类器                 | 特征分布分散但任务复杂度较低时可用。           |
| **Prototype Probing**        | 表现最差，且随着任务增多效果快速下降       | 仅在特征分布较集中的简单任务中可能有效。       |

------

#### **策略分析：为何选择“Use cosine linear classifiers only when no old data is available in a CIL scenario. Otherwise, use linear classifiers”？**

**1. 问题背景**

- CIL 场景的核心挑战:

  - 增量学习的类别不平衡问题：新任务类别的样本特征通常具有更大的范数，这会导致线性分类器倾向于预测新类别。
- 缺乏旧任务数据时，无法通过回放数据缓解这种倾向。

**2. 余弦线性分类器的作用**

- 解决新类别偏置问题:

  - 通过归一化权重和特征向量（余弦相似度），减少了范数对分类的影响。
- 在无旧数据时，可以避免分类器过度偏向新类别。

**3. 为什么保留线性分类器？**

- 线性分类器的全面性:

  - 如果可以访问旧任务数据，线性分类器的表现显著优于其他方法，因为它可以充分利用特征的范数和方向信息。
  - 例如，从 **Figure 3a** 和 **Figure 8a** 可以看出，当使用线性分类器时，其性能远高于余弦分类器，特别是在有多个任务数据时。
  
- 适应更复杂的任务:

  - 在任务类别边界复杂或特征分布复杂的情况下，线性分类器可以更灵活地学习类别之间的差异。

**4. 综合策略的合理性**

- 作者选择的策略充分结合了两种分类器的优势：
  - 当无法保存旧数据时，使用 **Cosine Linear Probing** 减少新类别偏置问题，适配 CIL 场景的约束。
  - 当旧数据可用时，使用 **Linear Probing** 提升整体性能，利用其对特征范数的敏感性。

------

**结论**

1. 实验结果表明:

   - 线性分类器在性能上表现最佳，但容易受新类样本范数偏大的影响。
   - 余弦线性分类器在无旧数据时可以有效避免这种偏置，但性能不及线性分类器。
   
2. 策略选择的原因:

   - 作者的策略合理地结合了两种分类器的优点：用余弦分类器解决数据不平衡问题，用线性分类器最大化性能。

3. 启示:

   - 增量学习中的分类器选择需要根据数据保存条件、任务复杂性和类别分布特点灵活调整。















>以下是对 **Probing Performance** 和 **Observed Performance** 的区别、训练设置的区别以及评估方法的区别的解释，结合论文中的图2和3.1部分内容。
>
>------
>
>### **1. 定义及核心区别**
>
>#### **Probing Performance**
>
>- **定义**: 用于评估预训练语言模型（PLM）内部对所有任务的知识保留情况。
>- 方法
>  - 在PLM的固定特征上训练一个探测分类器（Probing Classifier）。
>  - 探测分类器可以是线性层、余弦线性层或使用类别特征中心的原型分类器。
>  - **关键点**: 探测分类器的训练独立于增量学习的训练过程，因此它表示的是模型隐藏状态中所蕴含的“潜在”知识。
>- **作用**: 提供PLM的性能上限，当探测分类器能够完全利用隐藏特征时的最优分类性能。
>
>#### **Observed Performance**
>
>- **定义**: 使用原始模型直接对测试任务进行预测的性能。
>- 方法
>  - 使用增量学习过程中训练好的分类器进行测试。
>  - 分类器只在学习当前任务时接触过相关数据。
>  - **关键点**: 受灾难性遗忘影响，性能可能因旧任务的知识丢失而下降。
>- **作用**: 表示增量学习过程中实际的任务表现。
>
>**核心区别**:
>
>- Probing Performance: 测试隐藏状态的表示能力（理想状态）。
>- Observed Performance: 测试实际分类器的能力（实际状态）。
>
>------
>
>### **2. 训练设置的区别**
>
>#### **Probing Performance 的训练设置**
>
>- **模型**: PLM的参数被冻结，不会被更新。
>- **分类器**: 训练一个新的探测分类器，利用所有任务的数据（包括旧任务和新任务）进行训练。
>- **数据**: **训练时可以访问所有任务的数据，因此不受灾难性遗忘影响**。
>- **目的**: 评估隐藏特征的潜力，避免受到分类器质量的限制。
>
>#### **Observed Performance 的训练设置**
>
>- **模型**: PLM和分类器参数可以被更新（基于具体方法，例如SEQ或SEQ*）。
>- **分类器**: 在增量学习过程中只用新任务的数据对当前任务的分类器进行训练。
>- **数据**: 每次训练时只能访问当前任务的数据，旧任务的数据可能已经丢失。
>- **目的**: 测量实际增量学习过程中受灾难性遗忘影响的任务性能。
>
>**核心区别**:
>
>- Probing Performance：冻结模型，仅训练探测分类器，使用所有任务数据。
>- Observed Performance：允许更新模型和分类器，受限于增量学习的任务数据。
>
>------
>
>### **3. 评估方法的区别**
>
>#### **Probing Performance 的评估方法**
>
>- 使用探测分类器评估：
>  1. 将所有任务的数据输入冻结的PLM。
>  2. 训练探测分类器预测类别（例如使用线性分类器）。
>  3. 在所有任务的数据上计算准确率。
>- **评估重点**: 评估PLM的特征是否仍然能够区分所有任务的类别。
>
>#### **Observed Performance 的评估方法**
>
>- 使用增量学习过程中训练的分类器：
>  1. 将当前任务数据输入PLM和原始分类器。
>  2. 预测当前任务类别。
>  3. 计算当前任务的准确率（测试数据可能只包含当前任务的数据）。
>- **评估重点**: 测试增量学习过程中分类器的实际预测能力。
>
>------
>
>### **图2 的说明**
>
>图2解释了**Probing Performance**和**Observed Performance**的计算流程：
>
>- **Probing Performance**: 使用固定的PLM（不会更新），添加探测分类器后，评估其对所有任务的分类能力。
>- **Observed Performance**: 使用增量学习过程中训练的分类器直接预测，只能测试当前任务，可能会因遗忘而表现较差。
>
>------
>
>### **总结**
>
>1. **核心差异**: Probing Performance 测试的是PLM的潜力，Observed Performance 反映实际增量学习的结果。
>2. **训练设置差异**: Probing Performance 冻结PLM且使用所有数据，Observed Performance 根据当前任务更新模型且数据受限。
>3. **评估方法差异**: 前者关注隐藏特征的利用效率，后者关注分类器的实际预测能力。
>
>如果需要更详细的分析或例子，可以进一步展开讨论！
>
>











这里的变量作者认为是固定的PLM 还是 动了的PLM ，  如果PLM 不动，效果更好，说明PLM 有着充分的能力

>### **补充和说明对图2的认识**
>
>图2 的核心内容是通过对比 **Observed Performance（观察性能）** 和 **Probing Performance（探测性能）** 的两部分实验，分析 PLMs 在增量学习中的知识保持和遗忘情况。以下是具体说明。
>
>------
>
>### **上下两部分实验设置的描述与对比**
>
>1. **下半部分实验设置：观察性能（Observed Performance）**
>   - **实验设置**:
>     - 直接评估增量学习过程中 PLMs 的性能。
>     - 每次学习新任务时，模型的分类器仅在当前任务数据上训练。
>     - 测试阶段，模型直接用更新后的分类器对所有任务进行分类。
>   - **结果表现**:
>     - 性能随着任务数量增加而显著下降。
>     - 被用作评估灾难性遗忘的传统指标。
>   - **关键特点**:
>     - 强调分类器和 PLMs 的联合性能。
>     - 受限于分类器的设计和数据分布。
>2. **上半部分实验设置：探测性能（Probing Performance）**
>   - **实验设置**:
>     - 在冻结 PLMs 参数的情况下，为所有任务分别训练新的探测分类器。
>     - 探测分类器在增量学习所有已学任务的数据上进行训练，测试时对所有任务的分类性能进行评估。
>   - **结果表现**:
>     - 性能几乎没有显著下降，表明 PLMs 本身保留了绝大部分任务知识。
>     - 被用作评估 PLMs 的知识保持能力。
>   - **关键特点**:
>     - 分离了分类器对性能的影响，仅反映 PLMs 的知识保持上限。
>     - 不受分类器调整策略和数据分布的直接影响。
>
>------
>
>### **作者的目的与实验能够说明的问题**
>
>#### **目的**
>
>1. **揭示灾难性遗忘的真实来源**:
>   - **现有假设**: 认为灾难性遗忘主要源自 PLMs 对旧任务知识的丢失。
>   - **作者目的**: 验证灾难性遗忘是否源自 PLMs 本身，还是由分类器调整导致。
>2. **分离 PLMs 和分类器的作用**:
>   - 探测性能评估 PLMs 的表示能力，观察性能评估分类器和 PLMs 的联合表现。
>   - 通过对比两者，探明 PLMs 和分类器在增量学习中的具体表现差异。
>3. **优化增量学习策略的设计**:
>   - 如果探测性能高而观察性能低，说明遗忘问题主要集中在分类器上，而非 PLMs。
>   - 这种情况下，可以优化分类器设计，而无需复杂的 PLMs 调整策略。
>
>------
>
>#### **实验能够说明的问题**
>
>1. **PLMs 的抗遗忘能力**:
>   - 探测性能几乎不下降，表明 PLMs 在学习新任务时，仍然保留了对旧任务知识的表示能力。
>   - 说明灾难性遗忘的影响被现有研究夸大，PLMs 在增量学习中实际上具有强大的抗遗忘能力。
>2. **分类器是遗忘的主要来源**:
>   - 观察性能显著下降，而探测性能保持稳定，表明遗忘问题主要来源于分类器的调整。
>   - 分类器在增量学习中难以处理新旧任务之间的冲突，导致分类边界偏离。
>3. **重新评估灾难性遗忘的假设**:
>   - 实验挑战了灾难性遗忘的传统假设，强调分类器设计的重要性。
>   - 提示研究者需要重新设计和评价增量学习方法，避免过于依赖复杂的 PLMs 调整。
>
>------
>
>### **总结**
>
>图2 的上下两部分实验通过对比 **观察性能** 和 **探测性能**，提供了一种全新的分析视角，将灾难性遗忘的来源明确区分为分类器和 PLMs。作者通过此实验说明，PLMs 本身的知识保持能力远强于传统假设，遗忘问题主要集中在分类器设计上。这一实验为优化增量学习策略提供了直接的理论依据，强调了分类器的关键作用，而非过度调整 PLMs。如果还有未理解的部分，可以进一步探讨实验细节或结果分析！
>
>



我的理解：



>以下是对图二及相关内容的补充说明与完善，并解答其中没有理解的部分。
>
>------
>
>### 对图二的补充说明
>
>1. **上下两种样式的对比**：
>   - 上方的方式（Probing Performance）
>     - **训练方式**：固定 PLM（编码器不更新），只训练一个分类头（分类器）来适配所有15个任务。
>     - **评估方式**：分类器针对所有15个任务的样本进行训练和评估，因此能够反映 PLM 的表示能力。
>   - 下方的方式（Observed Performance）
>     - **训练方式**：在增量学习过程中，PLM 和分类器都会随着新任务的数据进行更新。
>     - **评估方式**：仅在当前任务（如第4个任务）上评估性能，无法完全反映模型是否保留了对所有任务的知识。
>2. **对比逻辑的关键点**：
>   - 上方方式的目标是测试 PLM 固有的表示能力及其是否保存了所有任务的知识。
>   - 下方方式则更贴近真实的增量学习场景，强调模型如何在动态更新中保留过去的知识。
>   - **对比的意义**：揭示增量学习中灾难性遗忘的来源是否是 PLM 自身，还是分类器的调整问题。
>
>------
>
>### 对对比实验的合理性分析
>
>1. **为何上方 Probing Performance 会表现更好？**
>   - 上方方式冻结了 PLM，仅仅训练一个分类头，并在所有任务数据上进行评估。因为 PLM 在整个过程中保持一致，只要分类器能够合理区分15个任务的类别，性能不会显著下降。
>   - 上面的PLM 是不变的，相当于一直在进行 zero-shoot，探测 PLM 内部是否保留了对所有任务的表示能力。
>2. **为何下方 Observed Performance 会表现较差？**
>   - 增量学习过程中，PLM 和分类器会因新任务的引入而调整，而这种调整可能会导致对旧任务的知识丢失。
>   - 评估仅在当前任务上进行，无法体现对旧任务知识的保留情况。
>3. **添加对比实验：冻结 PLM 的增量学习**：
>   - 你的建议是，在下方的增量学习实验中也固定住 PLM，这样分类器每次训练只针对当前任务的数据。这种情况下，分类器不会受 PLM 表示能力变化的影响。
>   - **假设**：如果 PLM 固定后性能显著提升，则进一步验证了灾难性遗忘主要源于 PLM 的动态调整，而非 PLM 表示能力的本身不足。
>
>------
>
>### 图二对比实验的进一步改进建议
>
>为了更全面地验证图二中对灾难性遗忘来源的论断，建议引入以下实验：
>
>1. **冻结 PLM 的增量学习**：
>   - **设置**：与下方的实验相同，但 PLM 保持冻结，仅更新分类器。
>   - **意义**：将此结果与 Observed Performance 进行对比，分析 PLM 更新是否对遗忘问题造成了显著影响。
>   - **预期结果**：如果性能提升，说明 PLM 的动态更新是遗忘的主要来源；反之则说明分类器调整仍然是关键。
>2. **固定分类器的增量学习**：
>   - **设置**：在增量学习中，固定分类器，而仅更新 PLM。
>   - **意义**：验证分类器的调整是否会导致性能下降，从而进一步分离遗忘的来源。
>   - **预期结果**：如果性能下降，则说明分类器的动态调整确实会对遗忘问题产生显著影响。
>3. **增量学习与全任务训练的对比**：
>   - 将下方的增量学习结果与上方全任务训练结果（Probing Performance）进行定量对比，分析性能损失的比例，并评估动态学习的影响程度。
>
>------
>
>### 结论和理解补充
>
>- **图二的关键点**：作者通过对比 Probing Performance 和 Observed Performance，提出了一个重要的观点：PLM 在增量学习中的灾难性遗忘，主要来源于分类器在增量任务中的动态调整，而非 PLM 本身对表示能力的遗忘。
>- **实验目的**：通过区分 PLM 表示能力和分类器调整的影响，帮助设计更高效的增量学习策略，如冻结 PLM 的 SEQ* 方法。
>- **进一步改进**：引入“冻结 PLM”的对比实验能够进一步验证这一结论，同时分析 PLM 更新和分类器调整在遗忘中的相对贡献。
>
>如果需要进一步设计具体实验细节或讨论结果，欢迎交流！
>
>



## Understand  the classifier  
