以下内容为对各论文主要数学推导与公式的统一总结与关联分析。使用了相同的符号体系：

- **训练、验证、测试集**：$\mathcal{D}^{tr}, \mathcal{D}^{vl}, \mathcal{D}^{ts}$
- **模型**：$f(\mathbf{x}; \boldsymbol{\theta})$
- **损失函数**：$$
\mathcal{L}(\mathcal{D}; \boldsymbol{\theta}) \;=\; \frac{1}{|\mathcal{D}|}\,\sum_{(\mathbf{x}_i, y_i)\in \mathcal{D}} \ell\!\bigl(f(\mathbf{x}_i;\boldsymbol{\theta}),\,y_i\bigr)
- **梯度 & Hessian**：$\nabla \mathcal{L}(\mathcal{D}; \boldsymbol{\theta})$ 与 $\nabla^2 \mathcal{L}(\mathcal{D}; \boldsymbol{\theta})$

---

# 1. Origin Papers

## 1.1 Paper1: Sharpness-Aware Minimization (SAM)

**核心数学公式**

1. **平坦度度量（min-max 形式）**

   \min_{\boldsymbol{\theta}} \;\max_{\|\boldsymbol{\epsilon}\|_p \le \rho}\;\mathcal{L}\bigl(\mathcal{D}^{tr};\,\boldsymbol{\theta}+\boldsymbol{\epsilon}\bigr).
$$
   其中，$\rho$ 为扰动半径，$\|\cdot\|_p$ 通常取 $p=2$。

2. **一阶近似**  
$$
   \boldsymbol{\epsilon}^*(\boldsymbol{\theta}) \;\approx\;
   \arg\max_{\|\boldsymbol{\epsilon}\|\le\rho}
   \Bigl\langle \nabla \mathcal{L}\bigl(\mathcal{D}^{tr}; \boldsymbol{\theta}\bigr),\,\boldsymbol{\epsilon}\Bigr\rangle
   \;\;\Longrightarrow\;\;
   \boldsymbol{\epsilon}^*(\boldsymbol{\theta}) \;=\;
   \rho \,\frac{\nabla \mathcal{L}\bigl(\mathcal{D}^{tr}; \boldsymbol{\theta}\bigr)}{\bigl\|\nabla \mathcal{L}(\mathcal{D}^{tr}; \boldsymbol{\theta})\bigr\|_2}.
$$
3. **更新步骤**（两次前后向）  
$$
   \boldsymbol{\theta} \;\leftarrow\;
   \boldsymbol{\theta}\;-\;\alpha\,\nabla_{\boldsymbol{\theta}}
   \Bigl[\mathcal{L}\bigl(\mathcal{D}^{tr};\,\boldsymbol{\theta}+\boldsymbol{\epsilon}^*(\boldsymbol{\theta})\bigr)\Bigr].
$$
**要点与推理**  
- 通过“最坏扰动”近似，显式地惩罚曲率尖锐区域，降低 $\nabla^2 \mathcal{L}$ 的特征值。  
- 相比标准SGD，多计算了 $\boldsymbol{\epsilon}^*$ 并在 $\boldsymbol{\theta}+\boldsymbol{\epsilon}^*$ 处评估梯度。

---

## 1.2 Paper2: SWA—Averaging Weights Leads to Wider Optima

**核心数学公式**

1. **SWA 权重平均**  
$$
   \boldsymbol{\theta}_{\text{SWA}} \;=\; \frac{1}{K}\sum_{k=1}^K \boldsymbol{\theta}_k,
$$
   其中 $\boldsymbol{\theta}_k$ 为训练后期不同迭代或多周期时刻的模型参数。

2. **平坦极小值解释**  
   - 并未直接定义“最坏扰动”，而是通过平均轨迹令模型落在更宽的极小值盆地：  
     $\min \mathcal{L}\bigl(\mathcal{D}^{tr}; \boldsymbol{\theta}_{\text{SWA}}\bigr)$.  
   - 类似对参数进行“低频滤波”，减少尖锐极小值敏感性。

**要点与推理**  
- 将 $\boldsymbol{\theta}$ 视为高维路径；在后期选取多个 $\boldsymbol{\theta}_k$ 并平均；  
- 与 SAM 不同，SWA 的实现成本接近普通SGD。

---

# 2. Improvement

## 2.1 ASAM: Adaptive Sharpness-Aware Minimization

**核心数学公式**

1. **自适应锐度定义**  
$$
   \max_{\|\mathbf{T}_{\boldsymbol{\theta}}^{-1}\boldsymbol{\epsilon}\|_p \le \rho} \,
   \mathcal{L}\bigl(\mathcal{D}^{tr};\,\boldsymbol{\theta}+\boldsymbol{\epsilon}\bigr),
$$
   其中 $\mathbf{T}_{\boldsymbol{\theta}}$ 为归一化算子，以平衡不同尺度的参数。

2. **更新**  
   - 扰动近似：  
$$
     \boldsymbol{\epsilon}^*(\boldsymbol{\theta}) \;\approx\;
     \rho \,\mathbf{T}_{\boldsymbol{\theta}}\,
     \frac{\nabla \mathcal{L}\bigl(\mathcal{D}^{tr}; \boldsymbol{\theta}\bigr)}
     {\bigl\|\mathbf{T}_{\boldsymbol{\theta}}\nabla \mathcal{L}(\boldsymbol{\theta})\bigr\|_q}.
     $$

**要点与推理**  
- 避免对所有层使用同一固定球形邻域；  
- 具备 scale-invariance，提升鲁棒性。

---

## 2.2 SWAD: Domain Generalization by Seeking Flat Minima

**核心数学公式**

1. **多域损失**  
   $$
   \mathcal{L}_{\text{multi}}(\boldsymbol{\theta}) \;=\;
   \frac{1}{M}\sum_{m=1}^M \,\mathcal{L}\bigl(\mathcal{D}_m^{tr}; \boldsymbol{\theta}\bigr).
   $$
   对 $M$ 个域数据联合训练。

2. **跨域平均**  
   $$
   \boldsymbol{\theta}_{\text{SWAD}} \;=\;
   \frac{1}{K}\sum_{k=1}^K \boldsymbol{\theta}_k^\text{(multi-domain)}.
   $$

**要点与推理**  
- 将 SWA 思想拓展到多域数据，减小域偏差对曲率的影响；  
- 不显式用“最坏扰动”，依靠轨迹平均抵御各域的差异。

---

## 2.3 LookSAM: Towards Efficient & Scalable SAM

**核心数学公式**

1. **间歇式最坏扰动**  
   - 与 SAM 同样采用  
     $$
     \max_{\|\boldsymbol{\epsilon}\|\le\rho}\mathcal{L}\bigl(\mathcal{D}^{tr};\boldsymbol{\theta}+\boldsymbol{\epsilon}\bigr),
     $$
   - 但只在每 $k$ 步时计算 $\boldsymbol{\epsilon}^*$，其它步普通梯度下降。

2. **示意**  
   - 若总迭代数 $T$，则仅在 $t \in \{k, 2k, 3k, \dots\}$ 做 SAM；其余步不做额外内循环。

---

## 2.4 SSAM: Sparsified Perturbation Approach

**核心数学公式**

1. **稀疏掩码**  
   $$
   \boldsymbol{\epsilon}^*(\boldsymbol{\theta}) \;=\;
   \rho \,\mathbf{m}\,\odot\,
   \frac{\nabla \mathcal{L}\bigl(\mathcal{D}^{tr}; \boldsymbol{\theta}\bigr)}
   {\bigl\|\nabla \mathcal{L}(\boldsymbol{\theta})\bigr\|_2},
   $$
   其中 $\mathbf{m}$ 是选定 $k\%$ 重要维度的掩码。

2. **更新**  
   - 同 SAM，但只在 $\mathbf{m}$ 所指示的维度上做扰动。

---

## 2.5 “An Adaptive Policy” & “SAF: Sharpness-Aware Training for Free”

**简要公式**

- **Adaptive Policy**：  
  $$
  \boldsymbol{\theta}_{t+1}\;=\;
    \begin{cases}
      \boldsymbol{\theta}_t - \alpha\,\nabla\mathcal{L}\bigl(\boldsymbol{\theta}_t\bigr), & \text{if not triggered},\\
      \boldsymbol{\theta}_t - \alpha\,\nabla\Bigl[\mathcal{L}\bigl(\boldsymbol{\theta}_t+\boldsymbol{\epsilon}^*\bigr)\Bigr], & \text{if triggered}.
    \end{cases}
  $$

- **SAF**：将 $\boldsymbol{\epsilon}$ 和 $\boldsymbol{\theta}$ 的更新合并在一次 BP 中以节约计算。

---

## 2.6 LPF-SGD: Low-Pass Filtering SGD

**核心数学公式**

$$
\tilde{\mathbf{g}}_{t} \;=\;
\alpha \,\tilde{\mathbf{g}}_{t-1}
\;+\;(1-\alpha)\,\nabla \mathcal{L}\bigl(\mathcal{D}^{tr}; \boldsymbol{\theta}_t\bigr),
\quad
\boldsymbol{\theta}_{t+1} \;=\;
\boldsymbol{\theta}_t \;-\;\eta\,\tilde{\mathbf{g}}_{t}.
$$

**要点与推理**  
- 通过对梯度序列低通滤波，减少在高曲率方向的“抖动”。  
- 等效地朝平坦区域收敛。

---

## 2.7 LETS: Learning Perturbation Radius

**核心数学公式**

1. **可学习 $\rho$**  
   $$
   \min_{\boldsymbol{\theta},\,\rho} \;\max_{\|\boldsymbol{\epsilon}\|\le\rho}
   \;\mathcal{L}\bigl(\mathcal{D}^{tr};\,\boldsymbol{\theta} + \boldsymbol{\epsilon}\bigr) \;+\; R(\rho).
   $$

2. **双层优化**  
   $$
   \rho^* \;=\; \arg\min_{\rho}\; \mathcal{L}_{\text{val}}\Bigl(\boldsymbol{\theta}^*(\rho)\Bigr), 
   \quad
   \boldsymbol{\theta}^*(\rho)\;=\;\arg\min_{\boldsymbol{\theta}}\;\max_{\|\boldsymbol{\epsilon}\|\le\rho}\,
     \mathcal{L}\bigl(\mathcal{D}^{tr}; \boldsymbol{\theta}+\boldsymbol{\epsilon}\bigr).
   $$

---

# 3. LoRA-Related

## 3.1 Paper1: Flat minima generalize for low-rank matrix recovery

**核心数学公式**

1. **低秩矩阵恢复**  
   $$
   \min_{\mathbf{X}:\,\mathrm{rank}(\mathbf{X})\le r}\;\mathcal{L}\bigl(\mathcal{D}^{tr};\,\mathbf{X}\bigr).
   $$

2. **平坦极小值在子空间**  
   - 分析 Hessian 在低秩子空间的特征值；  
   - 低曲率区域 $\Rightarrow$ 更鲁棒的矩阵重构。

---

## 3.2 Paper2: Implicit Regularization of SAM for Scale-Invariant Problems

**核心数学推理**

- 若 $\mathcal{L}(\boldsymbol{\theta}) = \mathcal{L}(c\,\boldsymbol{\theta})$ (“scale-invariance”)，则在最坏扰动意义下，SAM 引入了对 $\|\boldsymbol{\theta}\|$ 的某种隐式惩罚：
  $$
  \max_{\|\boldsymbol{\epsilon}\|\le\rho} \,\mathcal{L}(\boldsymbol{\theta}+\boldsymbol{\epsilon})
  \;\;\Rightarrow\;\;
  \text{偏向低范数解}.
  $$

---

## 3.3 Paper3: FLAT-LORA—Low-Rank Adaptation over a Flat Loss Landscape

**核心数学公式**

1. **LoRA 低秩分解**  
   $$
   \Delta W = A\,B^\top,\quad \mathrm{rank}(A)=r.
   $$

2. **最坏扰动仅限 LoRA 子空间**  
   $$
   \max_{\|\boldsymbol{\epsilon}\|\le\rho}\,\mathcal{L}\bigl(\mathcal{D}^{tr};\,W + \Delta W + \boldsymbol{\epsilon}\bigr),
   \quad
   \boldsymbol{\epsilon}\;\in\;\text{Span}(A,B^\top).
   $$

---

# 4. Analyse

## 4.1 Paper1: Towards Understanding SAM

**核心数学公式**

- 二阶泰勒展开  
  $$
  \mathcal{L}(\boldsymbol{\theta}+\boldsymbol{\epsilon}) \;\approx\;
  \mathcal{L}(\boldsymbol{\theta}) \;+\;
  \left\langle \nabla \mathcal{L}(\boldsymbol{\theta}),\,\boldsymbol{\epsilon}\right\rangle
  \;+\;\frac{1}{2}\,\boldsymbol{\epsilon}^\top \nabla^2 \mathcal{L}(\boldsymbol{\theta}) \,\boldsymbol{\epsilon}.
  $$
- 展示 SAM 如何直接影响 Hessian 最大特征值。

---

## 4.2 Paper2: Why Does SAM Generalize Better Than SGD?

- 通过 PAC-Bayes 或 Hessian 分析，指出 SAM 显式增加对高曲率方向的惩罚：  
  $$
  \max_{\|\boldsymbol{\epsilon}\|\le\rho}\;\ell(\boldsymbol{\theta}+\boldsymbol{\epsilon})
  \;\;\Rightarrow\;\;
  \text{更优的泛化界}.
  $$

---

## 4.3 Paper3: When Do Flat Minima Optimizers Work?

- 无新公式，系统比较  
  $$
  \min_{\boldsymbol{\theta}}\mathcal{L},\quad
  \min_{\boldsymbol{\theta}}\max_{\|\boldsymbol{\epsilon}\|\le\rho}\mathcal{L},\quad
  \text{and param averaging (SWA)}.
  $$
- 探讨何种网络/数据规模下效果显著。

---

## 4.4 Paper4: Revisiting Catastrophic Forgetting in Large Language Model Tuning

- 探究在多任务下  
  $$
  \mathcal{L}(\mathcal{D}^{tr_1}\cup\mathcal{D}^{tr_2}; \boldsymbol{\theta})
  $$
  的优化导致对原任务的遗忘；  
- 引入 Flat-Minima (SAM / SWA 等) 以减缓忘却现象。

---

## 结语

以上所有论文在统一的符号体系下主要体现为：

1. **SAM 类**：均基于 
   $$
   \max_{\|\boldsymbol{\epsilon}\|\le\rho}\,\mathcal{L}\bigl(\mathcal{D};\,\boldsymbol{\theta}+\boldsymbol{\epsilon}\bigr)
   $$
   的框架，通过改变邻域形状 (ASAM)、稀疏/随机扰动 (SSAM / RST)、自适应半径 (LETS) 或更新频率 (LookSAM) 等手段优化。
2. **SWA / SWAD**：通过轨迹或域间权重平均获得平坦解，不显式求最坏扰动。
3. **LoRA + Flat Minima**：仅在低秩子空间上执行局部或近似的最坏扰动；或分析低秩下 Hessian 的特征值以解释其泛化收益。
4. **分析部分**从 Hessian 光谱、泰勒展开、PAC-Bayes 视角说明上述方法为何能提升泛化与鲁棒性，并探讨适用性及在多任务/增量/对抗场景的价值。

SAM solves the minimax problem by iteratively applying the following two-step procedure for $t=0,1,2, \ldots$ as

$$
\left\{\begin{array}{l}
\epsilon_t=\rho \frac{\nabla L_S\left(\mathbf{w}_t\right)}{\left\|\nabla L_S\left(\mathbf{w}_t\right)\right\|_2} \\
\mathbf{w}_{t+1}=\mathbf{w}_t-\alpha_t\left(\nabla L_S\left(\mathbf{w}_t+\epsilon_t\right)+\lambda \mathbf{w}_t\right)
\end{array}\right.
$$

where $\alpha_t$ is an appropriately scheduled learning rate. This procedure can be obtained by a first order approximation of $L_S$ and dual norm formulation as

$$
\begin{aligned}
\epsilon_t & =\underset{\|\epsilon\|_p \leq \rho}{\operatorname{argmax}} L_S\left(\mathbf{w}_t+\epsilon\right) \\
& \approx \underset{\|\epsilon\|_p \leq \rho}{\operatorname{argmax}} \epsilon^{\top} \nabla L_S\left(\mathbf{w}_t\right) \\
& =\rho \operatorname{sign}\left(\nabla L_S\left(\mathbf{w}_t\right)\right) \frac{\left|\nabla L_S\left(\mathbf{w}_t\right)\right|^{q-1}}{\left\|\nabla L_S\left(\mathbf{w}_t\right)\right\|_q^{q-1}}
\end{aligned}
$$

and

$$
\begin{aligned}
\mathbf{w}_{t+1} & =\underset{\mathbf{w}}{\arg \min } L_S\left(\mathbf{w}+\epsilon_t\right)+\frac{\lambda}{2}\|\mathbf{w}\|_2^2 \\
& \approx \underset{\mathbf{w}}{\arg \min }\left(\mathbf{w}-\mathbf{w}_t\right)^{\top} \nabla L_S\left(\mathbf{w}_t+\epsilon_t\right)+\frac{\lambda}{2}\|\mathbf{w}\|_2^2 \\
& \approx \mathbf{w}_t-\alpha_t\left(\nabla L_S\left(\mathbf{w}_t+\epsilon_t\right)+\lambda \mathbf{w}_t\right)
\end{aligned}
$$

# 综述：结合统一符号对各论文方法的主要数学基础与推导进行总结

本文使用如下统一符号体系：
- **Datasets**：训练集记为 $\mathcal{D}^{tr}=\bigl\{(\mathbf{x}_i^{tr}, y_i^{tr})\bigr\}_{i=1}^{N^{tr}}$，验证集 $\mathcal{D}^{vl}$，测试集 $\mathcal{D}^{ts}$。
- **Model**：$f(\mathbf{x}; \mathbf{w})$ 表示模型函数，参数由 $\mathbf{w}$ 表示。
- **Loss Function**：
  $$
  \mathcal{L}(\mathcal{D}; \mathbf{w}) = \frac{1}{|\mathcal{D}|} \sum_{(\mathbf{x}_i, y_i) \in \mathcal{D}} \ell\bigl(f(\mathbf{x}_i; \mathbf{w}),\, y_i\bigr),
  $$
  其中 $\ell(\cdot,\cdot)$ 可以为交叉熵等具体形式。
- **Gradient / Hessian**：
  $$
  \nabla \mathcal{L}(\mathcal{D}; \mathbf{w}), \quad \nabla^2 \mathcal{L}(\mathcal{D}; \mathbf{w})
  $$
  分别表示对 $\mathbf{w}$ 的一阶与二阶梯度。

下文将结合上述符号，简要总结各论文的核心公式推导与关键思想。

---

## 1. **Origin Papers**

### 1.1 Paper1: SAM (Sharpness-Aware Minimization)
**核心思想**  
- 在每步更新中考虑 $\max_{\|\boldsymbol{\epsilon}\|\leq\rho} \mathcal{L}\bigl(\mathcal{D}^{tr};\, \mathbf{w} + \boldsymbol{\epsilon}\bigr)$，再最小化该最坏扰动下的损失。
- 数学上，目标可写为：
  $$
  \min_{\mathbf{w}}\, \Bigl[\, \max_{\|\boldsymbol{\epsilon}\|\le\rho} \mathcal{L}\bigl(\mathcal{D}^{tr};\, \mathbf{w} + \boldsymbol{\epsilon}\bigr)\Bigr].
  $$
- 实际实现：用一次一阶近似求取 
  $$
  \boldsymbol{\epsilon}^* \approx \rho \frac{\nabla \mathcal{L}(\mathcal{D}^{tr}; \mathbf{w})}{\|\nabla \mathcal{L}(\mathcal{D}^{tr}; \mathbf{w})\|}.
  $$
- 梯度更新则变为：
  $$
  \mathbf{w} \leftarrow \mathbf{w} - \eta \,\nabla \mathcal{L}\bigl(\mathcal{D}^{tr};\, \mathbf{w} + \boldsymbol{\epsilon}^*\bigr).
  $$

### 1.2 Paper2: SWA (Stochastic Weight Averaging)
**核心公式**  
- 在后期训练阶段，对不同迭代时刻的参数进行平均：
  $$
  \mathbf{w}_{\text{SWA}} = \frac{1}{K} \sum_{k=1}^{K} \mathbf{w}^{(k)},
  $$
  其中 $\mathbf{w}^{(k)}$ 为在不同 epoch（或 step）保存的模型权重。  
- 平坦极小值度量可用 Hessian 或插值实验：
  $$
  \nabla^2 \mathcal{L}(\mathcal{D}^{tr}; \mathbf{w}_{\text{SWA}}) \quad \text{具备更小的特征值}.
  $$

---

## 2. **Improvement Papers**

### 2.1 Paper1: ASAM (Adaptive Sharpness-Aware Minimization)
- 改进点：将 $\|\boldsymbol{\epsilon}\|\le\rho$ 换成 $\|\mathbf{T}_{\mathbf{w}}^{-1}\boldsymbol{\epsilon}\|\le\rho$，其中 $\mathbf{T}_{\mathbf{w}}$ 为可根据层或参数尺度自适应的算子。
  $$
  \max_{\|\mathbf{T}_{\mathbf{w}}^{-1}\boldsymbol{\epsilon}\|\le\rho} \mathcal{L}\bigl(\mathcal{D}^{tr};\, \mathbf{w} + \boldsymbol{\epsilon}\bigr).
  $$
- 实际更新近似：
  $$
  \boldsymbol{\epsilon}^* \approx \rho\, \mathbf{T}_{\mathbf{w}}\, \frac{\nabla\mathcal{L}(\mathcal{D}^{tr}; \mathbf{w})}{\|\mathbf{T}_{\mathbf{w}}\nabla\mathcal{L}(\mathcal{D}^{tr}; \mathbf{w})\|}.
  $$
- 解决了 SAM 对参数缩放敏感的问题，让锐度度量具备尺度不变性。

### 2.2 Paper SWAD 2: SWAD (Domain Generalization by Seeking Flat Minima)
- 在 **多域数据** $\{\mathcal{D}_1^{tr},\,\ldots,\, \mathcal{D}_m^{tr}\}$ 情形下，结合 SWA 思想对多域训练过程中的模型权重做平均：
  $$
  \mathbf{w}_{\text{SWAD}} = \frac{1}{K}\sum_{k=1}^K \mathbf{w}^{(k)}(\text{multiple domains}),
  $$
  并最小化跨域综合损失:
  $$
  \mathcal{L}_{\text{DG}}(\mathbf{w}) \;=\; \sum_{j=1}^m \alpha_j \,\mathcal{L}\bigl(\mathcal{D}_j^{tr}; \mathbf{w}\bigr).
  $$
- 得到在未知新域也更平坦、更具泛化的解。

### 2.3 Paper3: LookSAM (Towards Efficient and Scalable Sharpness-Aware Minimization)
- 核心：仅周期性地执行一次 SAM 的内层扰动；形式与普通 SAM 相同，但在大多数 step 没有计算 $\max_{\|\boldsymbol{\epsilon}\|\le\rho}$。
- 令训练分块 $(t_1, t_2,\dots)$，只在区间端点处执行：
  $$
  \boldsymbol{\epsilon}^{(k)} = \rho\, \frac{\nabla \mathcal{L}(\mathcal{D}^{tr}; \mathbf{w}^{(k)})}{\|\nabla \mathcal{L}(\mathcal{D}^{tr}; \mathbf{w}^{(k)})\|}.
  $$
- 其余步只做常规梯度下降。

### 2.4 Paper5: SSAM (Make SAM Stronger via Sparsified Perturbation)
- 定义稀疏扰动：只对一部分重要参数维度施加最大扰动。若记 $\mathbf{m}$ 表示掩码（1 表示扰动，0 表示冻结）：
  $$
  \max_{\|\mathbf{m}\cdot \boldsymbol{\epsilon}\|\le\rho} \mathcal{L}\bigl(\mathcal{D}^{tr}; \mathbf{w} + \mathbf{m}\cdot \boldsymbol{\epsilon}\bigr).
  $$
- 渐进式或固定稀疏度 $k\%$，计算量显著下降。

### 2.5 Paper6: An Adaptive Policy to Employ SAM
- 主要思想：在训练过程中动态决定是否执行 SAM 步骤/或执行多大扰动。
- 模型可写为类似：
  $$
  \mathbf{w}_{t+1} = \mathbf{w}_t - \eta\, \Bigl[\nabla \mathcal{L}\bigl(\mathcal{D}^{tr}; \mathbf{w}_t\bigr)\,\mathbf{1}_{\text{no-SAM}} + \nabla \mathcal{L}\bigl(\mathcal{D}^{tr}; \mathbf{w}_t + \boldsymbol{\epsilon}_t^*\bigr)\,\mathbf{1}_{\text{SAM}}\Bigr].
  $$
- 这里的 $\mathbf{1}_{(\cdot)}$ 表示根据某策略决定执行与否。

### 2.6 Paper7: SAF (Sharpness-Aware Training for Free)
- 借鉴 Free Adversarial Training 思路，尝试将 “寻找扰动” 与 “正常更新” 融合在一次更新中。
- 可能形式：  
  $$
  \boldsymbol{\epsilon}_t \leftarrow \boldsymbol{\epsilon}_{t-1} + \alpha \,\nabla_{\boldsymbol{\epsilon}} \ell\bigl(f(\mathbf{x}_i; \mathbf{w} + \boldsymbol{\epsilon}),\, y_i\bigr),
  $$
  并同期对 $\mathbf{w}$ 做更新，无需再做二次 forward-backward。

### 2.7 Paper4: LPF-SGD (Low-Pass Filtering SGD)
- 在每步更新中，对梯度做时序低通滤波：
  $$
  \tilde{g}_t = \alpha \tilde{g}_{t-1} + (1-\alpha) \nabla \mathcal{L}(\mathcal{D}^{tr}; \mathbf{w}_t),
  $$
  $$
  \mathbf{w}_{t+1} = \mathbf{w}_t - \eta \,\tilde{g}_t.
  $$
- 此平滑操作从信号处理角度减少对尖锐方向的过度拟合。

### 2.8 Paper9: LETS (Enhancing SAM by Learning Perturbation Radius)
- 令扰动半径 $\rho$ 成为可学习参数：  
  $$
  \rho \leftarrow \rho - \gamma \,\nabla_{\rho} \Bigl[\max_{\|\boldsymbol{\epsilon}\|\le\rho} \mathcal{L}(\mathcal{D}^{tr}; \mathbf{w} + \boldsymbol{\epsilon})\Bigr].
  $$
- 或以双层优化形式：外层更新 $\mathbf{w}$，内层更新 $\rho$，使得训练中 $\rho$ 能动态自适应数据与阶段变化。

### 2.9 Paper8: RST (Randomized Sharpness-Aware Training)
- 用随机近似替代精确最坏扰动：
  $$
  \boldsymbol{\epsilon}^* \approx \underset{\boldsymbol{\delta}\in\{\boldsymbol{\delta}_1,\dots,\boldsymbol{\delta}_k\}}{\arg\max} \ \mathcal{L}\bigl(\mathcal{D}^{tr};\, \mathbf{w} + \boldsymbol{\delta}\bigr),
  $$
  其中 $\boldsymbol{\delta}_i$ 为随机采样到的扰动向量，不再严格沿梯度方向。

---

## 3. **LoRA Related**

### 3.1 Paper1: Flat Minima Generalize for Low-Rank Matrix Recovery
- 主要分析在低秩优化问题中平坦极小值的优势，可设目标：
  $$
  \min_{\mathbf{W}:\,\operatorname{rank}(\mathbf{W})\le r} \mathcal{L}\bigl(\mathcal{D}^{tr};\, \mathbf{W}\bigr).
  $$
- 通过 Hessian 或其它二阶度量，证明在低秩子空间中，平坦解有更强的重构与泛化能力。

### Paper2: Implicit Regularization of SAM for Scale-Invariant Problems
- 设某些网络或问题满足“尺度不变”性质，即 $\alpha\,\mathbf{w}$ 与 $\mathbf{w}$ 有相同最优值。作者说明 SAM 的扰动可视为隐式正则：
  $$
  \max_{\|\boldsymbol{\epsilon}\|\le\rho} \mathcal{L}(\mathcal{D}^{tr}; \mathbf{w} + \boldsymbol{\epsilon}) \ \ \Rightarrow \ \ \text{倾向选择更低范数解}.
  $$

### Paper3: Flat-LoRA (Low-Rank Adaption over a Flat Loss Landscape)
- 在预训练模型微调中，仅对 LoRA 参数 $\Delta \mathbf{W}$ 做类似 SAM / SWA 处理：
  $$
  \Delta \mathbf{W}^{*} = \underset{\Delta \mathbf{W}}{\arg\min}\,\bigl[\max_{\|\boldsymbol{\epsilon}\|\le\rho} \mathcal{L}(\mathcal{D}^{tr};\, \mathbf{W}_0 + \Delta\mathbf{W} + \boldsymbol{\epsilon})\bigr],
  $$
  并限制 $\Delta \mathbf{W}$ 的秩。这样能有效减小运算量。

---

## 4. **Analysis Papers**

### 4.1 Paper1: Towards Understanding Sharpness-Aware Minimization
- 从二阶泰勒展开：
  $$
  \mathcal{L}(\mathbf{w} + \boldsymbol{\epsilon}) \approx \mathcal{L}(\mathbf{w}) + \nabla \mathcal{L}(\mathbf{w})^\top \boldsymbol{\epsilon} + \frac12 \boldsymbol{\epsilon}^\top \nabla^2 \mathcal{L}(\mathbf{w}) \,\boldsymbol{\epsilon}.
  $$
- 分析 SAM 如何降低 Hessian 最大特征值。

### 4.2 Paper2: Why Does SAM Generalize Better Than SGD?
- 对比 $\nabla \mathcal{L}(\mathbf{w})$ vs. $\nabla \mathcal{L}(\mathbf{w} + \boldsymbol{\epsilon}^*)$，说明后者“绕开”陡峭区域。
- 同时引用 PAC-Bayesian generalization bound 解释显式对尖锐度的抑制可以缩小泛化误差上界。

### 4.3 Paper3: When Do Flat Minima Optimizers Work?
- 系统实验比较 $\mathbf{w}_{\text{SWA}}$, $\mathbf{w}_{\text{SAM}}$, 以及普通 SGD，在不同数据规模/结构下的 Hessian / Loss Landscape。
- 得出结论：平坦极小值优化在中等规模数据+高容量网络场景下最为显著。

### 4.4 Paper4: Revisiting Catastrophic Forgetting in Large Language Model Tuning
- 将“平坦化”与大模型持续学习/增量学习结合，考虑：
  $$
  \min_{\mathbf{w}} \Bigl[\mathcal{L}\bigl(\mathcal{D}^{\text{old}}; \mathbf{w}\bigr) + \mathcal{L}\bigl(\mathcal{D}^{\text{new}}; \mathbf{w}\bigr) + \text{sharpness regularization}\Bigr].
  $$
- 证明若忽略平坦度，模型对新数据拟合会破坏原有知识；引入局部曲率约束能缓和遗忘。

---

## 5. **总结与联系**

1. **SAM / SWA 为起点**  
   - 定义了从最坏扰动或权重平均的角度寻找平坦极小值的两个典型公式：  
     - SAM：$\max_{\|\boldsymbol{\epsilon}\|\le\rho}\mathcal{L}$，  
     - SWA：对 $(\mathbf{w}^{(1)}, \dots, \mathbf{w}^{(K)})$ 均值。  
2. **Improvement Papers** 各自通过 **自适应扰动、稀疏化、低频滤波、随机近似** 等策略，对核心公式进行修正：  
   - ASAM 改邻域形状；LookSAM 改执行频率；SSAM 在扰动空间做稀疏；LETS 学习半径；RST 做随机近似；LPF-SGD 则从梯度平滑入手。  
3. **LoRA Related** 则将平坦极小值思想引入 **低秩子空间**：  
   - 分析低秩矩阵恢复的 Hessian 特性；  
   - 在大模型微调 (Flat-LoRA) 中，仅对 $\Delta \mathbf{W}$ 施加 $\max_{\|\epsilon\|\le\rho}$，节省计算量并提升泛化。  
4. **Analysis** 文献集中讨论二阶展开或泛化误差边界，从根本上解释何以“平坦极小值”方法能提高泛化、对抗鲁棒与防遗忘能力。

整体而言，这些论文在统一符号体系下，皆可视为对 **$\mathcal{L}(\mathcal{D}^{tr}; \mathbf{w})$ 的曲率（Hessian）进行显式/隐式调控**，在多种数据规模、网络结构、训练场景下收获更好的测试表现与鲁棒性。