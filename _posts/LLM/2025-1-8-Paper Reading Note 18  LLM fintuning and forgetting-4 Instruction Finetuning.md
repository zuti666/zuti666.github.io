---
layout: post
title:  Paper Reading 16 LLM finetuning and forgetting - 4 Instruction Finetuning
categories: [Paper Reading,  LLM, Continual Learning,] 
description:  [Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models]
keywords: [Paper Reading,  LLM, Continual Learning, ] 
---



# Paper Reading 18  LLM finetuning and forgetting - 4 Instruction Finetuning





首先第一篇论文介绍了 instrcution fintuning 的 基本概念，然后第二篇论文在不同的任务和数据集上设置了 contiuual learnig 的实验， 来实验探究  不同的大模型 不断 instrction finetunig 的影响。







- **FINETUNED LANGUAGE MODELS ARE ZERO-SHOT  LEARNERS**

  [`semanticscholar`](https://www.semanticscholar.org/paper/ff0b2681d7b05e16c46dfb71d980cc2f605907cd)  [`Paper`](https://www.semanticscholar.org/paper/ff0b2681d7b05e16c46dfb71d980cc2f605907cd)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fff0b2681d7b05e16c46dfb71d980cc2f605907cd%3Ffields%3DcitationCount)

  2021    International Conference on Learning Representations 



![image-20250112124206513](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250112124206584.png)

![image-20250112124255467](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250112124255534.png)



![](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250112124307582.png)
