---
layout: post
title:  Finsher信息矩阵
categories: math MachineLearning
description: 
keywords: math MachineLearning
---





# Finsher信息矩阵



在机器学习和深度学习中的目标通常是找到一组最优参数 $\theta^*$使得模型$f_\theta(X)$在 数据集$D$上的表现最佳。而在概率统计的框架下，我们可以使用**最大似然估计（Maximum Likelihood Estimation, MLE）**来描述这个优化过程

## 最大似然估计 MLE

在统计建模中，我们假设数据 $D$ 由某个概率分布生成，该分布的参数由 $\theta$ 控制。

因此，我们可以计算**数据在给定参数下的概率**，即 **似然函数 (Likelihood function)**：
$$
\ell(\theta) = p(D | \theta)
$$
对数似然函数 $\ell(\theta^*)$ 定义为观察到的数据的对数概率，它是关于参数$\theta$ 的函数

**最大似然估计 (MLE) 的目标**是找到最优参数 $\theta^*$，使得 $\ell(\theta)=p(D | \theta)$ 最大化：
$$
\theta^* =  \arg\max_{\theta} \ell(\theta)=\arg\max_{\theta} p(D | \theta) 
$$


当我们找到 $\theta^*$ 后，它的可靠程度如何？我们对这个估计值有多大信心？

## 2  二阶泰勒展开

在统计中，我们用 **参数的不确定性（uncertainty）** 来表示估计值是否稳定。也就是当估计值$\theta^*$发生变化的时候，其对应的似然函数的值的变化会有多大（也就是梯度或者方差的概念）。

![image-20250330175118608](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250330175118664.png)



我们可以从二阶导数（即**Hessian 矩阵**）的角度去理解这个问题。

设 $\ell(\theta)$ 为对数似然函数，我们在 $\theta^*$ 点对其进行二阶泰勒展开：
$$
\begin{aligned}
\ell(\theta)
 & \approx \ell(\theta^*) +\frac{\partial \ell}{\partial \theta} \bigg|_{\theta=\theta^*}(\theta-\theta^*) + \frac{1}{2}\frac{\partial^2 \ell}{\partial \theta^2} \bigg|_{\theta=\theta^*}(\theta-\theta^*)^2 \\



& \approx 

\ell(\theta^*) 

+ \frac{1}{2} (\theta - \theta^*)^\top H (\theta - \theta^*)

\end{aligned}
$$
其中 $H$ 是对数似然函数的Hessian矩阵：
$$
H =  \nabla^2 \ell(\theta) \bigg|_{\theta = \theta^*}
$$


在上面推导中，由于 $\theta^*$ 是最优点，所以其一阶导数为0，就只剩下了二阶导数。

> 推导证明 在 $\theta^*$处，一阶导数的期望值为0：
>
> 为了最大化对数似然函数，通常使用令梯度等于 0 的方法。梯度是对数似然函数关于参数$\theta$ 的偏导数。这个偏导数被称为 "Score function"（得分函数），通常记作 $U(\theta) $或 $\frac{\partial \ell}{\partial \theta} $。
>
> ###  得分函数(Score function)
>
> 得分函数（Score function）为对数似然函数关于参数的一阶导数。表示为:
> $$
> U(\theta) =\frac{\partial \ell}{\partial \theta} = \nabla_{\theta} \log ⁡p(x|\theta)
> $$
> 在最大化对数似然函数时，得分函数告诉我们在当前参数值下，增加或减少一个单位的参数值会对对数似然函数产生多大的变化。通常，通过将得分函数设置为零来找到对数似然函数的最大值。
>
> **值得注意的是，一方面，对于一个确定的得分函数，它是一个关于 θ 的函数；另一方面，对于得分函数本身，它是一个关于 x （观测到的数据点）的函数，因为每给 x 一个值，都等得到与之对应的得分函数。**
>
> 下面我们把 $x$ 作为自变量，得分函数作为因变量。即有映射：观测数据 $x$ → 得分函数 $U(\theta)$
>
> **性质：假设 $\theta$ 的真实值为** $\theta_0$ ，那么**得分函数的期望函数在 $\theta_0$ 处的值为 $0$。即**
> $$
> E[U(\theta, x)] \bigg|_{\theta=\theta_0} = 0
> $$
> 证明：
> $$
> \begin{aligned}
> E[U(\theta, x)] & =\int_{-\infty}^{+\infty} U(\theta, x) p\left(x \mid \theta_0\right) d x \\
> & =\int_{-\infty}^{+\infty} \nabla_\theta \log p(x \mid \theta) p\left(x \mid \theta_0\right) d x \\
> & =\int_{-\infty}^{+\infty} \frac{\nabla_\theta p(x \mid \theta)}{p(x \mid \theta)} p\left(x \mid \theta_0\right) d x \\
> \left.E[U(\theta, x)]\right|_{\theta=\theta_0} & =\int_{-\infty}^{+\infty} \frac{\nabla_\theta p(x \mid \theta)}{p\left(x \mid \theta_0\right)} p\left(x \mid \theta_0\right) d x \\
> & =\int_{-\infty}^{+\infty} \nabla_\theta p(x \mid \theta) d x \\
> & =\nabla_\theta \int_{-\infty}^{+\infty} p(x \mid \theta) d x \\
> & =\nabla 1 \\
> & =0
> \end{aligned}
> $$
> 



对于二阶导数，我们除了有Hessian矩阵的定义，我们还可以将其定义为Finsher信息矩阵如下：

对于二阶导数
$$
\frac{\partial^2 \ell}{\partial \theta^2}
$$
对于似然函数$\ell$ 我们更喜欢用它的对数形式 ，对数似然函数 $\log \ell$ ,于是可以写作
$$
\frac{\partial^2 \log \ell }{\partial \theta^2}
$$
我们可以将其更加泛化到多参数的空间，我们有
$$
\frac{\partial \log \ell}{\partial \theta_\alpha}  \frac{\partial \log \ell}{\partial \theta_\beta} = -F
$$
上面的 F 就是 Finsher信息矩阵 

## Finsher 信息矩阵定义

设数据 $x$ 来自某个分布 $p(x | \theta)$，对数似然函数为：
$$
\ell(\theta) = \log p(x | \theta)
$$
**Fisher 信息矩阵 $F(\theta)$ 有两种等价定义：**

### ✅ 定义一（梯度外积的期望）：

$$
F(\theta) := \mathbb{E}_{x \sim p(x|\theta)} \left[ \nabla \log p(x | \theta) \cdot \nabla \log p(x | \theta)^\top \right]
$$

这就是经典的 **期望形式**，本质上是梯度的方差：

> Fisher 信息 = 似然函数梯度的二阶动差（协方差）







它也可以视为负二阶导数的期望值：

### ✅ 定义二（负二阶导的期望）：

$$
F(\theta) := - \mathbb{E}_{x \sim p(x|\theta)} \left[ \nabla^2 \log p(x | \theta) \right] =  -\mathbb{E}[H_{\log p(x|\theta)}]
$$

两种形式在正则条件下是等价的。

## 两种定义的等价推导

###  📘 步骤 1：利用概率的性质

我们知道对所有 $x$，概率密度积分为 1：
$$
\int p(x|\theta) dx = 1
$$
对上式两边对 $\theta$ 求导：
$$
\frac{\partial}{\partial \theta} \int p(x|\theta) dx = \int \frac{\partial p(x|\theta)}{\partial \theta} dx = 0
$$
再对 $\theta$ 求一次导数：
$$
\frac{\partial^2}{\partial \theta^2} \int p(x|\theta) dx = \int \frac{\partial^2 p(x|\theta)}{\partial \theta^2} dx = 0
$$
这是后面将用到的 **正规化条件**。

### 📘 步骤 2：对梯度外积形式进行分析

我们将对数似然梯度平方项展开期望：
$$
\mathbb{E}_{x \sim p(x|\theta)} \left[ \nabla_\theta \log p(x|\theta) \cdot \nabla_\theta \log p(x|\theta)^\top \right]
$$
注意：

$\nabla_\theta \log p(x|\theta) = \frac{\nabla_\theta p(x|\theta)}{p(x|\theta)}$

因此：
$$
\mathbb{E} \left[ \nabla \log p(x|\theta) \nabla \log p(x|\theta)^\top \right] = \int p(x|\theta) \cdot \left( \frac{\nabla_\theta p(x|\theta)}{p(x|\theta)} \right) \left( \frac{\nabla_\theta p(x|\theta)}{p(x|\theta)} \right)^\top dx
$$
简化得：
$$
\int \frac{\nabla_\theta p(x|\theta) \nabla_\theta p(x|\theta)^\top}{p(x|\theta)} dx
$$
这就是 **梯度平方项的期望**。

### 📘 步骤 3：对负二阶导数形式进行分析

考虑对 $\log p(x|\theta)$ 的 Hessian：
$$
\nabla^2 \log p(x|\theta) = \frac{\nabla^2 p(x|\theta)}{p(x|\theta)} - \frac{\nabla p(x|\theta) \nabla p(x|\theta)^\top}{p(x|\theta)^2}
$$
因此：
$$
- \nabla^2 \log p(x|\theta) = - \frac{\nabla^2 p(x|\theta)}{p(x|\theta)} + \frac{\nabla p(x|\theta) \nabla p(x|\theta)^\top}{p(x|\theta)^2}
$$
两边对 $x$ 积分取期望：
$$
- \mathbb{E} \left[ \nabla^2 \log p(x|\theta) \right] = - \int \frac{\nabla^2 p(x|\theta)}{p(x|\theta)} p(x|\theta) dx + \int \frac{\nabla p(x|\theta) \nabla p(x|\theta)^\top}{p(x|\theta)^2} p(x|\theta) dx
$$
根据前面的正规化条件,第一个项化简为：

$- \int \nabla^2 p(x|\theta) dx = 0$

于是我们只剩下第二项：

$\mathbb{E} \left[ \nabla \log p(x|\theta) \nabla \log p(x|\theta)^\top \right]$

于是有
$$
- \mathbb{E} \left[ \nabla^2 \log p(x|\theta) \right] = \mathbb{E} \left[ \nabla \log p(x|\theta) \nabla \log p(x|\theta)^\top \right]
$$
✅ 这就证明了两种 Fisher 信息定义是等价的！



##  Finsher 信息矩阵与 Hessian 矩阵的关系

| 特性       | Hessian 矩阵            | Fisher 信息矩阵                                              |
| ---------- | ----------------------- | ------------------------------------------------------------ |
| 表达式     | $\nabla^2 \ell(\theta)$ | $- \mathbb{E}_{x \sim p(x|\theta)} \left[ \nabla^2 \log p(x |\theta) \right]$ |
| 随数据波动 | 是（依赖样本）          | 否（取期望）                                                 |
| 正定性     | 不一定                  | 恒为半正定                                                   |
| 稳定性     | 可能噪声大              | 更平滑稳健                                                   |
| 应用       | 贝叶斯近似中的 Laplace  | 自然梯度、EWC、变分推断等                                    |

> 结论：**Fisher 信息是 Hessian 的近似期望形式，更容易估计，也更稳定。**

## Finsher信息矩阵与协方差矩阵

方差表示随机变显偏离均值的程度。随机变量 $X$ 的方差表示为

$$
\begin{aligned}
\operatorname{Var}(X) & =E\left[(X-E(X))^2\right] \\
& =E\left(X^2\right)-E(X)^2
\end{aligned}
$$


协方差是方差概念的延申。随机变量 $X$ 和随机变量 $Y$ 的协方差表示为 $X$ 的偏差 $X-E(X)$ 与 $Y$ 的偏差 $Y-E(Y)$ 乘积的数学期望。

$$
\begin{aligned}
\operatorname{Cov}(X, Y) & =E[(X-E(X))(Y-E(Y))] \\
& =E(X Y)-E(X) E(Y)
\end{aligned}
$$


记 $n$ 维随机变量为 $X=\left(x_1, x_2, \ldots, x_n\right)^{\prime}$ ，若其每个分量的数学期望都存在，则 $X$ 的期望为

$$
E(X)=\left(E\left(X_1\right), E\left(x_2\right), \ldots, E\left(X_n\right)\right)^{\prime}
$$

$X$ 的方差－协方差矩阵为

$$
\begin{aligned}
\operatorname{Cov}(X) & =E\left[(X-E(X))(X-E(X))^{\prime}\right] \\
& =E\left(X X^{\prime}\right)-E(X) E(X)^{\prime} \\
& =\left[\begin{array}{cccc}
\operatorname{Var}\left(X_1\right) & \operatorname{Cov}\left(X_1, X_n\right) & \cdots & \operatorname{Cov}\left(X_1, X_n\right) \\
\operatorname{Cov}\left(X_2, X_1\right) & \operatorname{Var}\left(X_2\right) & \cdots & \operatorname{Cov}\left(X_2, X n\right) \\
\vdots & \vdots & \ddots & \vdots \\
\operatorname{Cov}\left(X_n, X_1\right) & \operatorname{Cov}\left(X_n, X_2\right) & \cdots & \operatorname{Var}\left(X_n\right)
\end{array}\right]
\end{aligned}
$$

 在 $ \theta=\theta_0$ 时，Finsher信息矩阵$F$就是得分函数$U(\theta)$的协方差矩阵。 
$$
\begin{aligned}
& E\left[\left(U(\theta, x)-\left.E(U(\theta, x))\left(U(\theta, x)-E(U(\theta, x))^{\prime}\right]\right|_{\theta=\theta_0}\right.\right. \\
= & \left.E\left[U(\theta, x) U(\theta, x)^{\prime}\right]\right|_{\theta=\theta_0}-\left.E(U(\theta, x)) E(U(\theta, x))^{\prime}\right|_{\theta=\theta_0} \\
= & \left.E\left[U(x \mid \theta) U(x \mid \theta)^{\prime}\right]\right|_{\theta=\theta_0}-0 \\
= & \left.E\left[U(x \mid \theta) U(x \mid \theta)^{\prime}\right]\right|_{\theta=\theta_0} \\
= & F\left(\theta_0\right)
\end{aligned}
$$


## Finshe信息矩阵与贝叶斯估计的关系

在贝叶斯估计中，我们不仅最大化似然 $p(D|\theta)$，还考虑先验分布 $p(\theta)$：
$$
\log p(\theta | D) = \log p(D | \theta) + \log p(\theta) + C
$$
于是我们最大化 **对数后验（Log-Posterior）**：
$$
\theta^*_{\text{MAP}} = \arg\max_\theta \log p(D | \theta) + \log p(\theta)
$$
拉普拉斯近似下，我们对 $\log p(\theta | D)$ 进行二阶展开，近似为高斯：
$$
p(\theta | D) \approx \mathcal{N}(\theta^*_{\text{MAP}}, \Sigma)
$$
其中：
$$
\Sigma^{-1} = - \nabla^2 \log p(\theta | D) \Big|_{\theta = \theta^*_{\text{MAP}}}
$$
**在先验为高斯、似然为条件独立情形下，这个 Hessian 可以很好地被 Fisher 信息矩阵近似**，从而使得：
$$
\Sigma^{-1} \approx \mathcal{F}(\theta^*)
$$
这一部分也就是 EWC中提到的 使用Finshe信息矩阵来近似 Hessian矩阵。



## Finsher 信息矩阵的对角近似

计算完整的 Hessian 是非常耗费计算资源的，特别是在神经网络中。所以我们进一步简化，将其**近似为 Fisher 信息矩阵的对角形式**：

### Fisher 信息矩阵（对角近似）：

$$
F_i = \mathbb{E}_{x \sim D_A} \left[ \left( \frac{\partial \log p(y|x, \theta)}{\partial \theta_i} \right)^2 \right]
$$

这个近似有三个优点：

1. **只用一阶导数（梯度）即可计算**，不需要昂贵的二阶导；
2. **与损失函数的二阶导数在极值点附近是等价的**；
3. 是一个 **正定矩阵**，天然可以构造概率分布。

因此，我们用对角化的 Fisher 信息矩阵来近似Hessian 矩阵

### 高斯近似后验（EWC 中的表达）：

$p(\theta | D_A) \approx \mathcal{N}(\theta^*_A, \text{diag}(F)^{-1})$

这意味着：

- 每个参数 $\theta_i$ 的方差近似为 $\sigma_i^2 = 1 / F_i$；
- 参数 $\theta_i$ 越重要（即 $F_i$ 越大），我们对它的不确定性就越小（方差越小）；
- 后续学习（例如任务 B）应尽可能避免改变这类重要参数。



## 📌 六、总结要点

- Fisher 信息矩阵度量了**参数对数据分布的敏感性**；
- 它是对**损失函数曲率（Hessian）的期望近似**；
- 在极大似然估计中，它给出参数估计的**方差下界**（Cramér-Rao bound）；
- 在贝叶斯估计中，它可以用来**近似后验分布的协方差**，用于拉普拉斯近似；
- 在机器学习中，它被广泛用于：**自然梯度下降（NGD）**、**弹性权重巩固（EWC）**、**贝叶斯神经网络中的不确定性估计**等。
