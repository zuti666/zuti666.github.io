---
layout: post
title:  二阶曲率三 权重二阶泰勒展开
categories: math MachineLearning
description: 
keywords: math MachineLearning
---





# 理解 二阶曲率三 权重二阶泰勒展开



# 二阶泰勒展开



令损失函数 $L:\mathbb{R}^d\to\mathbb{R}$ 在点 $\theta$ 处二阶可微，扰动向量 $\epsilon\in\mathbb{R}^d$。记

$g \;=\; \nabla_\theta L(\theta)\in\mathbb{R}^d,\qquad H \;=\; \nabla^2_\theta L(\theta)\in\mathbb{R}^{d\times d}.$

则
$$
\boxed{\; L(\theta+\epsilon) = L(\theta) + g^{\top}\epsilon + \tfrac12\,\epsilon^{\top}H\,\epsilon + R_3(\theta,\epsilon)\; }
$$
其中余项（third-order remainder）满足以下两级精度结论（用 $\|\cdot\|$ 表示欧氏/谱范数）：

1. **弱条件（$C^2$ 连续）**：若 $\nabla^2 L$ 在 $\theta$ 连续，则
   $$
   R_3(\theta,\epsilon)=o(\|\epsilon\|^2)\qquad(\epsilon\to 0).
   $$
   

2. **强条件（Hessian $M$-Lipschitz）**：若存在常数 $M\ge 0$ 使得

$$
\|\nabla^2 L(\theta)-\nabla^2 L(\theta')\|\ \le\ M\,\|\theta-\theta'\|\quad\forall\,\theta,\theta',
$$

则 $R_3$ 具有**显式三次界**（Nesterov 型）：
$$
\;|R_3(\theta,\epsilon)|\ \le\ \tfrac{M}{6}\,\|\epsilon\|^3\; 
$$
这一定量上保证了二次近似在小步长下的可控误差





## 一阶项：方向与幅度（陡增/陡降）

**结论**：在给定步长约束下，线性项 $g^{\top}\epsilon$ 在 $\epsilon$ 与 $g$ **同向**时最大，在 $\epsilon$ 与 $g$ **反向**时最小。

- 用柯西–施瓦茨不等式：
  $$
  g^{\top}\epsilon\ \le\ \|g\|\,\|\epsilon\|,\quad \text{当且仅当 }\epsilon=\alpha g\ (\alpha>0)\text{取等号}.
  $$
  在 $\|\epsilon\|=\eta$ 的球面上，最大值为 $+\|g\|\eta$（$\epsilon=+\eta\,g/\|g\|$），最小值为 $-\|g\|\eta$（$\epsilon=-\eta\,g/\|g\|$）。

- **优化含义**：梯度下降选择 $\epsilon=-\eta g$，正是把一阶增量推到其下界；当 $\|g\|$ 很大时，一阶项主导近似，步长需要更谨慎（线搜索/自适应学习率）。

## 二阶项：曲率的方向性与谱界

令 Hessian 的特征分解 $H=U\Lambda U^{\top}$（$\Lambda=\mathrm{diag}(\lambda_1,\dots,\lambda_d)$），将 $\epsilon$ 在特征基展开为 $\epsilon=\sum_i \alpha_i u_i$。则
$$
\epsilon^{\top}H\epsilon=\sum_{i=1}^d \lambda_i\,\alpha_i^2 = \|\epsilon\|^2\sum_{i=1}^d \lambda_i\,\Big(\frac{u_i^{\top}\epsilon}{\|\epsilon\|}\Big)^{\!2}.
$$

- 这表明“二阶项 = 各特征方向曲率 $\lambda_i$ 的加权平均”，权重是方向投影的平方。

- **谱界 / Rayleigh 商**：
  $$
  \lambda_{\min}(H)\,\|\epsilon\|^2 \ \le\ \epsilon^{\top}H\epsilon\ \le\ \lambda_{\max}(H)\,\|\epsilon\|^2.
  $$
  取上界即你写的 $|\epsilon^{\top}H\epsilon|\le\|H\|\,\|\epsilon\|^2=\lambda_{\max}(H)\,\|\epsilon\|^2$（当 $H\succeq0$ 时可去绝对值）。

- **几何解释**：
   沿最大特征向量 $u_{\max}$ 方向（$\epsilon\parallel u_{\max}$）的曲率最大，沿最小特征向量 $u_{\min}$ 方向最小。值“大”表示该方向上“尖锐”，值“小”表示“平坦”。

# Taylor 近似的界

给定步长 $\|\epsilon\|=\eta$，二阶近似给出
$$
\Delta L \approx g^{\top}\epsilon + \tfrac12\,\epsilon^{\top}H\epsilon.
$$
用上述上界可得一个“保守估计”
$$
\Delta L \ \le\ \|g\|\,\eta + \tfrac12\,\lambda_{\max}(H)\,\eta^2,
$$
再结合三阶余项界（若 Hessian 为 $M$-Lipschitz）：
$$
|\text{余项}|\ \le\ \tfrac{M}{6}\,\eta^3.
$$
**含义**：

- 当 $\|g\|$ 占优时，一阶项主导；当 $\|g\|$ 已很小（驻点附近），二阶项决定增减与速度（正定则上凸，存在负特征值则为鞍点）。
- **学习率/步长选择**与 $\lambda_{\max}(H)$ 强相关：二次模型下梯度下降稳定需要 $\eta<2/\lambda_{\max}(H)$；$\kappa=\lambda_{\max}/\lambda_{\min}$ 大说明各向异性强、收敛慢，需要预条件或二阶信息（如牛顿/准牛顿、K-FAC）。

# 常见极端情形的直观判断

- **平坦区**：$\|g\|$ 小且 $\lambda_{\max}$ 小，$\Delta L$ 对小扰动不敏感（“flat minimum”）。
- **尖锐区**：$\lambda_{\max}$ 大，即使 $\|g\|$ 小，$\tfrac12\,\epsilon^{\top}H\epsilon$ 也可能显著，说明对微小参数扰动敏感（泛化风险较高的典型信号）。
- **鞍点**：存在负特征值，沿某些方向二阶项为负，可借助噪声/动量/二阶法逃逸。
