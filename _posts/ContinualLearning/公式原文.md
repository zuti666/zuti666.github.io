The relationship between flat minima and generalization has emerged as a central issue in deep learning theory~\citep{hinton1993keeping,FlatMinima1997,keskarLargeBatchTrainingDeep2017}. Empirical and theoretical studies consistently show that convergence to flat regions of the loss landscape yields models with improved robustness, transferability, and generalization by reducing sensitivity to parameter perturbations~\citep{garipovLossSurfacesMode2018, foretSharpnessAwareMinimizationEfficiently2021, izmailovAveragingWeightsLeads2019}. Optimization techniques such as Sharpness-Aware Minimization (SAM),Gradient norm Aware Minimization (GAM) explicitly promote flat minima, delivering notable generalization gains, particularly in large-scale and fine-tuning regimes~\citep{foretSharpnessAwareMinimizationEfficiently2021, chenWhenVisionTransformers2022,zhangGradientNormAware2023}. Furthermore, recent work indicates that flat minima can alleviate catastrophic forgetting in continual learning~\citep{mirzadehUnderstandingRoleTraining2020}. However, despite these advances, recent critiques highlight that the precise and intrinsic connection between flat minima and generalization remains unresolved\citep{jakubovitzGeneralizationErrorDeep2019}, particularly due to the lack of a parameterization-invariant definition of flatness and the ambiguous causal relationship between landscape geometry and model generalizability.  The fundamental relationship between flat minima and generalization remains an open problem.





keskarLargeBatchTrainingDeep2017

garipovLossSurfacesMode2018, foretSharpnessAwareMinimizationEfficiently2021, izmailovAveragingWeightsLeads2019

foretSharpnessAwareMinimizationEfficiently2021, chenWhenVisionTransformers2022,zhangGradientNormAware2023

mirzadehUnderstandingRoleTraining2020

jakubovitzGeneralizationErrorDeep2019









根据原文和前后文，这个**归一化sharpness (Normalized Sharpness, NS)** 公式的每个符号含义如下：

------

### 公式原文

$\mathrm{NS} = \sum_{l=1}^d \inf_{\bar{\gamma}^{(l)} \in \mathbb{R}^{h_1^{(l)}}} \sqrt{ \exp\left( \left[\bar{\gamma}^{(l)}\right]^{\top}  \right) \bar{F}^{(l)} \left(\bar{U}^{(l)}\right)^{\top}  \exp\left( -\bar{\gamma}^{(l)} \right) }$

------

### 各个符号解释

- $\mathrm{NS}$：**Normalized Sharpness**，归一化sharpness指标，衡量最优节点缩放下的曲率（平坦度/尖锐度）。
- $d$：网络层数，$l=1,\ldots,d$ 表示对每一层单独求和。
- $\inf_{\bar{\gamma}^{(l)} \in \mathbb{R}^{h_1^{(l)}}}$：对每一层$l$，在节点级别的缩放变量 $\bar{\gamma}^{(l)}$ 上取最小值，$h_1^{(l)}$ 是该层的节点（神经元）个数。
- $\bar{\gamma}^{(l)}$：第$l$层的节点缩放参数，$\bar{\gamma}^{(l)} \in \mathbb{R}^{h_1^{(l)}}$，控制每个神经元的缩放（类似于$\log$尺度）。
- $\bar{F}^{(l)}$：第$l$层的**归一化Hessian（或近似Hessian，实际常为对角块或Fisher信息近似）**，具体含义见正文第6节；本质是曲率信息的矩阵表达。
- $\bar{U}^{(l)}$：第$l$层的归一化权重平方范数，代表该层神经元权重规模，见第6节定义。
- $\exp\left( \left[\bar{\gamma}^{(l)}\right]^{\top} \bar{F}^{(l)} \left(\bar{U}^{(l)}\right)^{\top} \right)$：节点缩放作用下的曲率调整项。通过调整$\bar{\gamma}^{(l)}$，在指数意义下重新缩放曲率指标。
- $\exp\left( -\bar{\gamma}^{(l)} \right)$：缩放的惩罚项，保证归一化sharpness度量不会因为任意缩放而被人为缩小。
- $\sqrt{\cdot}$：最终归一化sharpness以平方根输出，保证量纲与原sharpness一致。

------

### **公式含义与作用**

- 对于每一层，选择一组节点缩放参数$\bar{\gamma}^{(l)}$，使得缩放下的曲率和权重范数的组合最小化。
- 该过程本质是“在所有等价缩放变换下寻找最平坦的网络表示”，排除简单缩放操作对曲率指标的影响，使指标具有节点缩放不变性。
- 求和后整体$\mathrm{NS}$，即可衡量整个网络的归一化平坦度/尖锐度。

------

### **总结**

- **归一化sharpness NS** 用于消除网络参数简单缩放带来的度量歧义，从而更公平地反映不同模型、不同初始化、不同训练方法下的实际曲率。
- 它在PAC-Bayesian分析和实际经验中都能更好地关联泛化误差，是对传统sharpness度量的重要理论补充。

------

如需进一步给出每个量的原文定义引用或数学化推导，可继续细化！



We then discuss the relationship between the first-order flatness and the maximal eigenvalue of the Hessian matrix $\nabla^2 \hat{L}\left(\boldsymbol{\theta}^*\right)$ (denoted as $\lambda_{\max }\left(\nabla^2 \hat{L}\left(\boldsymbol{\theta}^*\right)\right)$ ). $\lambda_{\max }$ is proven to be a proper measure of the curvature of minima [32,33] and is closely related to generalization abilities $[10,28,58]$. As another definition of flatness in related works [8, 40], $\lambda_{\text {max }}$ is widely accepted yet hard to calculate. We show in the following lemma that given a radius $\rho$, the first-order flatness controls $\lambda_{\text {max }}$, which reinforces the validity of the first-order flatness.

Lemma 4.1. Let $\boldsymbol{\theta}^*$ be a local minimum of $\hat{L}$. Suppose $\hat{L}$ can be second-order Taylor approximated in the neighbourhood $B\left(\boldsymbol{\theta}^*, \rho\right)^2$, i.e., $\forall \boldsymbol{\theta} \in B\left(\boldsymbol{\theta}^*, \rho\right), \hat{L}(\boldsymbol{\theta})=\hat{L}\left(\boldsymbol{\theta}^*\right)+(\boldsymbol{\theta}-$ $\left.\boldsymbol{\theta}^*\right)^{\top} \nabla^2 \hat{L}\left(\boldsymbol{\theta}^*\right)\left(\boldsymbol{\theta}-\boldsymbol{\theta}^*\right) / 2$. Then

$$
\lambda_{\max }\left(\nabla^2 \hat{L}\left(\boldsymbol{\theta}^*\right)\right)=\frac{R_\rho^{(1)}\left(\boldsymbol{\theta}^*\right)}{\rho^2}
$$