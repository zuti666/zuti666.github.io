---
layout: post
title:  Paper Reading 5 Continual learning LLM1
categories: [Paper Reading, Continual Learning, LLM ] 
description: [Investigating Forgetting in Pre-Trained Representations Through Continual Learning]
keywords: [Paper Reading, Continual Learning, LLM ] 

---

# Paper Reading 5   Continual learning LLM1

# 论文信息
Investigating Forgetting in Pre-Trained Representations Through Continual Learning


# AI 概括

### 概括分析：预训练表示在持续学习中的遗忘问题研究

#### **1. 范围 (Scope) 和背景 (Setting)**

本论文聚焦于 **持续学习 (Continual Learning)** 环境下的 **预训练语言模型 (Pretrained Language Models, PLMs)** 的表现，重点分析 **表示遗忘 (Representation Forgetting)** 对模型通用性和知识保留的影响。主要目标是研究 PLMs 在处理下游任务时如何遗忘通用知识，并探索缓解遗忘的策略。

------

#### **2. 核心思想 (Key Idea)**

论文提出：

- **表示遗忘** 会破坏预训练模型的通用性和语法语义知识，从而影响未来任务性能。
- 通过设计新的指标，包括 **通用性破坏 (Generality Destruction, GD)**、**语法知识遗忘 (SynF)** 和 **语义知识遗忘 (SemF)**，量化模型知识的丢失程度。
- 引入混合持续学习策略以减少知识遗忘并维持通用性。

------

#### **3. 方法 (Method)**

1. 度量指标设计
   - **GD (Generality Destruction)**：通过比较持续学习模型与单任务训练模型的表现，衡量通用性损失。
   - **SynF 和 SemF**：通过探测任务（如语法树深度、语义一致性）衡量语法和语义知识遗忘。
2. 实验设置
   - **任务序列设计**：采用 GLUE 和 MBPA++ 基准中的分类任务，探索不同任务顺序对遗忘的影响。
   - **模型对比**：评估了多种预训练模型（如 BERT、DistilBERT、ALBERT 和 RoBERTa）。
   - **持续学习方法**：比较回放策略 (ER)、正则化方法 (LwF) 和混合方法 (DERPP)。

------

#### **4. 贡献 (Contribution)**

1. 首次提出通用性破坏的概念，系统分析表示遗忘对预训练模型通用性的影响。
2. 设计语法与语义知识的遗忘指标，量化不同知识的丢失程度。
3. 证明混合持续学习方法（DERPP）可以有效缓解遗忘并提升性能。
4. 揭示任务序列对知识保留的显著影响，提出先学习通用语言学任务的策略。

------

#### **5. 创新与差异 (Difference and Innovation)**

- 通过全新的知识遗忘指标和探测任务框架，提供了比传统精度更细粒度的模型评估工具。
- 系统比较了多种持续学习方法在不同任务序列下的表现，填补了以往工作中任务顺序影响的研究空白。

------

#### **6. 结果与结论 (Results and Conclusion)**

1. **通用性破坏**：
   - 所有模型在持续学习中均表现出通用性损失，RoBERTa 通用性最强，但遗忘程度高。
   - BERT 对通用知识的保留能力最强，ALBERT 表现最弱。
2. **语法与语义遗忘**：
   - 语法知识比语义知识更容易被遗忘（SynF > SemF）。
   - 语法和语义知识的遗忘存在显著正相关性。
3. **任务序列影响**：
   - 先学习通用语言学任务（如 CoLA）有助于缓解遗忘。
   - 任务顺序显著影响知识遗忘程度，合理设计任务序列可提升模型通用性。
4. **持续学习策略**：
   - 混合策略 DERPP 在减缓遗忘和提高任务性能方面最有效。
   - 回放方法 (ER) 容易引发更大的通用性破坏，正则化方法 (LwF) 虽保护知识，但对新任务适应性较差。

------

#### **7. 与其他方法的对比 (Contrast with Other Methods)**

- 单任务训练能够避免遗忘，但无法适应多任务场景。
- 基于正则化的方法保护知识效果好，但影响新任务性能。
- 基于回放的方法缓解遗忘效果有限，但能平衡新旧任务。
- 混合方法综合两者优点，提供更稳定的持续学习性能。

------

#### **8. 讨论 (Discussion)**

- 论文指出现有持续学习策略在应对复杂任务序列时仍存在局限性。
- 提出未来需要研究任务分组、动态序列设计，以及更高效的知识注入方法。

------

#### **9. 证明 (Proof)**

- 实验中通过多模型、多任务、多方法和多指标验证了论文提出的假设。
- 探测任务显示混合策略有效保留了语法和语义知识，同时提升了任务适应能力。

------

### 总结

本论文通过细粒度的分析框架揭示了预训练表示在持续学习中的遗忘机制，提出了缓解遗忘的策略和方法，为持续学习领域的研究提供了系统性参考。这项工作在理论分析和实际应用中均具有重要意义。

# AI 交互



## 问题一 文章主要结论

> 文中提出的三个问题的答案是什么，以及是如何回答这几个问题的？ In this paper, we draw attention to the following fundamental questions of representation forgetting in pre-trained LMs:  1. Is the generality of the pre-trained LMs destructed through representation forgetting?  2. What types of general knowledge of the representation are saliently forgotten?  3. How can we mitigate the general knowledge forgetting of representations?





论文围绕 **预训练语言模型（PLMs）中的表示遗忘（Representation Forgetting）** 提出了三个关键问题，并通过实验证实和解答了这些问题：

------

### **问题 1：预训练语言模型的通用性是否因表示遗忘而被破坏？**

#### **解答**：

实验表明，表示遗忘确实导致了预训练语言模型通用性的显著破坏：

1. 使用 **Generality Destruction (GD)** 指标比较单任务训练和持续学习模型的性能，发现持续学习过程中模型的通用性显著下降。
2. 所有模型均表现出通用性的丢失，不同模型的遗忘程度差异显著：
   - RoBERTa 的通用性最强，但也表现出较高的遗忘程度。
   - BERT 在保留通用性方面表现最佳。
   - ALBERT 的通用性和保留能力最弱。

#### **实验验证**：

- 在不同的任务序列（如 GLUE 基准中的多分类任务）上对模型进行训练，评估各模型在新任务中的表现和历史任务的回溯能力。
- 实验发现，随着持续学习的进行，模型在未见任务上的性能逐渐降低，证明了通用性被破坏。

------

### **问题 2：表示的哪些通用知识类型更容易遗忘？**

#### **解答**：

通过实验发现，语法知识（Syntactic Knowledge）比语义知识（Semantic Knowledge）更容易被遗忘：

1. 使用 **Syntactic Knowledge Forgetting (SynF)** 和 **Semantic Knowledge Forgetting (SemF)** 两个指标分别量化语法和语义知识的遗忘程度。
2. 语法知识的遗忘（SynF）在所有模型中均显著高于语义知识的遗忘（SemF）。
3. **语法知识和语义知识遗忘具有强正相关性**，即两者常常在表示遗忘中一起丢失。

#### **实验验证**：

- 设计探测任务（如 SentEval 中的 BShift、TreeDepth 等用于语法检测，Tense、MRPC 等用于语义评估）。
- 对比模型在持续学习不同任务序列后，探测任务中的表现下降情况，量化语法和语义知识的损失。

------

### **问题 3：如何缓解通用知识的遗忘？**

#### **解答**：

通过对不同持续学习策略的分析，提出以下缓解策略：

1. **任务顺序优化**：
   - **优先训练通用语言学任务**（如语法相关任务 CoLA）有助于维持通用知识。
   - 任务顺序对遗忘影响显著，合理的任务设计可以减缓遗忘。
2. **混合持续学习方法**：
   - **DERPP 方法**（结合回放和正则化）在保留通用知识和任务性能方面效果最佳。
   - 回放方法（ER）在一定程度上减缓遗忘，但对通用性破坏较大。
   - 正则化方法（LwF）能更好地保留通用知识，但对新任务适应性较差。
3. **专注语法和语义表示的保护**：
   - 针对语法知识（容易遗忘）设计增强机制可以显著提升持续学习性能。

#### **实验验证**：

- 比较基于回放 (ER)、正则化 (LwF) 和混合方法 (DERPP) 的模型表现。
- DERPP 在通用性破坏（GD）、语法知识遗忘（SynF）和语义知识遗忘（SemF）上的结果均优于其他方法，证明了其有效性。

------

### **总结**

1. **通用性被破坏**：表示遗忘导致模型通用性下降，不同模型的遗忘程度差异显著。
2. **语法知识更易遗忘**：语法知识遗忘较语义知识更为显著，两者呈正相关。
3. **有效缓解策略**：优化任务顺序、采用混合持续学习方法（DERPP）和专注于易遗忘知识的保护是缓解遗忘的有效手段。

这些发现为持续学习研究提供了关键见解，也为开发更鲁棒的持续学习模型奠定了理论基础。



## 问题二 文中的指标设计



> 文章提到的Generality Destruction (GD) 指标， Syntactic Knowledge Forgetting (SynF) 和 Semantic Knowledge Forgetting (SemF) 两个指标 是如何设计的，其思想是什么



### Generality Destruction (GD)、Syntactic Knowledge Forgetting (SynF) 和 Semantic Knowledge Forgetting (SemF) 指标的设计与思想

论文提出了三个指标用于量化预训练语言模型在持续学习中的知识遗忘程度：**Generality Destruction (GD)**、**Syntactic Knowledge Forgetting (SynF)** 和 **Semantic Knowledge Forgetting (SemF)**。这些指标旨在捕捉模型通用知识的丧失，并从语法和语义两个层次揭示知识遗忘的性质。

------

### **1. Generality Destruction (GD)**

#### **设计思想**

GD 指标用来衡量模型的整体通用性在持续学习过程中的破坏程度。通用性（Generality）反映了模型在未见任务上的潜在适应能力。

#### **公式**


$$
GD = \frac{1}{N-1} \sum_{m=2}^N (R_m^* - R_{m,m})
$$




- $R_m^*$：模型在任务 TmT_m 上的单任务训练性能（作为理想性能参考）。
- $R_{m,m}$：模型通过持续学习完成任务序列后在任务 TmT_m 上的性能。
- $N$：任务序列长度。

#### **指标意义**

- GD 量化了在持续学习过程中，由于表示遗忘导致的通用性破坏。
- GD 值越大，表明模型的通用性破坏越严重。

#### **使用方法**

- 对比持续学习后模型的任务性能与单任务训练的基线性能，确定持续学习对通用性影响的总体趋势。

------

### **2. Syntactic Knowledge Forgetting (SynF)**

#### **设计思想**

SynF 指标专注于评估语法知识的遗忘程度。语法知识通常被认为包含在预训练模型的底层和中间层表征中，遗忘语法知识会直接影响模型对基础语言结构的理解。

#### **公式**


$$
SynF = \frac{1}{|P_{\text{Syn}}|} \sum_{P^s \in P_{\text{Syn}}} \frac{1}{N} \sum_{m=1}^N \frac{S^{*,-1} - S^{m,-1}}{S^{*,-1}}
$$




- $P_{\text{Syn}}$：语法探测任务的集合（如 TreeDepth、SubjNum）。
- $S^{*,-1}$：初始预训练模型在探测任务上的语法评估分数。
- $S^{m,-1}$：模型在完成任务序列 TmT_m 后的语法评估分数。

#### **指标意义**

- SynF 表示语法知识的丢失程度。
- 语法知识丧失越多，SynF 值越高。

#### **使用方法**

- 通过探测任务（如语法树深度预测）比较初始模型与持续学习后模型的语法表现，揭示语法知识在持续学习过程中的演变。

------

### **3. Semantic Knowledge Forgetting (SemF)**

#### **设计思想**

SemF 指标用于评估语义知识的遗忘程度。语义知识往往包含在预训练模型的高层表征中，与复杂的语义关系建模密切相关。

#### **公式**


$$
SemF = \frac{1}{|P_{\text{Sem}}|} \sum_{P^s \in P_{\text{Sem}}} \frac{1}{N} \sum_{m=1}^N \frac{S^{*,-1} - S^{m,-1}}{S^{*,-1}}
$$




- $P_{\text{Sem}}$：语义探测任务的集合（如 MRPC、Tense）。
- $S^{*,-1}$：初始预训练模型在探测任务上的语义评估分数。
- $S^{m,-1}$：模型在完成任务序列 $T_m $后的语义评估分数。

#### **指标意义**

- SemF 表示语义知识的遗忘程度。
- 语义知识丧失越多，SemF 值越高。

#### **使用方法**

- 通过探测任务（如句子语义一致性识别）比较初始模型与持续学习后模型的语义表现，揭示语义知识的丢失。

------

### **设计思想总结**

1. **多层次量化知识遗忘**：
   - GD 指标从整体层次上评估模型通用性的破坏。
   - SynF 和 SemF 从具体知识（语法和语义）层面细化了模型遗忘的性质。
2. **依赖探测任务**：
   - SynF 和 SemF 借助语法和语义探测任务，直接量化模型在语法和语义理解上的能力变化。
3. **细化遗忘机制研究**：
   - SynF 和 SemF 的设计能够揭示语法和语义知识丧失的差异性，为持续学习模型的优化提供参考。

------

### **实际意义**

这些指标为研究持续学习中的表示遗忘提供了新的分析工具，能够帮助：

- 理解遗忘现象的根源。
- 比较不同持续学习方法的优劣。
- 设计针对性缓解策略，如优先保护易遗忘的知识（语法）或优化任务顺序。



说明解释这三个指标的设计思想，比如第一个GD相当于是同一个模型或方法，在整个任务序列的下降平均值，第二个synF  相当于是同一个模型或方法，在整个任务序列的下降百分比吗，请进行解释



## 问题二.2 指标设计思想



> 说明解释这三个指标的设计思想，比如第一个GD相当于是同一个模型或方法，在整个任务序列的下降平均值，第二个synF  相当于是同一个模型或方法，在整个任务序列的下降百分比吗，请进行解释



### 解释三个指标的设计思想及其直观含义

三个指标 (**GD, SynF, SemF**) 的设计目的都是量化 **预训练语言模型在持续学习中遗忘程度**，但每个指标的着眼点和计算方式不同。以下是对它们设计思想及直观含义的详细解释：

------

### **1. Generality Destruction (GD)**

#### **设计思想**

- **GD 衡量的核心是模型整体性能的下降。**
- 它通过对比模型在持续学习任务序列中的表现和单任务训练（即理想条件下）的表现，计算在任务序列中的性能平均下降值。

#### **直观解释**

- GD 是模型在完成一个任务序列后，与理想情况下（单任务训练）相比的性能平均差异。
- 它的本质是模型通用性（Generality）在任务序列中的 **下降平均值**。

#### **公式解读**


$$
GD = \frac{1}{N-1} \sum_{m=2}^N (R_m^* - R_{m,m})
$$


- $R_m^*$：单任务训练模型在任务 $T_m$ 上的理想表现。
- $R_{m,m}$：持续学习模型在任务 $T_m$ 上的实际表现。
- $N$：任务序列的总任务数。

> **GD 的数值反映模型在多个任务上的综合下降幅度：**
>
> - 如果 GD 越大，说明模型的整体通用性破坏越严重。
> - 如果 GD 为 0，则说明模型没有受到任务序列的影响，表现理想。

------

### **2. Syntactic Knowledge Forgetting (SynF)**

#### **设计思想**

- **SynF 衡量模型在持续学习中语法知识的丢失比例。**
- 与 GD 不同，SynF 专注于模型的语法知识（如树深度、主语数等），通过语法探测任务量化这种知识在任务序列中的变化。

#### **直观解释**

- SynF 是 **同一个模型** 在语法探测任务上的表现变化，衡量的是表现下降的 **百分比**。
- 它捕捉的是语法相关能力的遗忘程度，而不是整体性能。

#### **公式解读**


$$
SynF = \frac{1}{|P_{\text{Syn}}|} \sum_{P^s \in P_{\text{Syn}}} \frac{1}{N} \sum_{m=1}^N \frac{S^{*,-1} - S^{m,-1}}{S^{*,-1}}
$$




- $P_{\text{Syn}}$：语法探测任务集合（如 SubjNum、TreeDepth）。
- $S^{*,-1}$：初始预训练模型在探测任务上的语法表现。
- $S^{m,-1}$：模型在完成任务序列后，在探测任务上的语法表现。

> **SynF 的数值表示语法知识丧失的比例：**
>
> - 如果 SynF 为 0，则表示语法知识没有丢失。
> - 如果 SynF 为 0.2，则表示模型丧失了 20% 的语法知识。
> - 较高的 SynF 表示模型在语法相关任务上的能力大幅下降。

------

### **3. Semantic Knowledge Forgetting (SemF)**

#### **设计思想**

- **SemF 衡量模型在持续学习中语义知识的丢失比例。**
- 与 SynF 类似，SemF 关注的是语义知识（如时间一致性、句子语义等）在任务序列中的变化，使用语义探测任务评估这种变化。

#### **直观解释**

- SemF 也是 **同一个模型** 在语义探测任务上的表现变化，衡量的是表现下降的 **百分比**。
- 它专注于语义知识（如句子含义、语义一致性等）的遗忘情况。

#### **公式解读**


$$
SemF = \frac{1}{|P_{\text{Sem}}|} \sum_{P^s \in P_{\text{Sem}}} \frac{1}{N} \sum_{m=1}^N \frac{S^{*,-1} - S^{m,-1}}{S^{*,-1}}
$$




- $P_{\text{Sem}}$：语义探测任务集合（如 MRPC、Tense）。
- $S^{*,-1}$：初始预训练模型在探测任务上的语义表现。
- $S^{m,-1}$：模型在完成任务序列后，在探测任务上的语义表现。

> **SemF 的数值表示语义知识丧失的比例：**
>
> - 如果 SemF 为 0，说明语义知识完好无损。
> - 如果 SemF 为 0.15，表示模型丧失了 15% 的语义知识。

------

### **三个指标的异同与核心设计思想**

| **指标** | **衡量对象**       | **计算方式**                           | **表示的意义**                   |
| -------- | ------------------ | -------------------------------------- | -------------------------------- |
| **GD**   | 模型整体性能       | 持续学习与单任务训练的性能平均差异     | 模型整体通用性在任务序列中的下降 |
| **SynF** | 模型的语法知识表现 | 持续学习中语法探测任务的表现下降百分比 | 模型语法知识的遗忘程度           |
| **SemF** | 模型的语义知识表现 | 持续学习中语义探测任务的表现下降百分比 | 模型语义知识的遗忘程度           |

#### **设计思想的共性**：

- **对比性**：三个指标均以初始模型（理想状态）为基准，与持续学习后的模型进行对比。
- **量化遗忘**：SynF 和 SemF 使用探测任务细化 GD 的结果，分别量化语法和语义知识的丢失。

#### **设计思想的差异**：

- **广度与深度**：GD 是总体指标，衡量综合性能下降；SynF 和 SemF 是细粒度指标，专注特定知识类型的遗忘。
- **评估内容**：SynF 关注底层结构（语法），SemF 关注高层语义。

通过这些指标，研究者能够从整体和细节两个层面全面揭示持续学习中的遗忘现象，并为优化模型和设计策略提供指导。



## 问题三 评估任务介绍



> 介绍上面使用的 语法探测任务集合（如 SubjNum、TreeDepth），语义探测任务集合（如 MRPC、Tense）



### 语法探测任务集合（Syntactic Probing Tasks）与语义探测任务集合（Semantic Probing Tasks）

论文使用了多种探测任务，用以评估预训练语言模型在语法知识和语义知识上的保留能力。这些任务来自 **SentEval** 基准和 **GLUE** 数据集，分别针对语法和语义层面的不同特征进行设计。

------

### **1. 语法探测任务集合 (Syntactic Probing Tasks)**

#### **1.1 SubjNum（主语数量预测）**

- **任务描述**：判断句子主语的数量（单数或复数）。
- 示例
  - 输入句子：*The cat jumps over the fence.*
  - 输出标签：单数。
- **知识点**：主语的语法特性。

#### **1.2 ObjNum（宾语数量预测）**

- **任务描述**：判断句子宾语的数量（单数或复数）。
- 示例
  - 输入句子：*The boy sees the dogs.*
  - 输出标签：复数。
- **知识点**：宾语的语法特性。

#### **1.3 TreeDepth（语法树深度）**

- **任务描述**：预测句子对应的语法树的最大深度。
- 示例
  - 输入句子：*The quick brown fox jumps over the lazy dog.*
  - 输出标签：深度为 4。
- **知识点**：句子的语法复杂性。

#### **1.4 BShift（双词位置交换检测）**

- **任务描述**：判断句子中的某两个连续词是否被交换位置。
- 示例
  - 输入句子：*The cat on the sat mat.* （“sat” 和 “on” 被交换）
  - 输出标签：是。
- **知识点**：语法结构的正确性。

------

### **2. 语义探测任务集合 (Semantic Probing Tasks)**

#### **2.1 Tense（时态预测）**

- **任务描述**：判断句子的主句谓语动词是过去时还是现在时。
- 示例
  - 输入句子：*She was running in the park.*
  - 输出标签：过去时。
- **知识点**：句子中的时间信息。

#### **2.2 MRPC（Microsoft Research Paraphrase Corpus）**

- **任务描述**：判断句子对是否表达相同的语义（即是否为同义改写）。
- 示例
  - 输入句子对：
    - Sentence 1: *The company posted a profit of $1 million.*
    - Sentence 2: *The firm announced earnings of $1M.*
  - 输出标签：是。
- **知识点**：句子间的语义一致性。

#### **2.3 CoordInv（并列结构检测）**

- **任务描述**：判断句子的两个并列分句是否被颠倒。
- 示例
  - 输入句子：*She ate the cake and baked a pie.*
  - 输出标签：否。
- **知识点**：并列结构的语义合理性。

------

### **语法任务与语义任务的区别与设计目的**

| **任务类型** | **主要目标**                   | **评估内容**                     | **任务示例**                       |
| ------------ | ------------------------------ | -------------------------------- | ---------------------------------- |
| **语法任务** | 评估句子的基本语法结构知识     | 语法数量、树深度、词序是否合理   | SubjNum, ObjNum, TreeDepth, BShift |
| **语义任务** | 评估句子的语义一致性及句间关系 | 句子的时间、语义一致性、句间结构 | Tense, MRPC, CoordInv              |

------

### **探测任务的使用意义**

1. **语法探测任务**：
   - 关注模型对底层语言结构的理解能力。
   - 探测语法规则和句法关系的保留情况。
2. **语义探测任务**：
   - 检测模型对高层语义的表示能力。
   - 评估语义一致性、时间信息和句间关系的理解。

通过这些任务，研究者能够从语法和语义两个角度量化预训练语言模型在持续学习中的知识保留与遗忘情况。这种细化的分析方法为持续学习模型优化提供了重要的依据。



## 问题三.2

> 为什么时态预测和并列结构检测时语义任务，它们不是解决语法的吗



### 为什么时态预测（Tense）和并列结构检测（CoordInv）被归类为语义任务？

虽然 **时态预测** 和 **并列结构检测** 涉及到语法，但它们在本文的分类中被归类为 **语义任务** 是因为它们的任务设计和目标更偏向于语义层面。以下是详细分析：

------

### **1. 时态预测（Tense）**

#### **语法 vs. 语义**

- 语法方面
  - 时态形式（如过去时和现在时）在句子中体现为动词的形态变化，确实具有语法特性。
  - 比如“walked” 是过去时，“walks” 是现在时，这是明确的语法规则。
- 语义方面
  - 时态不仅是动词形式的变化，更传递了句子的时间语义信息。
  - 时态预测任务的重点是判断句子的时间语境，比如描述的是“过去的动作”还是“现在的动作”。这更倾向于对语义信息的理解，而非单纯的语法形式分析。

#### **为何归类为语义任务**

- **研究目标**：论文关注模型在语义层面的理解能力，时态预测任务的核心是让模型判断句子语境的时间信息，而非仅仅基于语法形态规则进行分类。
- **抽象层次**：时态信息对句子意义的构建至关重要，是语义分析的一部分。

------

### **2. 并列结构检测（CoordInv）**

#### **语法 vs. 语义**

- 语法方面
  - 并列结构通常受固定的语法规则约束，例如并列连词（如 "and", "or"）的使用。
  - 分句的位置变化可能影响句法结构的完整性。
- 语义方面
  - 并列分句的语义顺序对句子整体含义有重要影响。例如：
    - *She baked a pie and ate it.*（先烤饼再吃）
    - *She ate it and baked a pie.*（先吃再烤）
  - 两个句子语法上都是正确的，但其含义完全不同。因此，该任务的核心是判断句子语义是否被改变。

#### **为何归类为语义任务**

- **研究目标**：并列结构检测的目标是判断两个分句位置交换后是否影响语义，而非语法规则的遵循。
- **含义优先**：并列分句的顺序更多影响句子在语义层面上的逻辑关系。

------

### **时态预测和并列结构检测为何强调语义**

| **任务**                     | **语法属性**                                                 | **语义属性**                                                 | **归类为语义任务的原因**                                   |
| ---------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------------------------------------------------------- |
| **时态预测（Tense）**        | 动词时态形式（如过去式或现在式）是明确的语法规则。           | 时态提供时间信息，决定句子的语义背景（过去事件或现在事件）。 | 核心目标是识别时间语境，语义属性在任务中占主导地位。       |
| **并列结构检测（CoordInv）** | 并列结构的句法规则要求连词位置正确，两部分分句的语法需一致。 | 分句顺序影响句子整体语义（如动作先后顺序的变化）。           | 任务关注分句顺序对句子含义的影响，而非单纯判断语法正确性。 |

------

### **总结**

虽然时态预测和并列结构检测涉及语法规则，但论文将它们归类为语义任务是基于任务目标和对模型能力的关注点：

- **时态预测**：重点在于理解句子的时间语境（语义层次）。
- **并列结构检测**：关注分句顺序对句子意义的影响（语义优先）。

这种分类方式体现了任务设计的实际关注点，而非仅仅根据任务表面的语法特性进行划分。



## 问题三.3 评估任务介绍



> 解释 TreeDepth 和 bsHIFT 这两个任务的含义



### TreeDepth 和 BShift 任务的含义详解

这两个任务是 **语法探测任务集合（Syntactic Probing Tasks）** 的一部分，用于评估预训练语言模型在理解和保留语法知识方面的能力。以下是它们的详细含义：

------

### **1. TreeDepth（语法树深度预测）**

#### **任务含义**

- **TreeDepth** 是一个基于句法结构的任务，用于预测句子对应的 **语法树的最大深度**。
- 语法树深度衡量了句子的语法复杂性，是语言学中一个重要的句法特征。

#### **示例**

- 输入句子

  The quick brown fox jumps over the lazy dog.

  - 句子对应的语法树可能如下：

    ```
    [S
      [NP The quick brown fox]
      [VP jumps
        [PP over
          [NP the lazy dog]]]]
    ```

  - 语法树深度：4（从根节点到最深的词节点需要 4 层）。

#### **知识点**

- 该任务要求模型理解句子的 **句法层级结构**，如名词短语（NP）、动词短语（VP）和介词短语（PP）之间的嵌套关系。
- TreeDepth 任务挑战模型的句法解析能力和对复杂句法规则的捕捉。

#### **用途**

- TreeDepth 是衡量语言模型是否能够学习和保留句法复杂性知识的关键任务。

------

### **2. BShift（双词位置交换检测）**

#### **任务含义**

- **BShift** 是一个语法正确性检测任务，用于判断句子中的 **两个相邻词是否被交换位置**。
- 被交换的位置通常会违反语法规则，使得句子结构不再正确。

#### **示例**

- **正常句子**：*The cat sat on the mat.*

- 交换后句子

  The cat on sat the mat.

  - 交换了 *"sat"* 和 *"on"*，语法不正确。

#### **知识点**

- 该任务要求模型识别句子中词序的语法合理性。
- 如果模型理解句法结构并掌握词语之间的语法依赖关系，就能识别不合理的交换位置。

#### **用途**

- BShift 任务测试模型是否能够捕捉到词语之间的局部语法关系，如主谓宾结构中的依赖规则。

------

### **TreeDepth 和 BShift 的区别与联系**

| **任务**      | **核心目标**                       | **测试内容**                         | **难度**               |
| ------------- | ---------------------------------- | ------------------------------------ | ---------------------- |
| **TreeDepth** | 评估句子的语法层级复杂性           | 模型对嵌套语法结构的理解和解析能力   | 需要全局句法信息，较高 |
| **BShift**    | 检测句子的局部词序是否符合语法规则 | 模型对词序正确性的局部依赖关系的掌握 | 局部分析，难度较低     |

------

### **总结**

- **TreeDepth** 强调对句法层级结构的全局解析能力，适合评估模型对复杂句法规则的理解。
- **BShift** 聚焦于局部词序和依赖关系，是检测模型基础语法感知能力的一个有效工具。
- 这两个任务共同为评估预训练语言模型在语法知识上的学习和保留能力提供了不同的视角。





## 问题四 实验中使用的预训练模型介绍

以下是针对文中实验所使用的多种 **预训练语言模型（PLMs）** 的详细介绍，新增了每种模型的参数大小，并总结了它们的优劣以及在实验中的表现。

------

### **1. BERT (Bidirectional Encoder Representations from Transformers)**

#### **模型特点**

- **提出者**：Devlin et al., 2019
- **架构**：双向 Transformer，允许同时关注输入文本的上下文信息。
- 参数大小
  - **BERT-Base**：110M 参数。
  - **BERT-Large**：340M 参数。
- 预训练任务
  - **Masked Language Modeling (MLM)**：掩盖部分输入词，预测其概率分布。
  - **Next Sentence Prediction (NSP)**：判断两个句子是否逻辑连续。

#### **优劣**

- **优势**：语法和语义知识保留能力强，适合广泛的自然语言理解任务。
- **不足**：模型较大，推理速度较慢，计算资源需求较高。

#### **实验表现**

- BERT 显示出优越的通用性保留能力，遗忘程度相对较低。
- 在语法知识（SynF）和语义知识（SemF）的保留上表现优秀。

------

### **2. DistilBERT**

#### **模型特点**

- **提出者**：Sanh et al., 2019

- **架构**：通过知识蒸馏技术将 BERT 压缩为一个更小、更快的版本。

- 参数大小

  ：

  - DistilBERT：66M 参数（BERT-Base 的 60%）。

- 优化点

  ：

  - 减少 40% 的参数和 60% 的推理时间。
  - 保留了大部分语言理解能力。

#### **优劣**

- **优势**：轻量化设计显著降低内存需求和计算成本，适合实时推理和部署场景。
- **不足**：表征能力略低于 BERT，复杂任务上的性能稍弱。

#### **实验表现**

- DistilBERT 的轻量化设计导致其遗忘程度略高于 BERT，特别是在语法知识的保留上。
- 适合资源受限场景，但在长任务序列的持续学习中表现较弱。

------

### **3. ALBERT (A Lite BERT)**

#### **模型特点**

- **提出者**：Lan et al., 2020
- **架构**：通过参数共享和矩阵分解进一步优化 BERT 的架构。
- 参数大小
  - **ALBERT-Base**：12M 参数。
  - **ALBERT-Large**：18M 参数。
- 优化点
  - **参数共享**：Transformer 层间共享权重，减少模型大小。
  - **嵌入矩阵分解**：优化嵌入层，进一步降低计算成本。
  - **预训练任务**：用 **Sentence Order Prediction (SOP)** 替代 NSP，提升句间关系建模能力。

#### **优劣**

- **优势**：极其轻量化，内存占用和计算需求最低。
- **不足**：参数共享机制限制了模型的表征能力，表现略逊于 BERT。

#### **实验表现**

- 在持续学习中，ALBERT 的遗忘最为显著，特别是在语法知识（SynF）上的丢失明显。
- 虽然轻量化设计具有显著优势，但通用知识保留能力较弱。

------

### **4. RoBERTa (Robustly Optimized BERT)**

#### **模型特点**

- **提出者**：Liu et al., 2019
- **架构**：通过优化训练策略和数据利用效率，对 BERT 进行改进。
- 参数大小
  - **RoBERTa-Base**：125M 参数。
  - **RoBERTa-Large**：355M 参数。
- 优化点
  - 移除 NSP 任务，仅使用 **MLM** 进行预训练。
  - 增加训练数据量（使用了 10 倍于 BERT 的数据量）。
  - 引入 **动态 Masking**，每次训练生成新的掩码，提升泛化能力。

#### **优劣**

- **优势**：对复杂任务具有更强的泛化能力，性能优于 BERT。
- **不足**：模型大，训练和推理时间长，计算资源需求较高。

#### **实验表现**

- RoBERTa 在通用知识保留和任务性能上表现优越。
- 虽然遗忘程度较低，但模型对任务序列的 drift 仍有一定敏感性。

------

### **模型对比总结**

| **模型**       | **参数大小**                | **架构特点**                           | **优势**                           | **不足**                             |
| -------------- | --------------------------- | -------------------------------------- | ---------------------------------- | ------------------------------------ |
| **BERT**       | 110M（Base）/ 340M（Large） | 双向 Transformer，支持 MLM 和 NSP 任务 | 表现均衡，语法和语义知识保留能力强 | 模型大，计算成本高                   |
| **DistilBERT** | 66M                         | 通过知识蒸馏优化 BERT 的轻量级版本     | 轻量化，推理速度快，适合部署       | 表征能力略低，复杂任务表现受限       |
| **ALBERT**     | 12M（Base）/ 18M（Large）   | 参数共享和矩阵分解，改用 SOP 替代 NSP  | 模型小，计算和内存需求低           | 参数共享限制了适应能力，遗忘程度显著 |
| **RoBERTa**    | 125M（Base）/ 355M（Large） | 优化训练策略，改进数据利用效率         | 泛化能力强，适合复杂任务           | 模型大，训练和推理时间长             |

------

### **实验中的表现总结**

1. **遗忘表现**：
   - **BERT** 和 **RoBERTa**：遗忘程度最低，通用知识保留能力强，适合复杂任务。
   - **ALBERT**：通用知识遗忘最严重，适合资源有限场景但在持续学习任务中表现较弱。
   - **DistilBERT**：轻量化设计有利于部署，但知识保留能力稍逊。
2. **适用场景**：
   - **复杂任务**：优先选择 BERT 或 RoBERTa。
   - **轻量化需求**：选择 DistilBERT 或 ALBERT。

通过综合分析模型架构与实验表现，论文揭示了模型参数量与持续学习任务中遗忘问题的关系，为选择合适模型提供了系统性参考。

## 问题五 实验中使用的持续学习方法

### 实验中采用的几种持续学习方法

论文中采用了三类主流的持续学习方法，包括基于回放的策略、基于正则化的策略以及混合策略。以下是每种方法的详细介绍、设计思路、优缺点以及在实验中的表现。

------

### **1. 基于回放的策略（Rehearsal-Based Methods）**

#### **设计思路**

- **核心思想**：通过存储一部分旧任务的样本，在训练新任务时与当前任务的数据一起使用，以减缓旧知识的遗忘。

- 代表方法

  ：

  - Experience Replay (ER)

    ：

    - 简单有效的回放方法，将存储的旧样本与新任务样本混合训练。
    - 核心机制是通过回放历史样本来保留旧任务的知识。

  - Dark Experience Replay with Penalty (DERPP)

    ：

    - ER 的改进版本，同时保存历史样本的预测日志（logits），并在训练时通过正则化保持新模型输出与历史 logits 的一致性。

#### **优缺点**

- 优点

  ：

  - 能够有效减缓遗忘，特别是在多任务学习中表现突出。
  - 存储样本能直接提供旧任务的参考，有助于知识的保留。

- 缺点

  ：

  - 对存储空间要求较高，尤其是在任务数量多时。
  - 对样本选择的策略敏感，随机存储可能导致重要样本遗漏。

#### **实验表现**

- ER 在均衡任务序列上表现良好，但当任务间分布不均时效果下降。
- DERPP 显著提升了任务性能和通用知识的保留能力，是实验中表现最优的方法之一。

------

### **2. 基于正则化的策略（Regularization-Based Methods）**

#### **设计思路**

- **核心思想**：通过对模型参数的更新进行限制，保护与旧任务相关的重要参数，从而减少知识遗忘。

- 代表方法

  ：

  - Learning without Forgetting (LwF)

    ：

    - 使用旧任务的输出作为软标签，约束模型在新任务上的预测与旧任务一致。
    - 通过目标函数中的正则化项惩罚新任务对旧任务知识的干扰。

  - Elastic Weight Consolidation (EWC)

    ：

    - 基于贝叶斯理论，通过 Fisher 信息矩阵标识旧任务中重要的参数，并通过 L2 正则化限制这些参数的更新幅度。

#### **优缺点**

- 优点

  ：

  - 不需要存储旧样本，降低了存储成本。
  - 理论基础清晰，适合任务边界明确的场景。

- 缺点

  ：

  - 在长任务序列中，随着正则化项的累积，模型对新任务的适应能力显著下降。
  - 对任务顺序和任务间的相似性依赖较高。

#### **实验表现**

- LwF 对新任务的适应能力有限，但在减少遗忘方面效果较好。
- EWC 在任务数量较少时表现稳定，但当任务数量增加时，其有效性下降。

------

### **3. 混合策略（Hybrid Methods）**

#### **设计思路**

- **核心思想**：结合回放和正则化的优点，通过综合策略更全面地减少遗忘。

- 代表方法

  ：

  - Dark Experience Replay with Penalty (DERPP)

    ：

    - 同时结合回放机制和正则化方法：
      - 存储历史样本并在训练时回放。
      - 使用 logits 的一致性作为额外正则化项，限制新任务对旧任务表征的干扰。

#### **优缺点**

- 优点

  ：

  - 综合了回放和正则化的优点，在减少遗忘和提升新任务性能上更平衡。
  - 适合任务分布复杂、不均衡的场景。

- 缺点

  ：

  - 实现较复杂，对存储和计算资源有一定要求。

#### **实验表现**

- DERPP 在实验中表现最优，在减少遗忘和提高新任务性能上都优于单独的回放或正则化方法。
- 对语法和语义知识的保留效果显著，特别是在任务序列较长时。

------

### **方法对比总结**

| **方法类别**   | **代表方法** | **设计思路**                           | **优点**                       | **缺点**                     |
| -------------- | ------------ | -------------------------------------- | ------------------------------ | ---------------------------- |
| **基于回放**   | ER, DERPP    | 存储旧任务样本，训练新任务时回放       | 遗忘减少，旧任务性能保留显著   | 对存储要求高，对样本选择敏感 |
| **基于正则化** | LwF, EWC     | 限制重要参数更新，保护旧任务知识       | 不需要存储样本，理论清晰       | 随任务数量增加适应性降低     |
| **混合策略**   | DERPP        | 结合回放和正则化，通过综合策略减缓遗忘 | 平衡旧任务保留和新任务适应能力 | 实现复杂，计算和存储成本较高 |

------

### **实验表现总结**

1. 回放方法（ER, DERPP）

   ：

   - 在大多数场景中有效减少遗忘。
   - DERPP 的综合表现最佳，尤其是在长任务序列中。

2. 正则化方法（LwF, EWC）

   ：

   - 对存储要求低，但对新任务适应性较差。
   - LwF 和 EWC 更适合任务数量较少的场景。

3. 混合策略（DERPP）

   ：

   - 实验中显示出最优的平衡能力，同时减缓遗忘并提升任务性能。

这些方法为持续学习提供了多样化的解决方案，实验验证了不同策略的优劣，为特定场景下方法选择提供了参考依据。



## 问题六  论文的不足



> 这篇论文是不是只关注了 在新的数据集上训练过后的遗忘表现，没有评估其在未来任务的泛化表现



是的，这篇论文的确主要聚焦于 **预训练语言模型（PLMs）** 在 **新的任务序列训练后** 的 **遗忘表现**，而并未直接评估其在未来未见任务上的泛化能力。以下是对此问题的分析：

------

### **论文的关注点：遗忘而非泛化**

1. **核心目标是研究表示遗忘**：
   - 论文明确提出的核心问题是：
     - PLMs 的通用性是否因持续学习中的表示遗忘而被破坏？
     - 哪些类型的通用知识更容易被遗忘？
     - 如何缓解这些遗忘？
   - 这些问题都集中于模型在已知任务序列上的表现退化，而不是对未来任务的预测或泛化能力。
2. **指标的设计聚焦于遗忘**：
   - **GD (Generality Destruction)**：比较持续学习与单任务训练模型在相同任务上的性能下降，衡量遗忘的严重程度。
   - **SynF 和 SemF**：评估语法和语义知识在已知任务中的丢失情况。
   - 这些指标均未设计用于量化模型对未来任务的适应性，而是围绕当前任务序列中遗忘问题展开。
3. **实验设置未包含未来任务评估**：
   - 论文中选用的探测任务（如 SentEval 和 GLUE 的任务集合）和设计的任务序列（如任务顺序的对比实验）仅限于模型已经接触到的任务。
   - 实验未提供未来未见任务（out-of-distribution tasks）的测试结果。

------

### **泛化能力的重要性与现状**

1. **未来任务的泛化评估的重要性**：
   - 未来任务的泛化能力是持续学习的核心目标之一，旨在让模型能够在新任务中高效适应并表现良好。
   - 理想的持续学习方法不仅要减少遗忘，还要提高模型对未见任务的潜在适应性。
2. **论文对泛化的间接贡献**：
   - 论文通过研究 **任务顺序** 和 **混合学习策略（如 DERPP）** 的效果，为减少遗忘和保留通用知识提供了方法。通用知识的保留可以间接改善未来任务的泛化表现，但这一点并未被明确验证。
   - 如论文所述：“优先学习通用语言学任务可以帮助减少通用知识的丢失”，这可能提高了模型对未来任务的适应能力，但论文未对此进行实证分析。

------

### **如何验证未来任务的泛化表现**

要评估持续学习方法对未来任务泛化的影响，可以补充以下实验：

1. **加入未见任务测试**：
   - 在持续学习任务序列完成后，评估模型在完全未知的新任务（如其他基准数据集上的任务）的表现。
   - 例如：在完成 GLUE 的任务序列后，测试模型在 SuperGLUE、XTREME 或其他 OOD 任务上的表现。
2. **扩展指标**：
   - 引入新的泛化性指标，量化模型在未来任务上的迁移学习能力，而不仅是探测任务中的知识保留。
3. **对比不同方法**：
   - 比较不同持续学习方法（如 DERPP、LwF 和 ER）对未来任务泛化性能的提升效果。

------

### **总结**

当前论文的确主要关注 **在已知任务上的遗忘现象**，而对未来任务的 **泛化表现** 并未进行直接研究或评估。尽管研究提出的减少遗忘的策略可能间接改善未来任务的泛化能力，但这些影响仍需进一步实验验证。如果对未来任务的泛化能力有兴趣，可以在当前研究基础上补充实验，验证任务顺序优化或混合策略对新任务迁移能力的贡献。
