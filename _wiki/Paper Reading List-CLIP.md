---
layout: wiki
title: Paper Reading List - CLIP
categories: LLM and forgetting
description: CLIP  è®ºæ–‡é˜…è¯»æ±‡æ€»è®°å½•
keywords: LLM and forgetting
---





# Paper Reading-- CLIP 



## **AN IMAGE IS WORTH 16X16 WORDS:  TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE**

[`semanticscholar`](https://www.semanticscholar.org/paper/268d347e8a55b5eb82fb5e7d2f800e33c75ab18a)  [`Paper`](https://www.semanticscholar.org/paper/268d347e8a55b5eb82fb5e7d2f800e33c75ab18a)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F268d347e8a55b5eb82fb5e7d2f800e33c75ab18a%3Ffields%3DcitationCount)

2020    International Conference on Learning Representations 

â€‹	Vit  ä½¿ç”¨ Transfoirmer  ç»“æž„ä»£æ›¿ cnn ç»“æžœ



![image-20250130212109048](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250130212109107.png)



 

## **Learning Transferable Visual Models From Natural Language Supervision**

[`semanticscholar`](https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4)  [`Paper`](https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount)

2021    International Conference on Machine Learning 

![image-20250130213357520](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250130213357585.png)

CLIP å°†æ–‡æœ¬ä¸Žå›¾åƒè¿›è¡Œå¯¹é½





## **Image-based CLIP-Guided Essence Transfer**

â€‹	[`semanticscholar`](https://www.semanticscholar.org/paper/61432c11c359f6abb38a62a674fa4fdbc8be94d3)  [`Paper`](https://www.semanticscholar.org/paper/61432c11c359f6abb38a62a674fa4fdbc8be94d3)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F61432c11c359f6abb38a62a674fa4fdbc8be94d3%3Ffields%3DcitationCount)

â€‹     

![image-20250131112917188](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250131112917298.png)



![image-20250131112141004](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250131112141100.png)



åªä¼˜åŒ–è½¬ç§»å›¾åƒä¹‹é—´çš„å·®å¼‚ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„å›ºå®šçš„styleGANå’ŒCLIP ï¼Œä½¿ç”¨CLIP ç¡®ä¿å‰åŽå˜åŒ–ä¸€è‡´





>### **ðŸ“Œ è¿™å¼ å›¾æè¿°äº†ä¸‰ä¸ªæ­¥éª¤ï¼š**
>
>è®ºæ–‡çš„æ–¹æ³•éœ€è¦åœ¨ **StyleGAN å’Œ CLIP** çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè®¡ç®—ï¼Œä»¥ç¡®ä¿ Essence Transfer è¿‡ç¨‹æ˜¯ç¨³å®šçš„ã€‚  å›¾ 2 é€šè¿‡ **ä¸‰ä¸ªæ­¥éª¤** å±•ç¤ºäº†è¿™ä¸ªè¿‡ç¨‹ï¼š
>
>### **ðŸ”¹ Step 1: Invert Source Images & Add Essence Vector**
>
>**ç¿»è¯‘ï¼šå¯¹æºå›¾åƒè¿›è¡Œåæ¼” (Inversion) å¹¶æ·»åŠ  Essence å‘é‡ã€‚** âœ… **é€šä¿—è§£é‡Šï¼š**
>
>- **å…ˆæŠŠæºå›¾åƒ  $I_s$ è½¬æ¢æˆ StyleGAN çš„â€œæ½œåœ¨å‘é‡â€  $z_s$ **ã€‚
> - è¿™ä¸ªè¿‡ç¨‹å«åš **GAN Inversionï¼ˆGAN åæ¼”ï¼‰**ï¼Œå¯ä»¥è®© StyleGAN é‡æ–°ç”Ÿæˆæºå›¾åƒã€‚
> - ä½ å¯ä»¥ç†è§£ä¸ºæŠŠ **ä¸€å¼ è„¸â€œç¼–ç â€åˆ° StyleGAN** é‡Œï¼Œè¿™æ ·åŽç»­å¯ä»¥ä¿®æ”¹å®ƒã€‚
>- **å†åœ¨è¿™ä¸ªæ½œåœ¨ç©ºé—´é‡ŒåŠ ä¸Š Essence Vector  $b$**ã€‚
> - $b$  ä»£è¡¨ **ç›®æ ‡å›¾åƒ  $I_t$  å’Œæºå›¾åƒ  $I_s$  ä¹‹é—´çš„è¯­ä¹‰å·®å¼‚**ï¼Œæ¯”å¦‚å¹´é¾„ã€æ€§åˆ«ã€å‘åž‹ç­‰ç‰¹å¾ã€‚
>  - ðŸ’¡ **æ‰“æ¯”æ–¹ï¼š**æƒ³è±¡ StyleGAN æ˜¯ä¸€ä¸ªä¼šç”»ç”»çš„æœºå™¨äººï¼Œè€Œ **GAN åæ¼”** å°±åƒæ˜¯è®©æœºå™¨äººè®°ä½ä½ çš„ä¸€å¼ ç…§ç‰‡ï¼ˆåƒç´  â†’ æ•°å­—ä»£ç ï¼‰ã€‚
>    ç„¶åŽï¼Œæˆ‘ä»¬åŠ ä¸Š Essence Vector  $b$ ï¼Œå°±åƒæ˜¯åœ¨è¿™å¼ ç…§ç‰‡çš„ä»£ç é‡Œ **åŠ ä¸Šä¸€ç‚¹ç‚¹ç›®æ ‡å›¾åƒçš„æ„Ÿè§‰**ï¼Œæ¯”å¦‚å˜è€ã€å˜å¹´è½»ã€å˜æ¢å‘åž‹ã€‚
>
> ------
> 
> ### **ðŸ”¹ Step 2: Decode with StyleGAN**
>
>**ç¿»è¯‘ï¼šä½¿ç”¨ StyleGAN ç”Ÿæˆå˜æ¢åŽçš„å›¾åƒã€‚** 
>
>âœ… **é€šä¿—è§£é‡Šï¼š**
>
>- é€šè¿‡ **StyleGAN** ç”Ÿæˆæ–°çš„å›¾åƒ ** $I_{s,t}$ **ï¼š$I_{s,t} = G(z_s + b)$
>- è¿™æ„å‘³ç€ï¼š
> - **åŽŸå›¾  $I_s$  ä»ç„¶ä¿ç•™äº†ä¸»è¦çš„é¢éƒ¨èº«ä»½ä¿¡æ¯**ã€‚
> - **ä½†å®ƒå¼€å§‹å¸¦æœ‰ç›®æ ‡å›¾åƒ  $I_t$  çš„è¯­ä¹‰ç‰¹å¾**ï¼ˆæ¯”å¦‚å‘åž‹ã€å¹´é¾„ã€æ€§åˆ«ï¼‰ã€‚
>- è¿™ä¸ªé˜¶æ®µå®ŒæˆåŽï¼Œæˆ‘ä»¬å·²ç»å¾—åˆ°ä¸€ä¸ªåˆæ­¥çš„ Essence Transfer ç»“æžœï¼Œä½†è¿˜éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚ðŸ’¡ **æ‰“æ¯”æ–¹ï¼š**ä½ ç»™æœºå™¨äººè¯´ï¼šâ€œè®°ä½è¿™å¼ è„¸ï¼ˆæºå›¾åƒï¼‰ï¼Œç„¶åŽç»™ä»–åŠ ä¸Šç›®æ ‡å›¾åƒçš„ä¸€ç‚¹ç‚¹æ„Ÿè§‰ã€‚â€ æœºå™¨äººç”»å‡ºçš„æ–°å›¾åƒ **çœ‹èµ·æ¥åƒæºå›¾åƒï¼Œä½†å¸¦æœ‰ç›®æ ‡çš„ç‰¹å¾**ã€‚
>
> ------
> 
>### **ðŸ”¹ Step 3: Encode with CLIP & Calculate Loss**
>
>**ç¿»è¯‘ï¼šç”¨ CLIP ç¼–ç å¹¶è®¡ç®—æŸå¤±ã€‚** âœ… **é€šä¿—è§£é‡Šï¼š**
>
>- æŠŠ **æºå›¾åƒ  $I_s$ **ã€**ç›®æ ‡å›¾åƒ  $I_t$ ** å’Œ **å˜æ¢åŽçš„å›¾åƒ  $I_{s,t}$ ** éƒ½è¾“å…¥ **CLIP** è¿›è¡Œç‰¹å¾æå–ã€‚
>
>- è®¡ç®—ä¸¤ä¸ªå…³é”®çš„æŸå¤±ï¼š
>
> 1. Lsimilarityï¼ˆç›¸ä¼¼æ€§æŸå¤±ï¼‰ï¼š
>    - ç¡®ä¿å˜æ¢åŽçš„å›¾åƒ ** $I_{s,t}$ ** åœ¨ CLIP è¯­ä¹‰ç©ºé—´é‡Œ**æŽ¥è¿‘ç›®æ ‡å›¾åƒ**  $I_t$ ã€‚
> 2. Lconsistencyï¼ˆä¸€è‡´æ€§æŸå¤±ï¼‰ï¼š
>    - ç¡®ä¿ Essence Transfer è¿‡ç¨‹åœ¨ **ä¸åŒæºå›¾åƒä¸Šæ˜¯ä¸€è‡´çš„**ï¼Œä¸ä¼šå› ä¸åŒäººè„¸å¯¼è‡´é£Žæ ¼ä¸ç¨³å®šã€‚
> 
>  ðŸ’¡ **æ‰“æ¯”æ–¹ï¼š**è®© CLIP å……å½“â€œæ™ºèƒ½å®¡æŸ¥å®˜â€ï¼Œå®ƒä¼šæ£€æŸ¥ï¼š
> 
> 1. è¿™ä¸ªå˜æ¢åŽçš„å›¾åƒ **æœ‰æ²¡æœ‰æˆåŠŸå¸æ”¶ç›®æ ‡çš„è¯­ä¹‰**ï¼Ÿ
>2. è¿™ä¸ªå˜æ¢ **æ˜¯ä¸æ˜¯åœ¨æ‰€æœ‰æºå›¾åƒä¸Šéƒ½è¡¨çŽ°ä¸€è‡´**ï¼Ÿå¦‚æžœæ•ˆæžœä¸ç†æƒ³ï¼Œå°±è°ƒæ•´ **Essence Vector  $b$ **ï¼Œå†è¯•ä¸€æ¬¡ï¼Œç›´åˆ°æŸå¤±æœ€å°åŒ–ã€‚
> 
>------
>
>### **ðŸ“Œ æ€»ç»“**
>
>| **æ­¥éª¤**   | **ä¸»è¦ä»»åŠ¡**                         | **é€šä¿—ç†è§£**                                          |
>| ---------- | ------------------------------------ | ----------------------------------------------------- |
>| **Step 1** | **åæ¼”æºå›¾åƒ + æ·»åŠ  Essence Vector** | è®© StyleGAN è®°ä½æºå›¾åƒï¼Œå¹¶åŠ ä¸Šç›®æ ‡å›¾åƒçš„â€œæ„Ÿè§‰â€        |
>| **Step 2** | **ç”¨ StyleGAN ç”Ÿæˆå˜æ¢åŽçš„å›¾åƒ**     | è®© GAN ç”»å‡ºæ–°å›¾åƒï¼Œå®ƒçœ‹èµ·æ¥åƒæºå›¾åƒä½†å¸¦æœ‰ç›®æ ‡ç‰¹å¾     |
>| **Step 3** | **ç”¨ CLIP è®¡ç®—æŸå¤±**                 | è®© CLIP è¯„ä¼°æ–°å›¾åƒæ˜¯å¦ç¬¦åˆç›®æ ‡ï¼Œå¹¶è°ƒæ•´ Essence Vector |
>
>------
>
>### **ðŸ“Œ ç›´è§‚ç†è§£æ•´ä¸ªæµç¨‹**
>
>å¯ä»¥æŠŠæ•´ä¸ªæµç¨‹æ¯”ä½œ **â€œåŒ–å¦†+è¯„ä»·â€**ï¼š
>
>1. **Step 1ï¼šé€‰æ‹©åŒ–å¦†æ¨¡æ¿**ï¼ˆé€‰å®š Essence Vectorï¼‰ â†’ ç›®æ ‡å›¾åƒ  $I_t$  å‘Šè¯‰æˆ‘ä»¬æƒ³è¦çš„å˜åŒ–ï¼ˆæ¯”å¦‚æ¢å‘åž‹ï¼‰ã€‚
>2. **Step 2ï¼šåŒ–å¦†**ï¼ˆç”¨ StyleGAN ç”Ÿæˆæ–°å›¾åƒï¼‰ â†’ è®© StyleGAN ç”»å‡ºå¸¦æœ‰æ–°ç‰¹å¾çš„è„¸ã€‚
>3. **Step 3ï¼šæ£€æŸ¥åŒ–å¦†æ•ˆæžœ**ï¼ˆCLIP è®¡ç®—æŸå¤±ï¼‰ â†’ è®© CLIP è¯„ä¼°æ˜¯ä¸æ˜¯å˜å¾—å¤ªå¤š/å¤ªå°‘ï¼Œç„¶åŽè°ƒæ•´å¦†å®¹ï¼Œç›´åˆ°æœ€å®Œç¾Žã€‚è¿™æ ·ï¼Œ**æ—¢èƒ½ä¿æŒåŽŸæœ¬çš„é¢éƒ¨èº«ä»½ï¼Œåˆèƒ½ç²¾å‡†æå–ç›®æ ‡çš„ç‰¹å¾**ï¼ ðŸŽ¨âœ¨
>
>
>
>
>
>### **è®­ç»ƒæ—¶æ›´æ–°çš„æ˜¯å“ªä¸ªç½‘ç»œï¼ŸStyleGAN è¿˜æ˜¯ CLIPï¼Ÿ**
>
>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œ**StyleGAN å’Œ CLIP éƒ½ä¸ä¼šè¢«æ›´æ–°**ï¼Œè®ºæ–‡çš„ **è®­ç»ƒç›®æ ‡æ˜¯ä¼˜åŒ– Essence Vector  $b^*$ **ï¼Œå³ **åœ¨ StyleGAN çš„æ½œåœ¨ç©ºé—´  $W$  æˆ–  $W^+$  ä¸­æ‰¾åˆ°åˆé€‚çš„è¯­ä¹‰åç§»é‡**ã€‚ 
>
>âœ… **ç»“è®ºï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šæ›´æ–° StyleGAN å’Œ CLIPï¼Œåªä¼˜åŒ– Essence Vector  $b$ ã€‚**  
>
>âœ… **æ–¹æ³•æœ¬è´¨ä¸Šæ˜¯åˆ©ç”¨** **StyleGAN ä½œä¸ºä¸€ä¸ªå›ºå®šçš„ç”Ÿæˆå™¨**ï¼Œ**CLIP ä½œä¸ºä¸€ä¸ªå›ºå®šçš„è¯„ä¼°å™¨**ã€‚
>
>**ðŸ“Œ è®­ç»ƒæ—¶ä¼˜åŒ–çš„éƒ¨åˆ†**
>
>åœ¨ **è®­ç»ƒæ—¶ï¼Œè®ºæ–‡ä¼˜åŒ–çš„æ˜¯ Essence Vector  $b^*$ **ï¼Œå…·ä½“æ¥è¯´ï¼š
>
>- StyleGAN ( $G$ ) æ˜¯å›ºå®šçš„ âŒ ä¸è®­ç»ƒ
> - è®ºæ–‡çš„æ–¹æ³•ä¸ä¼šä¿®æ”¹ StyleGAN ç”Ÿæˆå™¨çš„å‚æ•°ï¼Œè€Œæ˜¯ç›´æŽ¥åœ¨ **StyleGAN é¢„è®­ç»ƒçš„æ½œåœ¨ç©ºé—´** è¿›è¡Œæ“ä½œã€‚
> - ä¹Ÿå°±æ˜¯è¯´ï¼Œ**StyleGAN åªæ˜¯ä¸€ä¸ªâ€œå˜æ¢å‡½æ•°â€**ï¼Œå®ƒæ ¹æ® **è¾“å…¥çš„æ½œåœ¨å‘é‡  $z$ ** ç”Ÿæˆå›¾åƒï¼Œä½†è‡ªèº«ä¸å˜ã€‚
>- CLIP ( $C$ ) ä¹Ÿæ˜¯å›ºå®šçš„âŒ ä¸è®­ç»ƒ
> - CLIP çš„ Image Encoder åªæ˜¯ç”¨æ¥æå–ç›®æ ‡å›¾åƒå’Œæºå›¾åƒçš„è¯­ä¹‰åµŒå…¥ï¼ˆfeature embeddingsï¼‰ã€‚
>  - è®ºæ–‡çš„æ–¹æ³•ä½¿ç”¨ CLIP **è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦**ï¼Œä½†ä¸ä¼šä¿®æ”¹ CLIP çš„å‚æ•°ã€‚
> - **ä¼˜åŒ–çš„å˜é‡æ˜¯ Essence Vector  $b^*$** âœ… è¦è®­ç»ƒ
> - è®ºæ–‡çš„æ–¹æ³•è®­ç»ƒçš„æ˜¯ **ä¸€ä¸ªåç§»å‘é‡  $b$  ï¼Œå®ƒä½äºŽ StyleGAN çš„æ½œåœ¨ç©ºé—´**ï¼Œä½¿å¾—ï¼š$G(z_s + b^*) = I_{s,t}$
>  - ç›®æ ‡æ˜¯æ‰¾åˆ°æœ€ä¼˜çš„  $b^*$ ï¼Œä½¿å¾— **StyleGAN ç”Ÿæˆçš„å›¾åƒ** æ—¢ä¿ç•™æºå›¾åƒçš„èº«ä»½ï¼Œåˆå…·æœ‰ç›®æ ‡å›¾åƒçš„è¯­ä¹‰ç‰¹å¾ã€‚
> 
>------
> 
> **ðŸ“Œ ä¸ºä»€ä¹ˆä¸æ›´æ–° StyleGANï¼Ÿ**
>
>1. StyleGAN å·²ç»æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ç”Ÿæˆå™¨
>  - StyleGAN æœ¬èº«å·²ç»åœ¨å¤§è§„æ¨¡äººè„¸æ•°æ®ä¸Šè®­ç»ƒå¥½ï¼Œå®ƒçš„æ½œåœ¨ç©ºé—´å·²ç»åŒ…å«ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼ˆå¹´é¾„ã€æ€§åˆ«ã€å‘åž‹ã€è¡¨æƒ…ç­‰ï¼‰ã€‚
>  - ç›´æŽ¥åœ¨æ½œåœ¨ç©ºé—´è°ƒæ•´  $b$  æ¯”é‡æ–°è®­ç»ƒ StyleGAN æ›´é«˜æ•ˆã€‚
>2. ä¿®æ”¹ StyleGAN å¯èƒ½ä¼šç ´åç”Ÿæˆèƒ½åŠ›
>  - å¦‚æžœé‡æ–°è®­ç»ƒ StyleGANï¼Œä¼šå¯¼è‡´æ•´ä¸ªæ¨¡åž‹åå‘æŸäº›ç‰¹å®šç›®æ ‡ï¼Œä¸å†é€‚ç”¨äºŽæ‰€æœ‰è¾“å…¥å›¾åƒã€‚
>   - è®ºæ–‡çš„æ–¹æ³•å¸Œæœ› Essence Transfer **å¯¹ä¸åŒç›®æ ‡å›¾åƒéƒ½é€‚ç”¨**ï¼Œæ‰€ä»¥è®© StyleGAN ä½œä¸ºä¸€ä¸ªé€šç”¨æ¨¡åž‹ï¼Œè€Œä¸åšè°ƒæ•´ã€‚
> 
>------
> 
> **ðŸ“Œ ä¸ºä»€ä¹ˆä¸æ›´æ–° CLIPï¼Ÿ**
>
>1. CLIP æ˜¯ä¸€ä¸ªé€šç”¨çš„è¯­ä¹‰è¯„ä¼°å™¨
>  - CLIP é€šè¿‡å¤§è§„æ¨¡å›¾åƒ-æ–‡æœ¬åŒ¹é…è®­ç»ƒï¼Œå·²ç»å­¦ä¹ åˆ°ä¸°å¯Œçš„é«˜å±‚è¯­ä¹‰ä¿¡æ¯ã€‚
>  - è®ºæ–‡çš„æ–¹æ³•åˆ©ç”¨ CLIP æ¥è¡¡é‡å›¾åƒçš„è¯­ä¹‰ç‰¹å¾ï¼Œè€Œä¸æ˜¯è®© CLIP å­¦ä¹ æ–°çš„ç‰¹å¾ã€‚
>2. ä¸ä¿®æ”¹ CLIP ç¡®ä¿æ–¹æ³•çš„æ³›åŒ–æ€§
>  - CLIP æä¾›çš„è¯­ä¹‰è¯„ä¼°æ˜¯é€šç”¨çš„ï¼Œé€‚ç”¨äºŽä¸åŒç±»åž‹çš„å›¾åƒå’Œç¼–è¾‘ä»»åŠ¡ã€‚
>   - å¦‚æžœè®­ç»ƒ CLIPï¼Œå¯èƒ½ä¼šè®©å®ƒè¿‡æ‹ŸåˆäºŽç‰¹å®šæ•°æ®é›†ï¼Œé™ä½Žæ³›åŒ–èƒ½åŠ›ã€‚
> 
>------
> 
> **ðŸ“Œ è®­ç»ƒæµç¨‹**
>
>### **1ï¸âƒ£ è®¡ç®—ç›®æ ‡å›¾åƒä¸Žæºå›¾åƒçš„è¯­ä¹‰åç§»**
>
>- ç”¨ CLIP æå– **ç›®æ ‡å›¾åƒ  $I_t$  å’Œæºå›¾åƒ  $I_s$  çš„ç‰¹å¾**ï¼š$\Delta c = C(I_t) - C(I_s)$
>- è¿™é‡Œ CLIP åªæ˜¯ç”¨æ¥**è®¡ç®—è¯­ä¹‰å·®å¼‚**ï¼Œ**CLIP æœ¬èº«ä¸ä¼šè¢«æ›´æ–°**ã€‚
>
>### **2ï¸âƒ£ è®­ç»ƒ Essence Vector  $b^*$ **
>
>- **åœ¨ StyleGAN æ½œåœ¨ç©ºé—´ä¼˜åŒ–**  $b$ ï¼Œä½¿å¾—ï¼š$G(z_s + b^*) \approx I_{s,t}$
>- å…¶ä¸­  $I_{s,t}$  æ˜¯å˜æ¢åŽçš„å›¾åƒã€‚
>- ä¼˜åŒ–ç›®æ ‡ï¼š
> - $L_{\text{similarity}}$ ï¼šç¡®ä¿å˜æ¢åŽçš„å›¾åƒè¯­ä¹‰æŽ¥è¿‘ç›®æ ‡å›¾åƒ  $I_t$ ã€‚
> - $L_{\text{consistency}}$ ï¼šç¡®ä¿ Essence Transfer åœ¨ä¸åŒæºå›¾åƒä¸Šè¡¨çŽ°ä¸€è‡´ã€‚
> - $L_{L2}$ ï¼šæ­£åˆ™åŒ–ï¼Œé˜²æ­¢  $b$  è¿‡åº¦å˜åŒ–å¯¼è‡´å›¾åƒå¤±çœŸã€‚
>
> ### **3ï¸âƒ£ åå‘ä¼ æ’­ & è¿­ä»£ä¼˜åŒ–**
> 
> - **æ›´æ–°  $b$ ï¼Œè€Œä¸æ˜¯ StyleGAN æˆ– CLIP**ï¼š$b^* = \arg\min (L_{\text{similarity}} + \lambda_{\text{consistency}} L_{\text{consistency}} + \lambda_{L2} \|b\|^2)$
>- é€šè¿‡ **æ¢¯åº¦ä¸‹é™ï¼ˆAdam ä¼˜åŒ–å™¨ï¼‰** è¿­ä»£ä¼˜åŒ–  $b$ ï¼Œç›´åˆ°ç”Ÿæˆçš„å›¾åƒç¬¦åˆè¦æ±‚ã€‚
>
>------
>
>**ðŸ“Œ æ€»ç»“**
>
>â€‹    âœ… **è®­ç»ƒæ—¶ä¸ä¼šæ›´æ–° StyleGAN æˆ– CLIPï¼Œåªä¼˜åŒ– Essence Vector  $b^*$ **
>â€‹	âœ… **StyleGAN ä½œä¸ºå›ºå®šç”Ÿæˆå™¨ï¼ŒCLIP ä½œä¸ºå›ºå®šè¯„ä¼°å™¨**
>â€‹	âœ… **ä¼˜åŒ–çš„ç›®æ ‡æ˜¯æ‰¾åˆ°æœ€ä¼˜çš„  $b^\*$ ï¼Œè®© StyleGAN ç”Ÿæˆç¬¦åˆç›®æ ‡è¯­ä¹‰çš„å›¾åƒ** è¿™ç§æ–¹æ³•çš„ **æœ€å¤§ä¼˜åŠ¿æ˜¯è®¡ç®—é«˜æ•ˆï¼Œé¿å…äº†å¯¹å¤§æ¨¡åž‹çš„é¢å¤–è®­ç»ƒï¼ŒåŒæ—¶ç¡®ä¿æ³›åŒ–æ€§å¼º**ï¼ ðŸš€



## **Scaling Autoregressive Models for Content-Rich Text-to-Image Generation**

[`semanticscholar`](https://www.semanticscholar.org/paper/1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe)  [`Paper`](https://www.semanticscholar.org/paper/1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe%3Ffields%3DcitationCount)

2022    Trans. Mach. Learn. Res. 



Parti proves that autoregressive models remain competitive in text-to-image generation, particularly when scaled to 20B parameters.  

è¾“å…¥ä¸€æ®µæ–‡æœ¬ï¼Œæ¨¡åž‹æŒ‰â€œæ‹¼å›¾â€çš„æ–¹å¼é€æ­¥ç”Ÿæˆä¸€å¼ å®Œæ•´çš„å›¾ç‰‡ï¼

![image-20250131120028561](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250131120028718.png)



![image-20250131120106562](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250131120106680.png)







## **Data Determines Distributional Robustness  in Contrastive Language-Image Pre-training (CLIP)**

[`semanticscholar`](https://www.semanticscholar.org/paper/0b73a37c06f3d79a1cb5fd61e3556676634b9d2f)  [`Paper`](https://www.semanticscholar.org/paper/0b73a37c06f3d79a1cb5fd61e3556676634b9d2f)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0b73a37c06f3d79a1cb5fd61e3556676634b9d2f%3Ffields%3DcitationCount)

â€‹     

The robustness of CLIP is determined by dataset diversityâ€”not by contrastive learning or language supervision.





## **DINO: DETR with Improved DeNoising Anchor  Boxes for End-to-End Object Detection**

[`semanticscholar`](https://www.semanticscholar.org/paper/9dc481ec44178e797466bbad968071917842156b)  [`Paper`](https://www.semanticscholar.org/paper/9dc481ec44178e797466bbad968071917842156b)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9dc481ec44178e797466bbad968071917842156b%3Ffields%3DcitationCount)

â€‹     

![image-20250131122802813](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250131122802929.png)

ç»“åˆ Transformer å’Œ å¯¹æ¯”åŽ»å™ªè¿›è¡Œå­¦ä¹  





## **GLEAN: Generative Latent Bank for Image  Super-Resolution and Beyond**

[`semanticscholar`](https://www.semanticscholar.org/paper/3bb06c310573ad3749ae855e0b9ebdb4322918f8)  [`Paper`](https://www.semanticscholar.org/paper/3bb06c310573ad3749ae855e0b9ebdb4322918f8)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3bb06c310573ad3749ae855e0b9ebdb4322918f8%3Ffields%3DcitationCount)

â€‹     

![image-20250131123820365](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250131123820488.png)



![image-20250131123903661](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250131123903782.png)



## **VQGAN-CLIP: Open Domain Image Generation  and Editing with Natural Language Guidance**

[`semanticscholar`](https://www.semanticscholar.org/paper/6979ce65b9f657672cd3a0b9217ead51511c1838)  [`Paper`](https://www.semanticscholar.org/paper/6979ce65b9f657672cd3a0b9217ead51511c1838)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6979ce65b9f657672cd3a0b9217ead51511c1838%3Ffields%3DcitationCount)

â€‹     

è®ºæ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯ **åˆ©ç”¨å·²ç»è®­ç»ƒå¥½çš„ CLIP ä½œä¸ºâ€œå›¾åƒ-æ–‡æœ¬åŒ¹é…è¯„åˆ†å™¨â€ï¼Œæ¥æŒ‡å¯¼ VQGAN ç”Ÿæˆç¬¦åˆæ–‡æœ¬æè¿°çš„å›¾åƒ**ã€‚

![image-20250131124908519](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250131124908616.png)



## **Language Driven Image Editing via Transformers**

[`semanticscholar`](https://www.semanticscholar.org/paper/753b96c3251c68b515ccab71dd7e6f775bccbb12)  [`Paper`](https://www.semanticscholar.org/paper/753b96c3251c68b515ccab71dd7e6f775bccbb12)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F753b96c3251c68b515ccab71dd7e6f775bccbb12%3Ffields%3DcitationCount)

âœ… **è¾“å…¥ï¼š** **æ–‡æœ¬æŒ‡ä»¤ + æºå›¾åƒ token**
âœ… **æ¨¡åž‹å¤„ç†æ–¹å¼ï¼š** **GPT-2 ä½œä¸º Seq2Seq æ¨¡åž‹ï¼Œé¢„æµ‹ç›®æ ‡å›¾åƒ token**
âœ… **è¾“å‡ºï¼š** **ç›®æ ‡å›¾åƒçš„ tokenï¼Œç»è¿‡ VQ-VAE åå‘è§£ç æˆæœ€ç»ˆçš„å›¾åƒ**

   **GPT-2 ä½œä¸º Transformer ä¸ç›´æŽ¥ç”Ÿæˆåƒç´ ï¼Œè€Œæ˜¯ç”Ÿæˆå›¾åƒ tokenï¼ŒVQ-VAE è´Ÿè´£æœ€ç»ˆçš„å›¾åƒé‡å»ºã€‚**   



## **Towards Counterfactual Image Manipulation via CLIP**

[`semanticscholar`](https://www.semanticscholar.org/paper/9c7ec94901efcbc22656cb0d9924d1716578bfb1)  [`Paper`](https://www.semanticscholar.org/paper/9c7ec94901efcbc22656cb0d9924d1716578bfb1)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9c7ec94901efcbc22656cb0d9924d1716578bfb1%3Ffields%3DcitationCount)

â€‹     

![image-20250131170254533](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250131170254765.png)

![image-20250131170348551](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250131170348665.png)



![image-20250131171228960](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250131171229075.png)



>
>
># **å¦‚ä½•æž„é€ æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Ÿ**
>
>åœ¨ **CF-CLIP** è®ºæ–‡ä¸­ï¼Œ**Counterfactual Image Manipulation**ï¼ˆåäº‹å®žå›¾åƒç¼–è¾‘ï¼‰æ„å‘³ç€**æ•°æ®åº“ä¸­å¯èƒ½æ²¡æœ‰ç›´æŽ¥å¯¹åº”çš„ç›®æ ‡å›¾åƒ**ï¼ˆä¾‹å¦‚â€œè“è‰²ç‹—â€ï¼‰ã€‚é‚£ä¹ˆï¼Œåœ¨æ²¡æœ‰çœŸå®žæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•æž„é€ **æ­£æ ·æœ¬ï¼ˆPositive Pairsï¼‰**å’Œ**è´Ÿæ ·æœ¬ï¼ˆNegative Pairsï¼‰**æ¥è¿›è¡Œå¯¹æ¯”å­¦ä¹ å‘¢ï¼Ÿ
>
>è®ºæ–‡ä½¿ç”¨äº†**å¯¹æ¯”æŸå¤±ï¼ˆContrastive Lossï¼ŒCLIP-NCEï¼‰**æ¥ä¼˜åŒ–**æ–‡æœ¬-å›¾åƒåŒ¹é…**ï¼Œå³ç¡®ä¿**ç¼–è¾‘åŽçš„å›¾åƒç¬¦åˆæ–‡æœ¬æè¿°ï¼ŒåŒæ—¶é¿å…ä¸å¿…è¦çš„ä¿®æ”¹**ã€‚æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„æž„é€ æ–¹å¼å¦‚ä¸‹ï¼š
>
>---
>
>## **1. å¦‚ä½•æž„é€ æ­£æ ·æœ¬ï¼ˆPositive Pairsï¼‰**
>**æ­£æ ·æœ¬çš„ç›®æ ‡**ï¼šè®©**ç¼–è¾‘åŽçš„å›¾åƒ**å’Œç›®æ ‡æ–‡æœ¬ä¿æŒé«˜åº¦ç›¸ä¼¼ï¼ˆå³ç¬¦åˆç›®æ ‡è¯­ä¹‰ï¼‰ã€‚
>
>### **(1) ç”Ÿæˆçš„ Counterfactual å›¾åƒ**
>ç”±äºŽæ•°æ®åº“ä¸­**ä¸å­˜åœ¨ç›´æŽ¥åŒ¹é…çš„çœŸå®žåäº‹å®žå›¾åƒ**ï¼ˆå¦‚â€œè“è‰²ç‹—â€ï¼‰ï¼Œè®ºæ–‡é‡‡ç”¨ä»¥ä¸‹æ–¹æ³•æž„é€ **æ­£æ ·æœ¬**ï¼š
>- **ä½¿ç”¨å½“å‰è¿­ä»£çš„ StyleGAN ç”Ÿæˆçš„å›¾åƒ**ï¼š
>  - ç»è¿‡**æ–‡æœ¬å¼•å¯¼çš„ StyleGAN ç”Ÿæˆ**çš„æ–°å›¾åƒ $ I' $ã€‚
>  - è¿™ä¸ªå›¾åƒæ˜¯ç”±**$ w' = w + \delta w $ ç”Ÿæˆçš„**ï¼Œç†è®ºä¸Šåº”è¯¥ç¬¦åˆæ–‡æœ¬æè¿°ï¼ˆä½†å¯èƒ½ä»æœ‰éƒ¨åˆ†åå·®ï¼‰ã€‚
>  - è®¡ç®—æ–°å›¾åƒçš„ CLIP åµŒå…¥ï¼š
>    $$
>    e_{\text{target}} = \text{CLIP}_{\text{image}}(I')
>    $$
>  - è¿™ä¸ª $ e_{\text{target}} $ ä½œä¸º**æ­£æ ·æœ¬**ï¼Œå¸Œæœ›å®ƒæŽ¥è¿‘æ–‡æœ¬åµŒå…¥ $ e_{\text{text}} $ã€‚
>
>### **(2) ä½¿ç”¨ CLIP ä½œä¸ºç›‘ç£**
>ç”±äºŽ CLIP é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å·²ç»å­¦ä¹ äº†**å¤§è§„æ¨¡çš„è§†è§‰-æ–‡æœ¬å…³ç³»**ï¼Œå®ƒèƒ½æä¾›ä¸€å®šçš„ç›‘ç£ä¿¡å·ï¼š
>- ç›´æŽ¥è®¡ç®—ç›®æ ‡æ–‡æœ¬ $ e_{\text{text}} $ å’Œ**å½“å‰ç”Ÿæˆçš„ $ e_{\text{target}} $ ä¹‹é—´çš„ç›¸ä¼¼åº¦**ï¼Œå¹¶è¿›è¡Œä¼˜åŒ–ã€‚
>
>---
>
>## **2. å¦‚ä½•æž„é€ è´Ÿæ ·æœ¬ï¼ˆNegative Pairsï¼‰**
>**è´Ÿæ ·æœ¬çš„ç›®æ ‡**ï¼šè®©**æœªç¼–è¾‘çš„åŽŸå§‹å›¾åƒ**ï¼ˆæˆ–å…¶ä»–é”™è¯¯ç¼–è¾‘çš„å›¾åƒï¼‰è¿œç¦»ç›®æ ‡æ–‡æœ¬ã€‚
>
>### **(1) æœªç¼–è¾‘çš„åŽŸå§‹å›¾åƒ**
>- åŽŸå§‹å›¾åƒ $ I_{\text{src}} $ ä»£è¡¨çš„æ˜¯**æ²¡æœ‰ç»è¿‡ä¿®æ”¹çš„åŽŸå§‹æ ·æœ¬**ï¼Œå®ƒçš„ CLIP è¡¨å¾ä¸ºï¼š
> $$
>  e_{\text{src}} = \text{CLIP}_{\text{image}}(I_{\text{src}})
> $$
>- ç”±äºŽåŽŸå§‹å›¾åƒ**ä¸ç¬¦åˆç›®æ ‡æ–‡æœ¬æè¿°**ï¼Œå› æ­¤å®ƒåº”è¯¥è¿œç¦» $ e_{\text{text}} $ã€‚
>- è®¡ç®—å…¶ä½™çš„è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦ï¼š
> $$
>  \cos(e_{\text{src}}, e_{\text{text}})
> $$
>- ç›®æ ‡æ˜¯æœ€å°åŒ–è¿™ä¸ªç›¸ä¼¼åº¦ï¼Œä½¿å¾—åŽŸå§‹å›¾åƒçš„è¡¨å¾è¿œç¦»æ–‡æœ¬æè¿°ã€‚
>
>### **(2) å…¶ä»–é”™è¯¯ä¿®æ”¹çš„æ ·æœ¬**
>- åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯èƒ½ä¼š**ç”Ÿæˆä¸€äº›é”™è¯¯çš„ç¼–è¾‘ç»“æžœ**ï¼ˆä¾‹å¦‚é¢œè‰²æ²¡å˜ï¼Œæˆ–è€…å˜å¾—è¿‡åº¦æžç«¯ï¼‰ã€‚
>- è¿™äº›é”™è¯¯çš„å›¾åƒä¹Ÿå¯ä»¥è¢«ç”¨ä½œè´Ÿæ ·æœ¬ï¼š
>  - æ¯”å¦‚â€œè“è‰²ç‹—â€ä»»åŠ¡ä¸­ï¼Œå¯èƒ½ StyleGAN ç”Ÿæˆçš„æ˜¯ä¸€åªå¸¦è“è‰²å…‰ç…§çš„ç‹—ï¼Œè€Œä¸æ˜¯çš®è‚¤çœŸçš„å˜è“ã€‚
>  - è¿™ç§**ä¸å®Œå…¨ç¬¦åˆæ–‡æœ¬æè¿°**çš„å›¾åƒ $ I_{\text{wrong}} $ ä¹Ÿä¼šè¢«ç”¨ä½œè´Ÿæ ·æœ¬ï¼š
>    $$
>    e_{\text{wrong}} = \text{CLIP}_{\text{image}}(I_{\text{wrong}})
>    $$
>  - ç›®æ ‡æ˜¯è®© $ e_{\text{wrong}} $ è¿œç¦» $ e_{\text{text}} $ã€‚
>
>---
>
>## **3. CLIP-NCE æŸå¤±å¦‚ä½•ä¼˜åŒ– Counterfactual ç”Ÿæˆï¼Ÿ**
>æœ€ç»ˆï¼Œä½¿ç”¨ **å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆCLIP-NCE Lossï¼‰** æ¥ä¼˜åŒ– $ \delta w $ï¼Œè®©æ¨¡åž‹æ›´æŽ¥è¿‘æ–‡æœ¬ç›®æ ‡ï¼š
>$$
>\mathcal{L}_{\text{CLIP-NCE}} = - \log \frac{\exp(\cos(e_{\text{target}}, e_{\text{text}}))}{\sum_{n} \exp(\cos(e_{\text{neg}}^n, e_{\text{text}}))}
>$$
>å…¶ä¸­ï¼š
>- **$ e_{\text{target}} $**ï¼ˆæ­£æ ·æœ¬ï¼‰æ˜¯ç»è¿‡ **$ w' $ ç”Ÿæˆçš„æœ€ç»ˆå›¾åƒ**ï¼Œå¸Œæœ›å®ƒæœ€å¤§åŒ–ä¸Žæ–‡æœ¬çš„ç›¸ä¼¼åº¦ã€‚
>- **$ e_{\text{neg}}^n $**ï¼ˆè´Ÿæ ·æœ¬ï¼‰åŒ…æ‹¬ï¼š
>  1. **$ e_{\text{src}} $**ï¼ˆæœªç¼–è¾‘åŽŸå›¾ï¼‰
>  2. **$ e_{\text{wrong}} $**ï¼ˆé”™è¯¯ç¼–è¾‘å›¾ï¼‰
>
>ä¼˜åŒ–ç›®æ ‡ï¼š
>- **æœ€å¤§åŒ– $ \cos(e_{\text{target}}, e_{\text{text}}) $**ï¼Œè®©ç›®æ ‡å›¾åƒæ›´ç¬¦åˆæ–‡æœ¬æè¿°ã€‚
>- **æœ€å°åŒ– $ \cos(e_{\text{neg}}, e_{\text{text}}) $**ï¼Œè®©é”™è¯¯ç¼–è¾‘å’ŒåŽŸå›¾è¿œç¦»æ–‡æœ¬æè¿°ã€‚
>
>---
>
>## **4. æ€»ç»“**
>### **å¦‚ä½•æž„é€ æ­£æ ·æœ¬ï¼Ÿ**
>âœ… **æ­£æ ·æœ¬ = ç”± StyleGAN ç”Ÿæˆçš„æ–°å›¾åƒ**ï¼ˆç¬¦åˆç›®æ ‡æ–‡æœ¬ï¼‰  
>- ç›´æŽ¥ä½¿ç”¨ **$ w' = w + \delta w $ ç”Ÿæˆçš„å›¾åƒ $ I' $** ä½œä¸ºæ­£æ ·æœ¬ã€‚
>- ç”¨ CLIP è®¡ç®—å…¶è¡¨å¾ $ e_{\text{target}} $ å¹¶ä¼˜åŒ–ï¼Œä½¿å…¶æŽ¥è¿‘æ–‡æœ¬åµŒå…¥ $ e_{\text{text}} $ã€‚
>
>### **å¦‚ä½•æž„é€ è´Ÿæ ·æœ¬ï¼Ÿ**
>âœ… **è´Ÿæ ·æœ¬ = ä¸ç¬¦åˆæ–‡æœ¬æè¿°çš„å›¾åƒ**ï¼ŒåŒ…æ‹¬ï¼š
>1. **åŽŸå§‹è¾“å…¥å›¾åƒ $ I_{\text{src}} $**ï¼ˆæ²¡æœ‰è¢«ä¿®æ”¹ï¼‰ã€‚
>2. **é”™è¯¯ä¿®æ”¹çš„å›¾åƒ $ I_{\text{wrong}} $**ï¼ˆç¼–è¾‘å¤±è´¥çš„æ ·æœ¬ï¼‰ã€‚
>
>### **å¦‚ä½•ä¼˜åŒ–ï¼Ÿ**
>âœ… **CLIP-NCE é€šè¿‡å¯¹æ¯”æŸå¤±ä¼˜åŒ– $ \delta w $**ï¼Œç¡®ä¿ï¼š
>- **ç›®æ ‡å›¾åƒ $ I' $ ç¬¦åˆæ–‡æœ¬æè¿°**ï¼ˆæé«˜ç›¸ä¼¼åº¦ï¼‰ã€‚
>- **åŽŸå›¾å’Œé”™è¯¯ä¿®æ”¹çš„å›¾åƒè¿œç¦»æ–‡æœ¬æè¿°**ï¼ˆé™ä½Žç›¸ä¼¼åº¦ï¼‰ã€‚
>
>---
>
>## **5. è¿™æ ·åšçš„ä¼˜åŠ¿**
>âœ… **å³ä½¿æ²¡æœ‰ ground truthï¼ˆæ•°æ®åº“é‡Œæ²¡æœ‰è“è‰²ç‹—ï¼‰ï¼Œä¹Ÿèƒ½è¿›è¡Œåäº‹å®žç¼–è¾‘ï¼**  
>âœ… **é¿å… CLIP ä½œå¼Šï¼Œç¡®ä¿ç¼–è¾‘æ•ˆæžœçœŸå®žå¯è§ï¼**  
>âœ… **è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­è¿­ä»£ï¼Œæœ€ç»ˆç”Ÿæˆç¬¦åˆæ–‡æœ¬æè¿°çš„å›¾åƒï¼**  
>
>ðŸš€ **CF-CLIP é€šè¿‡ CLIP-NCE æˆåŠŸåœ°å®žçŽ°äº† Counterfactual Image Manipulationï¼Œä½¿å¾—åäº‹å®žç¼–è¾‘æˆä¸ºå¯èƒ½ï¼**





## **CLIP4IDC: CLIP for Image Difference Captioning**

[`semanticscholar`](https://www.semanticscholar.org/paper/cb01ca278cac5fd924f2180ff4dff8be34d14083)  [`Paper`](https://www.semanticscholar.org/paper/cb01ca278cac5fd924f2180ff4dff8be34d14083)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcb01ca278cac5fd924f2180ff4dff8be34d14083%3Ffields%3DcitationCount)

â€‹     

![image-20250131173300682](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250131173300828.png)



># **CLIP4IDC: Training and Inference Details**
>
>## **1. Training: Input, Output, and Updated Components**
>The training process of **CLIP4IDC** involves two main stages: **Retrieval Pretraining (IDC-Specific Adaptation)** and **Fine-Tuning for Caption Generation**. 
>
>### **Training Input and Output**
>- **Input:**
>  - A **pair of images**: $(I_{\text{before}}, I_{\text{after}})$ representing two versions of the same scene with slight differences.
>  - A **textual caption** $T$ describing the difference between the two images.
>
>- **Output:**
>  - A **difference-aware embedding** for the image pair, which aligns well with the textual description.
>  - A **generated textual caption** that describes the differences.
>
>### **Updated Components During Training**
>- **CLIPâ€™s vision encoder**: Fine-tuned to improve its capability in capturing **fine-grained image differences**.
>- **Text embedding space**: Updated through **contrastive learning** to better associate text with image differences.
>- **Transformer-based captioning model**: Trained from scratch to generate captions based on difference-aware embeddings.
>
>---
>
>## **2. Training Workflow: From Input to Output**
>### **Step 1: IDC-Specific Adaptation (Retrieval Pretraining)**
>1. **Image Difference Representation Extraction**:
>   - The **two images** $(I_{\text{before}}, I_{\text{after}})$ are encoded using CLIPâ€™s vision encoder.
>   - Their embeddings are **combined** to form a **difference-aware embedding** $v$.
>
>2. **Contrastive Learning (Image-Pair-to-Text & Text-to-Image-Pair Retrieval)**:
>   - CLIP text encoder processes the **difference caption** $T$, producing an embedding $g$.
>   - **Contrastive loss** ensures that:
>     - The image difference embedding $v$ is **closer to the corresponding text embedding** $g$.
>     - Unrelated image-text pairs are pushed further apart.
>
>   The retrieval contrastive loss is defined as:
> $$
>   \mathcal{L}_{\text{IP-T}} =
>   -\frac{1}{B} \sum_{i=1}^{B} \log \frac{\exp(s(v_i, g_i)/\tau)}{\sum_{j=1}^{B} \exp(s(v_i, g_j)/\tau)}
> $$
> $$
>   \mathcal{L}_{\text{T-IP}} =
>   -\frac{1}{B} \sum_{i=1}^{B} \log \frac{\exp(s(v_i, g_i)/\tau)}{\sum_{j=1}^{B} \exp(s(v_j, g_i)/\tau)}
> $$
>
>3. **Updating CLIP Embeddings**:
>   - CLIPâ€™s visual and text encoders are fine-tuned to ensure **better alignment of image differences and textual descriptions**.
>
>### **Step 2: Fine-Tuning for Caption Generation**
>1. **Feature Extraction**:
>   - The **fine-tuned CLIP encoder** processes the image pair $(I_{\text{before}}, I_{\text{after}})$ and outputs a **difference-aware feature**.
>  
>2. **Caption Generation**:
>   - The extracted feature is fed into a **Transformer-based captioning model**, which generates a textual caption $T'$.
>
>3. **Loss Optimization**:
>   - A **cross-entropy (XE) loss** is applied to optimize the generated caption:
>     $$
>     \mathcal{L}_{\text{caption}} = -\sum_{t=1}^{T} \log P(y_t | y_{1:t-1}, v)
>     $$
>   - This fine-tunes the **Transformer decoder** while keeping the updated **CLIP embeddings**.
>
>---
>
>## **3. Inference: Input and Output**
>After training, the model is used for **inference (image difference captioning on new samples)**.
>
>### **Inference Input and Output**
>- **Input:**
>  - A pair of images $(I_{\text{before}}, I_{\text{after}})$.
>
>- **Output:**
>  - A generated caption $T'$ describing the differences between the two images.
>
>### **Inference Workflow**
>1. **Feature Extraction**:
>   - The two images are passed through the **fine-tuned CLIP visual encoder**.
>   - A **difference-aware embedding** $v$ is computed.
>
>2. **Caption Generation**:
>   - The extracted feature $v$ is fed into the **Transformer decoder**.
>   - The decoder **autoregressively generates the caption** word by word.
>
>3. **Final Output**:
>   - The generated caption $T'$ is returned as the final output.
>
>---
>
>## **4. Summary**
>| **Stage**                 | **Input**                 | **Updated Components**     | **Output**                        |
>| ------------------------- | ------------------------- | -------------------------- | --------------------------------- |
>| **Retrieval Pretraining** | Image pair + text caption | CLIP vision & text encoder | Improved image-text alignment     |
>| **Caption Fine-Tuning**   | Image pair                | Transformer decoder        | Generated caption                 |
>| **Inference**             | Image pair                | None (uses trained model)  | Caption describing the difference |
>
>ðŸš€ **CLIP4IDC effectively improves IDC tasks by fine-tuning CLIP for better image difference awareness and utilizing a Transformer decoder for high-quality caption generation.**
>
>



## **Matryoshka Representation Learning**

[`semanticscholar`](https://www.semanticscholar.org/paper/020b09bd0757bf41a8b3c99300feb223404035ed)  [`Paper`](https://www.semanticscholar.org/paper/020b09bd0757bf41a8b3c99300feb223404035ed)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F020b09bd0757bf41a8b3c99300feb223404035ed%3Ffields%3DcitationCount)







## **Alpha-CLIP: A CLIP Model Focusing on Wherever You Want**

[`semanticscholar`](https://www.semanticscholar.org/paper/d198a5a1a0c6e31bd0ad70658c8c2a74b8753aed)  [`Paper`](https://www.semanticscholar.org/paper/d198a5a1a0c6e31bd0ad70658c8c2a74b8753aed)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fd198a5a1a0c6e31bd0ad70658c8c2a74b8753aed%3Ffields%3DcitationCount)

2023    Computer Vision and Pattern Recognition 



![image-20250131180306929](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250131180307114.png)





>
>
>**Alpha é€šé“** åœ¨ **Alpha-CLIP** ä¸­æ˜¯ä¸€ä¸ª **é¢å¤–çš„è¾“å…¥é€šé“**ï¼Œç”¨äºŽ**å¼•å¯¼æ¨¡åž‹å…³æ³¨å›¾åƒä¸­çš„ç‰¹å®šåŒºåŸŸ**ï¼Œä»Žè€Œå¢žå¼º CLIP åœ¨ **åŒºåŸŸæ„ŸçŸ¥ä»»åŠ¡**ï¼ˆå¦‚ç›®æ ‡è¯†åˆ«ã€å›¾åƒç”Ÿæˆï¼‰ä¸Šçš„èƒ½åŠ›ã€‚
>
>**1. ä»€ä¹ˆæ˜¯ Alpha é€šé“ï¼Ÿ**
>
>åœ¨ **è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†** ä¸­ï¼Œå¸¸è§çš„ **RGB å›¾åƒ** ç”± **ä¸‰ä¸ªé€šé“ï¼ˆRed, Green, Blueï¼‰** ç»„æˆï¼Œæ¯ä¸ªé€šé“è¡¨ç¤ºåƒç´ çš„é¢œè‰²ä¿¡æ¯ã€‚
>**Alpha é€šé“** æ˜¯ **ç¬¬å››ä¸ªé€šé“**ï¼Œç”¨äºŽè¡¨ç¤ºå›¾åƒçš„ **é€æ˜Žåº¦ï¼ˆOpacityï¼‰** æˆ– **æƒé‡æŽ©ç ï¼ˆMaskï¼‰**ã€‚
>
>åœ¨ **Alpha-CLIP** ä¸­ï¼Œ**Alpha é€šé“å¹¶ä¸ç”¨äºŽé€æ˜Žåº¦**ï¼Œè€Œæ˜¯ç”¨äºŽ **æ ‡è®°æ¨¡åž‹åº”è¯¥å…³æ³¨çš„åŒºåŸŸ**ï¼š
>
>- **Alpha å€¼ä¸º 1ï¼ˆç™½è‰²ï¼‰** çš„åŒºåŸŸæ˜¯ **éœ€è¦å…³æ³¨çš„ç›®æ ‡åŒºåŸŸ**ã€‚
>- **Alpha å€¼ä¸º 0ï¼ˆé»‘è‰²ï¼‰** çš„åŒºåŸŸæ˜¯ **èƒŒæ™¯ï¼Œæ¨¡åž‹ä¸éœ€è¦é‡ç‚¹å…³æ³¨**ã€‚
>
>å› æ­¤ï¼Œ**Alpha é€šé“æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª "å…³æ³¨å¼•å¯¼ä¿¡å·ï¼ˆAttention Mapï¼‰"**ï¼Œå‘Šè¯‰ CLIP **å“ªäº›éƒ¨åˆ†æ›´é‡è¦**ï¼Œä»Žè€Œæå‡æ¨¡åž‹çš„åŒºåŸŸçº§ç†è§£èƒ½åŠ›ã€‚
>
>**2. Alpha é€šé“åœ¨ Alpha-CLIP ä¸­çš„ä½œç”¨**
>
>åœ¨ **åŽŸå§‹ CLIP** ä¸­ï¼Œæ¨¡åž‹ä¼šå…³æ³¨æ•´ä¸ªå›¾åƒï¼Œå¹¶ä»Žæ•´ä½“ä¸Šå­¦ä¹ è§†è§‰ç‰¹å¾ï¼Œä½†å®ƒ **æ— æ³•åŒºåˆ†å“ªäº›åŒºåŸŸæ˜¯é‡è¦çš„**ã€‚Alpha-CLIP é€šè¿‡ **Alpha é€šé“** å¼•å¯¼æ¨¡åž‹ **ä¸“æ³¨äºŽç‰¹å®šåŒºåŸŸ**ï¼Œé¿å…å¯¹æ— å…³èƒŒæ™¯çš„å¹²æ‰°ï¼Œä»Žè€Œæé«˜æ¨¡åž‹åœ¨ **åŒºåŸŸçº§ä»»åŠ¡ï¼ˆå¦‚ç›®æ ‡è¯†åˆ«ã€Referring Expressionã€2D/3D ç”Ÿæˆï¼‰** ä¸Šçš„æ€§èƒ½ã€‚
>
>**ç¤ºä¾‹**
>
>å‡è®¾æœ‰ä¸€å¼ åŒ…å« **çŒ«å’Œç‹—** çš„å›¾ç‰‡ï¼Œè€Œæˆ‘ä»¬æƒ³è®©æ¨¡åž‹å…³æ³¨ **çŒ«**ï¼š
>
>- **æ™®é€š CLIP**ï¼šä¼šåŒæ—¶å¤„ç† **çŒ«å’Œç‹—**ï¼Œå¯èƒ½ä¼šå—åˆ°å¹²æ‰°ã€‚
>- Alpha-CLIPï¼š
>  - **Alpha é€šé“ = 1ï¼ˆç™½è‰²ï¼‰** â†’ é€‰å®šçš„ "çŒ«" åŒºåŸŸ
>  - **Alpha é€šé“ = 0ï¼ˆé»‘è‰²ï¼‰** â†’ "ç‹—" å’ŒèƒŒæ™¯åŒºåŸŸ
>  - è¿™æ ·ï¼Œæ¨¡åž‹åœ¨è®¡ç®— CLIP ç›¸ä¼¼åº¦æ—¶ï¼Œå°±ä¼š**ä¼˜å…ˆå…³æ³¨çŒ«**ï¼Œå¿½ç•¥ç‹—å’ŒèƒŒæ™¯ã€‚
>
>**3. Alpha é€šé“çš„è¾“å…¥æ ¼å¼**
>
>åœ¨è®­ç»ƒå’ŒæŽ¨ç†æ—¶ï¼ŒAlpha-CLIP çš„è¾“å…¥æ˜¯ **RGBA å›¾åƒ**ï¼Œå…¶ä¸­ï¼š
>
>- **RGBï¼ˆ3 é€šé“ï¼‰**ï¼šå›¾åƒçš„é¢œè‰²ä¿¡æ¯
>- **Alphaï¼ˆ1 é€šé“ï¼‰**ï¼šè¡¨ç¤ºå…³æ³¨åŒºåŸŸçš„æŽ©ç 
>
>é€šå¸¸ï¼ŒAlpha é€šé“çš„æ ¼å¼å¦‚ä¸‹ï¼š
>
>| **åƒç´ ç‚¹ä½ç½®** | **RGB å€¼ï¼ˆé¢œè‰²ï¼‰** | **Alpha å€¼**ï¼ˆå…³æ³¨åº¦ï¼‰ |
>| -------------- | ------------------ | ---------------------- |
>| ç›®æ ‡åŒºåŸŸï¼ˆçŒ«ï¼‰ | (255, 100, 50)     | 1.0ï¼ˆå®Œå…¨å…³æ³¨ï¼‰        |
>| èƒŒæ™¯åŒºåŸŸï¼ˆç‹—ï¼‰ | (120, 50, 200)     | 0.0ï¼ˆå®Œå…¨å¿½ç•¥ï¼‰        |
>| è¾¹ç¼˜æ¨¡ç³Šéƒ¨åˆ†   | (180, 90, 40)      | 0.5ï¼ˆéƒ¨åˆ†å…³æ³¨ï¼‰        |
>
>åœ¨æ¨¡åž‹è®­ç»ƒæ—¶ï¼ŒAlpha é€šé“çš„ä¿¡æ¯ä¼šè¢«è¾“å…¥åˆ° **Alpha Conv å±‚**ï¼Œç”¨äºŽè°ƒæ•´ CLIP å…³æ³¨çš„åŒºåŸŸã€‚
>
>**4. Alpha é€šé“ vs å…¶ä»–æ–¹æ³•**
>
>| **æ–¹æ³•**                   | **æ–¹å¼**            | **é—®é¢˜**                           |
>| -------------------------- | ------------------- | ---------------------------------- |
>| **è£å‰ªï¼ˆCroppingï¼‰**       | ä»…ä¿ç•™ç›®æ ‡åŒºåŸŸ      | **æŸå¤±èƒŒæ™¯ä¿¡æ¯**ï¼Œå½±å“ç†è§£         |
>| **åƒç´ çº§é®æŒ¡ï¼ˆMaskingï¼‰**  | ç”¨é»‘è‰²å¡«å……èƒŒæ™¯      | **ä¸¢å¤±èƒŒæ™¯ä¸Šä¸‹æ–‡**ï¼Œå½±å“è¯­ä¹‰       |
>| **çº¢è‰²åœ†åœˆï¼ˆRed Circleï¼‰** | åœ¨ç›®æ ‡ä¸ŠåŠ åœˆ        | **æ”¹å˜åŽŸå›¾ï¼Œå½±å“æ¨¡åž‹æ³›åŒ–**         |
>| **Alpha-CLIPï¼ˆOursï¼‰**     | é¢å¤–è¾“å…¥ Alpha é€šé“ | **ä¿ç•™èƒŒæ™¯ä¿¡æ¯ï¼ŒåŒæ—¶å¼ºè°ƒç›®æ ‡åŒºåŸŸ** |
>
>ç›¸æ¯” **è£å‰ªã€åƒç´ é®æŒ¡ç­‰æ–¹æ³•**ï¼ŒAlpha é€šé“çš„ä¼˜åŠ¿æ˜¯ï¼š
>
>1. **ä¸ä¿®æ”¹åŽŸå§‹å›¾åƒ**ï¼Œä»…å¢žåŠ é¢å¤–å¼•å¯¼ä¿¡æ¯ã€‚
>2. **å¯è¿›è¡Œæ›´ç»†ç²’åº¦çš„åŒºåŸŸå…³æ³¨**ï¼ˆä¸ä»…ä»…æ˜¯çŸ©å½¢æ¡†ï¼Œè¿˜èƒ½æ˜¯ä»»æ„å½¢çŠ¶çš„ maskï¼‰ã€‚
>3. **ä¿æŒä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±





![image-20250131180327709](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250131180327885.png)



>Alpha-CLIP ä¸»è¦åœ¨ **CLIP çš„å›¾åƒç¼–ç å™¨ï¼ˆViTï¼‰** ç»“æž„ä¸Šåšäº†å¦‚ä¸‹ä¿®æ”¹ï¼š
>
>- **å¼•å…¥ Alpha é€šé“**ï¼šåœ¨åŽŸæœ¬ **RGB è¾“å…¥** ä¹‹å¤–ï¼Œå¢žåŠ  **Alpha é€šé“**ï¼Œå½¢æˆ **RGBA è¾“å…¥**ã€‚
>- **Alpha Conv å±‚**ï¼šå¢žåŠ ä¸€ä¸ª **Alpha Conv**ï¼ˆä¸ŽåŽŸå§‹ **RGB Conv** å¹¶è¡Œï¼‰ï¼Œç”¨äºŽå¤„ç† Alpha é€šé“ä¿¡æ¯ã€‚
>- **Transformer ç»“æž„**ï¼šç»§æ‰¿åŽŸæœ‰ CLIP çš„ **Transformer å—**ï¼Œä½†å¯¹ä¸€éƒ¨åˆ†è¿›è¡Œ **å¾®è°ƒ** ä»¥é€‚åº” Alpha é€šé“è¾“å…¥ã€‚
>- **æ–‡æœ¬ç¼–ç å™¨ä¸å˜**ï¼šAlpha-CLIP **ä¿æŒ CLIP æ–‡æœ¬ç¼–ç å™¨ä¸å˜**ï¼Œåªå¾®è°ƒå›¾åƒç¼–ç å™¨ã€‚
>
>### **1.2 è®­ç»ƒè¿‡ç¨‹**
>
>**è®­ç»ƒè¾“å…¥ï¼š**
>
>1. **(RGBA å›¾åƒ, æ–‡æœ¬) å¯¹**ï¼ˆæ¥è‡ªæ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼‰
>2. **Alpha é€šé“ä¿¡æ¯**ï¼ˆæ ‡æ³¨ç›®æ ‡åŒºåŸŸï¼‰
>3. **CLIP æ–‡æœ¬åµŒå…¥å‘é‡**ï¼ˆç”± CLIP æ–‡æœ¬ç¼–ç å™¨è®¡ç®—ï¼‰
>
>**è®­ç»ƒç›®æ ‡ï¼š**
>
>- è®­ç»ƒ Alpha-CLIP çš„å›¾åƒç¼–ç å™¨ä½¿å…¶ï¼š
>  - ä»èƒ½å­¦ä¹  **å…¨å±€å›¾åƒç‰¹å¾**
>  - ä½†èƒ½ **æ›´å…³æ³¨ Alpha æŒ‡å®šçš„åŒºåŸŸ** è¿›è¡ŒåŒ¹é…
>
>**è®­ç»ƒæ­¥éª¤ï¼š**
>
>1. **æ–‡æœ¬ç¼–ç å™¨ï¼ˆå›ºå®šæƒé‡ï¼‰** è®¡ç®— **æ–‡æœ¬åµŒå…¥**ã€‚
>
>2. **å›¾åƒç¼–ç å™¨ï¼ˆViT + Alpha-Convï¼‰** å¤„ç† **RGBA è¾“å…¥** ç”Ÿæˆ **è§†è§‰ç‰¹å¾**ã€‚
>
>3. è®¡ç®— 
>
>   å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆContrastive Lossï¼‰ï¼š
>
>   
> $$
>   \mathcal{L} = -\sum_{(I, T)} \log \frac{\exp(\text{sim}(F_I, F_T)/\tau)}{\sum_{j} \exp(\text{sim}(F_I, F_{T_j})/\tau)}
> $$
>   
>
>   - å…¶ä¸­ $F_I$ æ˜¯ Alpha-CLIP æå–çš„å›¾åƒç‰¹å¾ï¼Œ$F_T$ æ˜¯æ–‡æœ¬ç‰¹å¾ã€‚
>   - ç›®æ ‡æ˜¯è®©é…å¯¹ (I, T) çš„ç›¸ä¼¼åº¦æœ€å¤§åŒ–ï¼Œè€Œéžé…å¯¹çš„ç›¸ä¼¼åº¦æœ€å°åŒ–ã€‚
>
>4. ä¼˜åŒ–è¿‡ç¨‹ï¼š
>
>   - **è®­ç»ƒ Alpha-Conv å±‚æƒé‡**
>   - **å¾®è°ƒ ViT Transformer éƒ¨åˆ†æƒé‡**
>   - **æ–‡æœ¬ç¼–ç å™¨ä¿æŒå†»ç»“**
>
>5. æ··åˆæ•°æ®è®­ç»ƒç­–ç•¥ï¼š
>
>   - è®¾å®š **10% è®­ç»ƒæ•°æ®** ä¸å¸¦ Alpha é€šé“ï¼ˆå³å…¨ 1ï¼‰ï¼Œç¡®ä¿æ¨¡åž‹ä»èƒ½è¿›è¡Œ **å…¨å±€å›¾åƒç†è§£**ã€‚
>
>**è®­ç»ƒæ›´æ–°çš„éƒ¨åˆ†ï¼š**
>
>- **Alpha-Conv å±‚ï¼ˆæ–°å¢žï¼‰**
>- **ViT Transformer å±‚ï¼ˆéƒ¨åˆ†å‚æ•°ï¼‰**
>- **å…¶ä½™éƒ¨åˆ†ï¼ˆæ–‡æœ¬ç¼–ç å™¨ & é¢„è®­ç»ƒçš„ CLIP å‚æ•°ï¼‰ä¸å˜**
>
>**è®­ç»ƒè¾“å‡ºï¼š**
>
>- **æ›´æ–°åŽçš„ Alpha-CLIP æ¨¡åž‹**
>- å…·å¤‡ **åŒºåŸŸæ„ŸçŸ¥èƒ½åŠ›** çš„ **å›¾åƒç¼–ç å™¨**ï¼ˆèƒ½å…³æ³¨ Alpha é€‰å®šåŒºåŸŸï¼‰







## **Tune-An-Ellipse: CLIP Has Potential to Find What You Want**

[`semanticscholar`](https://www.semanticscholar.org/paper/490b8fb5e9bc36d82ec2748c2347df7a17f76bda)  [`Paper`](https://www.semanticscholar.org/paper/490b8fb5e9bc36d82ec2748c2347df7a17f76bda)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F490b8fb5e9bc36d82ec2748c2347df7a17f76bda%3Ffields%3DcitationCount)

â€‹     

![](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250201102638723.png)



- æå‡ºäº† **å¯å¾®è§†è§‰æç¤ºæ–¹æ³•**ï¼Œä½¿ CLIP èƒ½å¤Ÿ **æ— éœ€å¤–éƒ¨ç›®æ ‡æ£€æµ‹å™¨** å³å¯è¿›è¡Œ **é›¶æ ·æœ¬å¯¹è±¡å®šä½**ã€‚
- é€šè¿‡ **æ¤­åœ†å‚æ•°ä¼˜åŒ–**ï¼Œæœ¬æ–¹æ³•èƒ½å¤Ÿ **é€æ­¥æ‹Ÿåˆç›®æ ‡åŒºåŸŸ**ï¼Œ

![image-20250201103400022](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250201103400194.png)







## **GUIDING INSTRUCTION-BASED IMAGE EDITING VIA  MULTIMODAL LARGE LANGUAGE MODELS**

[`semanticscholar`](https://www.semanticscholar.org/paper/092245d86b77181c36f972b1b7a17a59cd989c4a)  [`Paper`](https://www.semanticscholar.org/paper/092245d86b77181c36f972b1b7a17a59cd989c4a)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F092245d86b77181c36f972b1b7a17a59cd989c4a%3Ffields%3DcitationCount)

â€‹     

![image-20250202162148345](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202162148554.png)





![image-20250202120904808](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202120904978.png)



>
>
>### Training Model Architecture
>
>- **Architecture Components:**
>
>  - **Multimodal Large Language Model (MLLM)**: Processes input text and generates refined expressive instructions.
>  - **Edit Head (T)**: Converts textual instructions into latent visual features.
>  - **Stable Diffusion Model**: Performs the actual image editing in a latent space.
>
>- **Training Inputs and Outputs:**
>
>  - **Inputs:** Input image $V$, initial instruction $X$, and ground-truth goal image $O$.
>  - **Outputs:** Edited image $O'$ matching the goal image $O$ as closely as possible.
>
>- **Training Process:**
>
>  1. **Instruction Derivation:** The MLLM refines $X$ into an expressive instruction $E$.
>
>  2. **Latent Representation:** **The edit head** transforms $E$ into a latent visual representation $U$.
>
>  3. **Image Editing:** The diffusion model generates $O'$ from $V$ using $U$ as a guiding condition.
>
>  4. **Loss Computation:**
>
>     
>     $$
>     L_{ins} = \sum_{t=1}^{l} CELoss(w'_t, w_t)
>     $$
>             
>     $$
>     L_{edit} = \mathbb{E}[||\epsilon - \epsilon_\theta(z_t, t, V, \{u\})||^2]
>     $$
>     
>
>  5. **Optimization:**
>
>     
>     $$
>     L_{all} = L_{ins} + 0.5 \cdot L_{edit}
>     $$
>     
>
>     Updates occur in:
>        
>     - **MLLM:** Trains the word embeddings and LM head to refine instruction generation.
>     - **Edit Head:** Learns to map textual instructions to latent visual representations.
>     - **Diffusion Model:** Fine-tunes parameters for image editing based on guidance.
>
>### Inference Process
>
>- **Inputs:**
>  - Image $V$.
>  - Instruction $X$.
>- **Outputs:**
>  - Edited image $O'$.
>- **Inference Steps:**
>  1. **Instruction Processing:** The MLLM generates refined expressive instruction $E$.
>  2. **Latent Feature Transformation:** The edit head converts $E$ into latent features $U$.
>  3. **Image Editing:** The diffusion model generates $O'$ from $V$, guided by $U$.
>  4. **Final Output:** The edited image $O'$ is decoded and presented.





## **CLIP is Also a Good Teacher: A New Training  Framework for Inductive Zero-shot Semantic  Segmentation**

[`semanticscholar`](https://www.semanticscholar.org/paper/c9242e402a8c12d616b793661d22ed0d56a9f5e1)  [`Paper`](https://www.semanticscholar.org/paper/c9242e402a8c12d616b793661d22ed0d56a9f5e1)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc9242e402a8c12d616b793661d22ed0d56a9f5e1%3Ffields%3DcitationCount)

   

![image-20250202122244995](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202122245205.png)

  

![image-20250202122855065](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202122855270.png)





## **CONTROLLING VISION-LANGUAGE MODELS FOR  MULTI-TASK IMAGE RESTORATION**

[`semanticscholar`](https://www.semanticscholar.org/paper/5e7d9e031c7eeaaad102627a9894bf14bf240a63)  [`Paper`](https://www.semanticscholar.org/paper/5e7d9e031c7eeaaad102627a9894bf14bf240a63)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5e7d9e031c7eeaaad102627a9894bf14bf240a63%3Ffields%3DcitationCount)

â€‹     



   

  ![image-20250202124545090](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202124545295.png)











## **HQ-Edit: A High-Quality Dataset for  Instruction-based Image Editing**

[`semanticscholar`](https://www.semanticscholar.org/paper/09609bd28855fd9b27f043b4dbf509615229bd08)  [`Paper`](https://www.semanticscholar.org/paper/09609bd28855fd9b27f043b4dbf509615229bd08)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F09609bd28855fd9b27f043b4dbf509615229bd08%3Ffields%3DcitationCount)

â€‹     

![image-20250202124809045](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202124809271.png)







## **FastEdit: Fast Text-Guided Single-Image Editing via  Semantic-Aware Diffusion Fine-Tuning**

[`semanticscholar`](https://www.semanticscholar.org/paper/c2e5ebe5959a3aa96fd9ca65339e1d0df6b68120)  [`Paper`](https://www.semanticscholar.org/paper/c2e5ebe5959a3aa96fd9ca65339e1d0df6b68120)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc2e5ebe5959a3aa96fd9ca65339e1d0df6b68120%3Ffields%3DcitationCount)

2024    arXiv.org 

![image-20250202125218573](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202125218769.png)





## **InstructIR: High-Quality Image Restoration  Following Human Instructions**

[`semanticscholar`](https://www.semanticscholar.org/paper/03ad1a40a4399c8b77bbeaa389fcd14b10b322c0)  [`Paper`](https://www.semanticscholar.org/paper/03ad1a40a4399c8b77bbeaa389fcd14b10b322c0)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F03ad1a40a4399c8b77bbeaa389fcd14b10b322c0%3Ffields%3DcitationCount)

2024    European Conference on Computer Vision 



![image-20250202130232883](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202130233087.png)



![image-20250202130217023](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202130217234.png)



![image-20250202130340442](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202130340690.png)



![image-20250202130444563](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202130444759.png)



![image-20250202130626823](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202130627027.png)





## **GG-Editor: Locally Editing 3D Avatars with Multimodal Large Language Model Guidance**

[`semanticscholar`](https://www.semanticscholar.org/paper/2b6bf85a297e7d5586d46984247782e7279627f1)  [`Paper`](https://www.semanticscholar.org/paper/2b6bf85a297e7d5586d46984247782e7279627f1)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2b6bf85a297e7d5586d46984247782e7279627f1%3Ffields%3DcitationCount)

2024    ACM Multimedia 

![image-20250202132232266](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202132232483.png)



infer **reasonable local editing regions**.



## **CorrCLIP: Reconstructing Correlations in CLIP with Off-the-Shelf Foundation  Models for Open-Vocabulary Semantic Segmentation**

[`semanticscholar`](https://www.semanticscholar.org/paper/af2cb49fe124efd87b0cfce991dfb206e04cada5)  [`Paper`](https://www.semanticscholar.org/paper/af2cb49fe124efd87b0cfce991dfb206e04cada5)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Faf2cb49fe124efd87b0cfce991dfb206e04cada5%3Ffields%3DcitationCount)

2024    arXiv.org 



![image-20250202133106141](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202133106363.png)



## **UltraEdit: Instruction-based Fine-Grained Image Editing at Scale**

[`semanticscholar`](https://www.semanticscholar.org/paper/90c383413af5334f5d406b8c2fa4ca6c7fcaa97e)  [`Paper`](https://www.semanticscholar.org/paper/90c383413af5334f5d406b8c2fa4ca6c7fcaa97e)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F90c383413af5334f5d406b8c2fa4ca6c7fcaa97e%3Ffields%3DcitationCount)

2024    arXiv.org 

![image-20250202133412894](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202133413101.png)





## **TeD-Loc: Text Distillation for Weakly Supervised Object Localization**

[`semanticscholar`](https://www.semanticscholar.org/paper/061d4d21628ff865d7ab7f0b48d2328a03dc82fc)  [`Paper`](https://www.semanticscholar.org/paper/061d4d21628ff865d7ab7f0b48d2328a03dc82fc)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F061d4d21628ff865d7ab7f0b48d2328a03dc82fc%3Ffields%3DcitationCount)

2025     



![image-20250202140836586](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202140836800.png)



## **EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation**

[`semanticscholar`](https://www.semanticscholar.org/paper/5f41df027907a7e4241fb937d8280722b6a01bcd)  [`Paper`](https://www.semanticscholar.org/paper/5f41df027907a7e4241fb937d8280722b6a01bcd)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5f41df027907a7e4241fb937d8280722b6a01bcd%3Ffields%3DcitationCount)

â€‹     



## **Chat-Edit-3D: Interactive 3D Scene Editing  via Text Prompts**

[`semanticscholar`](https://www.semanticscholar.org/paper/c3033708bba45a64916732506afbeb51d602425b)  [`Paper`](https://www.semanticscholar.org/paper/c3033708bba45a64916732506afbeb51d602425b)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc3033708bba45a64916732506afbeb51d602425b%3Ffields%3DcitationCount)

2024    European Conference on Computer Vision 



![image-20250202141110373](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202141110592.png)





![image-20250202145532166](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202145532416.png)



## **InstructGIE: Towards Generalizable Image  Editing**

[`semanticscholar`](https://www.semanticscholar.org/paper/03c36f01ba2150955e1928e408143dfa6e6bd489)  [`Paper`](https://www.semanticscholar.org/paper/03c36f01ba2150955e1928e408143dfa6e6bd489)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F03c36f01ba2150955e1928e408143dfa6e6bd489%3Ffields%3DcitationCount)

2024    European Conference on Computer Vision 



![image-20250202150629250](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202150629474.png)





 





## **TurboEdit: Instant text-based image editing**

[`semanticscholar`](https://www.semanticscholar.org/paper/33584b9023c4cddc74c9925e33f6b89b99ff4eb6)  [`Paper`](https://www.semanticscholar.org/paper/33584b9023c4cddc74c9925e33f6b89b99ff4eb6)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F33584b9023c4cddc74c9925e33f6b89b99ff4eb6%3Ffields%3DcitationCount)

2024    European Conference on Computer Vision 

![image-20250202151350134](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202151350375.png)





## **CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary  semantic segmentation**

[`semanticscholar`](https://www.semanticscholar.org/paper/d38a00348487b02dad98782506fb8ebe31aef477)  [`Paper`](https://www.semanticscholar.org/paper/d38a00348487b02dad98782506fb8ebe31aef477)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fd38a00348487b02dad98782506fb8ebe31aef477%3Ffields%3DcitationCount)

2023    European Conference on Computer Vision 



![image-20250202152815371](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202152815590.png)



## **InstructPix2Pix: Learning to Follow Image Editing Instructions**

[`semanticscholar`](https://www.semanticscholar.org/paper/a2d2bbe4c542173662a444b33b76c66992697830)  [`Paper`](https://www.semanticscholar.org/paper/a2d2bbe4c542173662a444b33b76c66992697830)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fa2d2bbe4c542173662a444b33b76c66992697830%3Ffields%3DcitationCount)

â€‹     ![image-20250202153529226](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202153529455.png)



## **Adding Conditional Control to Text-to-Image Diffusion Models**

[`semanticscholar`](https://www.semanticscholar.org/paper/efbe97d20c4ffe356e8826c01dc550bacc405add)  [`Paper`](https://www.semanticscholar.org/paper/efbe97d20c4ffe356e8826c01dc550bacc405add)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fefbe97d20c4ffe356e8826c01dc550bacc405add%3Ffields%3DcitationCount)

â€‹     ![image-20250202153608611](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202153608825.png)



## **Segment Anything**

[`semanticscholar`](https://www.semanticscholar.org/paper/7470a1702c8c86e6f28d32cfa315381150102f5b)  [`Paper`](https://www.semanticscholar.org/paper/7470a1702c8c86e6f28d32cfa315381150102f5b)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7470a1702c8c86e6f28d32cfa315381150102f5b%3Ffields%3DcitationCount)

2023    IEEE International Conference on Computer Vision 



![image-20250202153915698](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202153915915.png)



## **CLIP is Also an Efficient Segmenter: A Text-Driven Approach for  Weakly Supervised Semantic Segmentation**

[`semanticscholar`](https://www.semanticscholar.org/paper/3a27dfb4b87f74c3c663cc42cec83ccd58f72f23)  [`Paper`](https://www.semanticscholar.org/paper/3a27dfb4b87f74c3c663cc42cec83ccd58f72f23)    ![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3a27dfb4b87f74c3c663cc42cec83ccd58f72f23%3Ffields%3DcitationCount)

2022    Computer Vision and Pattern Recognition 

![image-20250202154239293](https://zuti.oss-cn-qingdao.aliyuncs.com/img/20250202154239538.png)
